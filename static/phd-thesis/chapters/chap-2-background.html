<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.31">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="description" content="Ph.D.&nbsp;Thesis, Center for Language and Cognition (CLCG), University of Groningen">

<title>2&nbsp; Background – From Insights to Impact</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../index.html" rel="prev">
<link href="../figures/logos/rug_crest_icon.png" rel="icon" type="image/png">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-e1a5c8363afafaef2c763b6775fbf3ca.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-df7dc7f297c6c2c740a551c3cb7e1581.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script src="../site_libs/quarto-contrib/pseudocode-2.4.1/pseudocode.min.js"></script>
<link href="../site_libs/quarto-contrib/pseudocode-2.4.1/pseudocode.min.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../html/custom.css">
</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../chapters/chap-2-background.html"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Background</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="../index.html" class="sidebar-logo-link">
      <img src="../figures/logos/rug_eng_red_hat_line.png" alt="RUG Coat of Arms" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="../">From Insights to Impact</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/gsarti/phd-thesis" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-github"></i></a>
    <a href="https://gsarti.com" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-person-circle"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/chap-2-background.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Background</span></span></a>
  </div>
</li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Attributing Context Usage in Multilingual NLP</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/chap-3-inseq.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Attributing Language Model Generations with the Inseq Toolkit</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/chap-4-pecore.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Quantifying Context Usage in Neural Machine Translation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/chap-5-mirage.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Answer Attribution for Trustworthy Retrieval-Augmented Generation</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Conditioning Generation for Personalized Machine Translation</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/chap-6-ramp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Retrieval and Marking for Attribute-Controlled Translation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/chap-7-sae-litmt.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Steering Language Models for Personalized Machine Translation</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Interpretability in Human Translation Workflows</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/chap-8-divemt.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Machine Translation Post-editing for Typologically Diverse Languages</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/chap-9-qe4pe.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Word-level Quality Estimation for Machine Translation Post-editing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/chap-10-unsup-wqe.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Unsupervised MT Error Detection and Human Disagreement</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/chap-11-conclusion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Conclusion</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Bibliography</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/appendix-a.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Attributing Context Usage in Multilingual NLP</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/appendix-b.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Conditioning Generation for Personalized Machine Translation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/appendix-c.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Interpretability in Human Translation Workflows</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#sec-chap2-nlm" id="toc-sec-chap2-nlm" class="nav-link active" data-scroll-target="#sec-chap2-nlm"><span class="header-section-number">2.1</span> From Neural Networks to Neural Language Models</a>
  <ul class="collapse">
  <li><a href="#sec-chap2-nlm-supervised" id="toc-sec-chap2-nlm-supervised" class="nav-link" data-scroll-target="#sec-chap2-nlm-supervised"><span class="header-section-number">2.1.1</span> Supervised Learning for Neural Networks</a></li>
  <li><a href="#sec-chap2-nlm-transformers" id="toc-sec-chap2-nlm-transformers" class="nav-link" data-scroll-target="#sec-chap2-nlm-transformers"><span class="header-section-number">2.1.2</span> Transformer Neural Networks</a></li>
  <li><a href="#sec-chap2-nlm-lm" id="toc-sec-chap2-nlm-lm" class="nav-link" data-scroll-target="#sec-chap2-nlm-lm"><span class="header-section-number">2.1.3</span> Transformer Language Models</a></li>
  </ul></li>
  <li><a href="#sec-chap2-attrib" id="toc-sec-chap2-attrib" class="nav-link" data-scroll-target="#sec-chap2-attrib"><span class="header-section-number">2.2</span> Explaining Predictions with Input Attribution</a>
  <ul class="collapse">
  <li><a href="#sec-chap2-attrib-categories" id="toc-sec-chap2-attrib-categories" class="nav-link" data-scroll-target="#sec-chap2-attrib-categories"><span class="header-section-number">2.2.1</span> Attribution Method Categories</a></li>
  <li><a href="#sec-chap2-attrib-eval" id="toc-sec-chap2-attrib-eval" class="nav-link" data-scroll-target="#sec-chap2-attrib-eval"><span class="header-section-number">2.2.2</span> Evaluating and Using Attribution Methods</a></li>
  </ul></li>
  <li><a href="#sec-chap2-steer" id="toc-sec-chap2-steer" class="nav-link" data-scroll-target="#sec-chap2-steer"><span class="header-section-number">2.3</span> Conditioning Language Model Generations</a>
  <ul class="collapse">
  <li><a href="#sec-chap2-steer-context" id="toc-sec-chap2-steer-context" class="nav-link" data-scroll-target="#sec-chap2-steer-context"><span class="header-section-number">2.3.1</span> Controlling Input Context</a></li>
  <li><a href="#sec-chap2-steer-activations" id="toc-sec-chap2-steer-activations" class="nav-link" data-scroll-target="#sec-chap2-steer-activations"><span class="header-section-number">2.3.2</span> Controlling Model Representations</a></li>
  </ul></li>
  <li><a href="#sec-chap2-mt" id="toc-sec-chap2-mt" class="nav-link" data-scroll-target="#sec-chap2-mt"><span class="header-section-number">2.4</span> Machine Translation</a></li>
  <li><a href="#sec-chap2-pe" id="toc-sec-chap2-pe" class="nav-link" data-scroll-target="#sec-chap2-pe"><span class="header-section-number">2.5</span> MT Post-Editing and Evaluation</a>
  <ul class="collapse">
  <li><a href="#post-editing-mt" id="toc-post-editing-mt" class="nav-link" data-scroll-target="#post-editing-mt"><span class="header-section-number">2.5.1</span> Post-editing MT</a></li>
  <li><a href="#mt-evaluation" id="toc-mt-evaluation" class="nav-link" data-scroll-target="#mt-evaluation"><span class="header-section-number">2.5.2</span> MT Evaluation</a></li>
  </ul></li>
  <li><a href="#sec-chap2-qe" id="toc-sec-chap2-qe" class="nav-link" data-scroll-target="#sec-chap2-qe"><span class="header-section-number">2.6</span> Quality Estimation for MT</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="sec-chap-2-background" class="quarto-section-identifier"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Background</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>The initial sections of this chapter adapt contents from <em>A Primer on the Inner Workings of Transformer-based Language Models</em> <span class="citation" data-cites="ferrando-etal-2024-primer">(<a href="references.html#ref-ferrando-etal-2024-primer" role="doc-biblioref">Ferrando et al., 2024</a>)</span>.</p>
</div>
</div>
</div>
<blockquote class="blockquote">
<p><em>Distress not yourself if you cannot at first understand the deeper mysteries of Spaceland. By degrees they will dawn upon you.</em></p>
<p><em>– Edwin A. Abbott, Flatland: A Romance of Many Dimensions, 1884</em></p>
</blockquote>
<p>This chapter provides a succinct introduction to various topics discussed in the experimental chapters of this dissertation. Rather than a comprehensive review of relevant literature, it aims to provide key background about the research presented in this manuscript.</p>
<p>In particular, <a href="#sec-chap2-nlm" class="quarto-xref"><span>Section 2.1</span></a> and <a href="#sec-chap2-mt" class="quarto-xref"><span>Section 2.4</span></a> discuss the basic functioning of neural networks-based language models and machine translation (MT) systems, and introduce the machine translation task representing the core focus of this work. <a href="#sec-chap2-attrib" class="quarto-xref"><span>Section 2.2</span></a> and <a href="#sec-chap2-steer" class="quarto-xref"><span>Section 2.3</span></a> provide an introduction to methods for attributing inputs and conditioning generation in language models, corresponding to the topics discussed in Part I and Part II. Finally, <a href="#sec-chap2-pe" class="quarto-xref"><span>Section 2.5</span></a> and <a href="#sec-chap2-qe" class="quarto-xref"><span>Section 2.6</span></a> dive deeper in the translation domain, providing an overview of how MT models are employed in the translation industry by human post-editors, and discussing techniques for automatically evaluating machine translation quality. These notions provide a valuable background to Part III, which focuses on the impact of interpretability insights on human translation workflows.</p>
<p>Beyond this background section, each experimental chapter briefly summarizes relevant literature to contextualize the research questions and findings.</p>
<section id="sec-chap2-nlm" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="sec-chap2-nlm"><span class="header-section-number">2.1</span> From Neural Networks to Neural Language Models</h2>
<p><em>Neural networks</em> are computational models which integrate principles from statistical learning theory <span class="citation" data-cites="vapnik-1995-nature">(<a href="references.html#ref-vapnik-1995-nature" role="doc-biblioref">Vapnik, 1995</a>)</span>, consisting of interconnected nodes (<em>neurons</em>) organized in layers, where each connection has an associated weight. Formally, a neural network is a function <span class="math inline">\(\mathbf{f}: \mathcal{X} \to \mathcal{Y}\)</span> that maps inputs <span class="math inline">\(\mathbf{x} \in \mathcal{X}\)</span> to outputs <span class="math inline">\(\mathbf{y} \in \mathcal{Y}\)</span>, where <span class="math inline">\(\mathcal{X}\)</span> and <span class="math inline">\(\mathcal{Y}\)</span> are the input and output feature spaces, respectively. The function <span class="math inline">\(\mathbf{f}\)</span> is parameterized by weights <span class="math inline">\(\mathbf{\theta} \in \Theta\)</span>, which are typically learned from data through the training process described in <a href="#sec-chap2-nlm-supervised" class="quarto-xref"><span>Section 2.1.1</span></a>. Individual neurons are functions parametrized by <em>weights</em> <span class="math inline">\(\mathbf{w} \in \mathbb{R}^d\)</span> and <em>biases</em> <span class="math inline">\(b \in \mathbb{R}\)</span>, which are combined to produce an output <span class="math inline">\(\sigma(\mathbf{w}^T\mathbf{x} + b)\)</span>, where <span class="math inline">\(\sigma\)</span> is a non-linear <em>activation function</em>. Thanks to <em>non-linearities</em>, sequences of neurons can learn to represent complex relations from input vector <span class="math inline">\(\mathbf{x}\)</span>.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<section id="sec-chap2-nlm-supervised" class="level3" data-number="2.1.1">
<h3 data-number="2.1.1" class="anchored" data-anchor-id="sec-chap2-nlm-supervised"><span class="header-section-number">2.1.1</span> Supervised Learning for Neural Networks</h3>
<p>In the <em>supervised learning</em> paradigm, given a training dataset <span class="math inline">\(\mathcal{D}\)</span> containing paired instances:</p>
<p><span id="eq-chap2-dataset"><span class="math display">\[\mathcal{D} = \{(\mathbf{x}_1, y_1), \ldots, (\mathbf{x}_N, y_N)\} \in (\mathcal{X} \times \mathcal{Y})^n \tag{2.1}\]</span></span></p>
<p>where <span class="math inline">\(\mathbf{x}_i\)</span> is a vector of input features and <span class="math inline">\(\mathbf{y}_i\)</span> is the expected output, a neural network is trained to learn a functional mapping <span class="math inline">\(\mathbf{f}\)</span> from inputs <span class="math inline">\(\mathbf{x}\)</span> to labels <span class="math inline">\(\mathbf{y}\)</span> by minimizing the average value of a <em>loss function</em> <span class="math inline">\(\ell: \mathcal{Y} \times \mathcal{Y} \to \mathcal{R}\)</span>, such that <span class="math inline">\(\ell(\mathbf{f}(\mathbf{x}), \mathbf{y})\)</span> quantifies the gap between predicted outcomes <span class="math inline">\(\tilde{y}\)</span> and ground truth <span class="math inline">\(y\)</span> over examples in <span class="math inline">\(\mathcal{D}\)</span>. The function <span class="math inline">\(\mathbf{f}\)</span> is parameterized by weights <span class="math inline">\(\mathbf{\theta} \in \Theta\)</span>, which are optimized during training so as to minimize the loss function. Such optimization is typically performed using some variant of stochastic gradient descent (SGD), in which iterative steps <span class="math inline">\(1, \dots, t \dots, T\)</span> are taken to update <span class="math inline">\(\mathbf{\theta}\)</span> in the direction of the negative gradient of the loss function with respect to the weights:</p>
<p><span id="eq-chap2-gradient-update"><span class="math display">\[\mathbf{\theta}_{t+1} \leftarrow \mathbf{\theta}_t - \eta \;\nabla_{\mathbf{\theta}} \,\ell(\mathbf{f}(\mathbf{x}_j; \mathbf{\theta}_t), \mathbf{y}_j) \tag{2.2}\]</span></span></p>
<p>where <span class="math inline">\(\eta\)</span> is a chosen <em>learning rate</em>, and <span class="math inline">\(\mathbf{x}_j\)</span> and <span class="math inline">\(\mathbf{y}_j\)</span> are a subset of randomly sampled input-output pairs from the training set <span class="math inline">\(\mathcal{D}\)</span>, typically referred to as <em>mini-batch</em>. This iterative refinement of model parameters is repeated until convergence, i.e.&nbsp;until the model performance on a left-out validation set does not improve significantly, allowing for a robust convergence to a local minimum of the loss function, even for non-convex problems and high-dimensional parameter spaces.</p>
<p>We commonly refer to the inference process going from input <span class="math inline">\(\mathbf{x}\)</span> to output <span class="math inline">\(\mathbf{y}\)</span> as <em>forward pass</em>, and to the process of computing gradients and updating model parameters as <em>backward pass</em>.</p>
</section>
<section id="sec-chap2-nlm-transformers" class="level3" data-number="2.1.2">
<h3 data-number="2.1.2" class="anchored" data-anchor-id="sec-chap2-nlm-transformers"><span class="header-section-number">2.1.2</span> Transformer Neural Networks</h3>
<p>Transformers <span class="citation" data-cites="vaswani-etal-2017-attention">(<a href="references.html#ref-vaswani-etal-2017-attention" role="doc-biblioref">Vaswani et al., 2017</a>)</span> are a class of neural networks that have become the de-facto standard for most natural language processing tasks, constituting the core neural network architecture employed throughout this thesis’ experiments. In essence, a transformer consists of a sequence of identical macro-layers, dubbed <em>blocks</em>, progressively contextualizing a sequence of input features <span class="math inline">\(\mathbf{Z} \in \mathbb{R}^{S \times d}\)</span>, where <span class="math inline">\(S\)</span> is the sequence length and <span class="math inline">\(d\)</span> is the size of each feature vector. <a href="#fig-chap2-transformer-block" class="quarto-xref">Figure&nbsp;<span>2.1</span></a> illustrates the structure of a single transformer module, constituting the core of <em>decoder-only language models</em> such as GPT-3 <span class="citation" data-cites="brown-etal-2020-language">(<a href="references.html#ref-brown-etal-2020-language" role="doc-biblioref">Brown et al., 2020</a>)</span> presented later in <a href="#sec-chap2-nlm-lm" class="quarto-xref"><span>Section 2.1.3</span></a>. Notably, the transformer architecture is characterized by its ability to process input sequences in parallel, as opposed to recurrent models <span class="citation" data-cites="rumelhart-mcclelland-1987-learning hochreiter-schmidhuber-1997-long">(<a href="references.html#ref-rumelhart-mcclelland-1987-learning" role="doc-biblioref">Rumelhart and McClelland, 1987</a>; <a href="references.html#ref-hochreiter-schmidhuber-1997-long" role="doc-biblioref">Hochreiter and Schmidhuber, 1997</a>)</span>, making it highly efficient for training on large datasets.</p>
<div id="fig-chap2-transformer-block" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="t">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-chap2-transformer-block-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../figures/chap-2-background/transformer_block.webp" class="img-fluid figure-img" style="width:45.0%" data-fig-pos="t">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-chap2-transformer-block-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.1: An example transformer module with <span class="math inline">\(N\)</span> blocks. We adopt the residual stream view of <span class="citation" data-cites="elhage-etal-2021-mathematical">Elhage et al. (<a href="references.html#ref-elhage-etal-2021-mathematical" role="doc-biblioref">2021</a>)</span>, with residual connections linearized to emphasize the read-write operations performed by attention and feed-forward network modules.
</figcaption>
</figure>
</div>
<p>We now describe the main components of a transformer block in order of execution during the forward pass, using <span class="math inline">\(\mathbf{z}_i \in \mathbb{R}^d\)</span> to denote the input representations at each component for sequence element <span class="math inline">\(i\)</span>. This will be useful for explaining steering intervention and vocabulary projection methods used in <a href="chap-7-sae-litmt.html" class="quarto-xref"><span>Chapter 7</span></a> and <a href="chap-10-unsup-wqe.html" class="quarto-xref"><span>Chapter 10</span></a>, respectively.</p>
<p><span class="paragraph">Layer normalization (LN).</span> The layer normalization operation, also known as LayerNorm <span class="citation" data-cites="ba-etal-2016-layer">(<a href="references.html#ref-ba-etal-2016-layer" role="doc-biblioref">Ba et al., 2016</a>)</span>, is a common approach for stabilizing the training process of deep neural networks. In practice, layer normalization applies the transformation:</p>
<p><span id="eq-chap2-layernorm"><span class="math display">\[\text{LN}(\mathbf{z}_i) = \frac{\mathbf{z}_i - \mu(\mathbf{z}_i)}{\sigma(\mathbf{z}_i)} \odot \gamma + \beta \tag{2.3}\]</span></span></p>
<p>where <span class="math inline">\(\mu, \sigma\)</span> are the mean and the standard deviation of <span class="math inline">\(\mathbf{z}\)</span>, and <span class="math inline">\(\gamma, \beta\)</span> are learnable scale and bias parameters for the normalization. This operation helps to mitigate issues related to internal covariate shift, improving convergence during training. Recently, LayerNorm has been substituted with <em>RMSNorm</em> <span class="citation" data-cites="zhang-sennrich-2019-root">(<a href="references.html#ref-zhang-sennrich-2019-root" role="doc-biblioref">Zhang and Sennrich, 2019</a>)</span>, which removes the mean centering step and scales the input using the root mean square (RMS) statistic.</p>
<p><span class="paragraph">Multi-head self-attention (MHSA).</span> The self-attention mechanism is the core component of the transformer architecture, allowing the model to contextualize its representations at each layer by combining information across the input sequence. While the original formulation of multi-head self-attention by <span class="citation" data-cites="vaswani-etal-2017-attention">Vaswani et al. (<a href="references.html#ref-vaswani-etal-2017-attention" role="doc-biblioref">2017</a>)</span> involves a concatenation of attention head outputs before the final output projection, we follow here the more recent formulation by <span class="citation" data-cites="kobayashi-etal-2021-incorporating">Kobayashi et al. (<a href="references.html#ref-kobayashi-etal-2021-incorporating" role="doc-biblioref">2021</a>)</span> and <span class="citation" data-cites="elhage-etal-2021-mathematical">Elhage et al. (<a href="references.html#ref-elhage-etal-2021-mathematical" role="doc-biblioref">2021</a>)</span>, which reformulates the attention output computation using the sum of individual attention heads, emphasizing the linear reading and writing operations within the attention computation.</p>
<p>Concretely, the self-attention module is composed by a series of <span class="math inline">\(H\)</span> <em>attention heads</em> <span class="math inline">\(\text{Attn}_1, \ldots, \text{Attn}_H\)</span>, each computing the following weighted sum:</p>
<p><span id="eq-chap2-single-head-attn"><span class="math display">\[\text{Attn}_h(\mathbf{z}_i) = \sum_{j} \alpha^h_{ij} \mathbf{z}_j \mathbf{W}_V \mathbf{W}_O \tag{2.4}\]</span></span></p>
<p>Intuitively, the sharding of the attention mechanism into separate computations can be beneficial when processing the complex relations within different elements of the input sequence, for example, the lexical, syntactic and semantic dimensions of words in a text. The learnable weight matrices <span class="math inline">\(\mathbf{W}_V \in \mathbb{R}^{d \times d_h}\)</span> and <span class="math inline">\(\mathbf{W}_O \in \mathbb{R}^{d_h \times d}\)</span>, where <span class="math inline">\(d_h\)</span> represents the dimension of each head, can be combined into the so-called <em>output-value (OV) circuit</em> as <span class="math inline">\({\mathbf{W}_V \mathbf{W}_O = \mathbf{W}_{OV} \in \mathbb{R}^{d \times d}}\)</span>. For every key <span class="math inline">\(j\)</span> given the current query position <span class="math inline">\(i &lt; S\)</span>, the corresponding <em>attention weight</em> <span class="math inline">\(\alpha^h_{i}\)</span> is computed as:</p>
<p><span id="eq-chap2-attn-weight"><span class="math display">\[\alpha^h_{i} = \text{softmax}(\frac{\mathbf{z}_i \mathbf{W}_Q (\mathbf{W}_K \mathbf{Z})^T}{\sqrt{d_h}}) \tag{2.5}\]</span></span></p>
<p>Once again, the learnable weight matrices <span class="math inline">\(\mathbf{W}_Q \in \mathbb{R}^{d \times d_h}\)</span> and <span class="math inline">\(\mathbf{W}_K \in \mathbb{R}^{d \times d_h}\)</span> can be combined as the <em>query-key (QK) circuit</em> <span class="math inline">\({\mathbf{W}_Q \mathbf{W}_K^T = \mathbf{W}_{QK} \in \mathbb{R}^{d \times d}}\)</span>. This decomposition enables a view of QK and OV circuits as the units responsible for reading from (QK) and writing to (OV) the residual stream. Finally, the attention block output is the sum of individual attention heads: <span id="eq-chap2-multi-head-attn"><span class="math display">\[\text{Attn}(\mathbf{z}_i) = \sum_{h=1}^{H} \text{Attn}_h(\mathbf{z}_i) \tag{2.6}\]</span></span></p>
<p><span class="paragraph">Residual connection.</span> The introduction of residual connections <span class="citation" data-cites="he-etal-2016-deep-residual">(<a href="references.html#ref-he-etal-2016-deep-residual" role="doc-biblioref">He et al., 2016</a>)</span> in the transformer architecture allows the model to learn identity mappings more easily, facilitating the training of deeper networks and avoiding the <em>vanishing gradient</em> problem <span class="citation" data-cites="hochreiter-1998-vanishing">(<a href="references.html#ref-hochreiter-1998-vanishing" role="doc-biblioref">Hochreiter, 1998</a>)</span>. A residual connection is commonly applied to the output of the self-attention module, resulting in:</p>
<p><span id="eq-chap2-residual"><span class="math display">\[\text{ResAttn}(\mathbf{z}_i) = \text{Attn}\big(\text{LN}(\mathbf{z}_i)\big) + \mathbf{z}_i \tag{2.7}\]</span></span></p>
<p><span class="paragraph">Feedforward network (FFN).</span> The feedforward network (FFN) in the transformer block is composed of two learnable weight matrices<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>: <span class="math inline">\(\mathbf{W}_{\text{in}} \in \mathbb{R}^{d \times d_{\text{ffn}}}\)</span> and <span class="math inline">\(\mathbf{W}_{\text{out}} \in \mathbb{R}^{d_{\text{ffn}} \times d}\)</span>. <span class="math inline">\(\mathbf{W}_{\text{in}}\)</span> reads from the residual stream state <span class="math inline">\(\mathbf{z}\)</span>, and its result is passed through an element-wise non-linear activation function <span class="math inline">\(\sigma\)</span>, producing a set of <em>neuron activations</em>. These get transformed by <span class="math inline">\(\mathbf{W}_{\text{out}}\)</span> to produce the output <span class="math inline">\(\text{FFN}(\mathbf{z})\)</span>:</p>
<p><span id="eq-chap2-ffn"><span class="math display">\[\text{FFN}(\mathbf{z}_i) = \sigma(\mathbf{z}_i \mathbf{W}_{\text{in}}) \mathbf{W}_{\text{out}} \tag{2.8}\]</span></span></p>
<p>The FFN operation was compared to a retrieval step from a key-value memory <span class="citation" data-cites="geva-etal-2021-transformer">(<a href="references.html#ref-geva-etal-2021-transformer" role="doc-biblioref">Geva et al., 2021</a>)</span>, with keys stored in columns of <span class="math inline">\(\mathbf{W}_{\text{in}}\)</span> acting as pattern detectors over the input sequence, and values in rows of <span class="math inline">\(\mathbf{W}_{\text{out}}\)</span> being upweighted by respective neuron activation. The overall block structure from <a href="#fig-chap2-transformer-block" class="quarto-xref">Figure&nbsp;<span>2.1</span></a> can then be summarized as:</p>
<p><span id="eq-chap2-transformer-block"><span class="math display">\[\text{Block}(\mathbf{z}_i) = \text{FFN}\Big(\text{LN}\big(\text{ResAttn}(\mathbf{z}_i)\big)\Big) + \text{ResAttn}(\mathbf{z}_i) \tag{2.9}\]</span></span></p>
<p>We will henceforth use <span class="math inline">\(\mathbf{z}_i^l\)</span> to denote the output of the <span class="math inline">\(l\)</span>-th block for the <span class="math inline">\(i\)</span>-th element of the input sequence for transformer models.</p>
</section>
<section id="sec-chap2-nlm-lm" class="level3" data-number="2.1.3">
<h3 data-number="2.1.3" class="anchored" data-anchor-id="sec-chap2-nlm-lm"><span class="header-section-number">2.1.3</span> Transformer Language Models</h3>
<p>A <em>language model</em> is a probabilistic model that can assign probabilities to sequences of tokens. Formally, given an input sequence <span class="math inline">\(\mathbf{X} = \langle t_1, \dots, t_S \rangle\)</span> of <span class="math inline">\(S\)</span> tokens, which in the case of natural language are typically words or subword units <span class="citation" data-cites="sennrich-etal-2016-neural">(<a href="references.html#ref-sennrich-etal-2016-neural" role="doc-biblioref">Sennrich et al., 2016</a>)</span> from a vocabulary <span class="math inline">\(\mathcal{V}\)</span>, a language model <span class="math inline">\(f\)</span> computes the probability of the sequence as the product of token-level conditional probabilities:</p>
<p><span id="eq-chap2-lm"><span class="math display">\[P(\mathbf{X}) = P(t_1, \dots, t_S) = \prod_{i=1}^{S} P(t_i|t_1, \dots, t_{i-1}) \tag{2.10}\]</span></span></p>
<p>Language models operating under such formulation are typically referred to as <em>auto-regressive</em> or <em>causal</em> language models (CLMs, or simply LMs), to differentiate them from <em>masked</em> language models (MLMs) trained to fill the blanks in a sequence <span class="citation" data-cites="devlin-etal-2019-bert">(<a href="references.html#ref-devlin-etal-2019-bert" role="doc-biblioref">Devlin et al., 2019</a>)</span>. While MLMs were the main object of analysis of early interpretability research on transformer models <span class="citation" data-cites="tenney-etal-2019-bert clark-etal-2019-bert rogers-etal-2020-primer">(<a href="references.html#ref-tenney-etal-2019-bert" role="doc-biblioref">Tenney et al., 2019</a>; <a href="references.html#ref-clark-etal-2019-bert" role="doc-biblioref">Clark et al., 2019</a>; <a href="references.html#ref-rogers-etal-2020-primer" role="doc-biblioref">Rogers et al., 2020</a>)</span>, this dissertation focuses solely on CLMs, which after the advent of ChatGPT<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> in 2022 became the dominant paradigm in the NLP and interpretability community. CLMs are typically decoder-only models, following the structure introduced in <a href="#sec-chap2-nlm-transformers" class="quarto-xref"><span>Section 2.1.2</span></a>, or encoder-decoder models, such as the MT systems later discussed in <a href="#sec-chap2-mt" class="quarto-xref"><span>Section 2.4</span></a>.</p>
<p>Importantly, LMs can be used for generating text by iteratively sampling from the probability distribution over the next token <span class="math inline">\(t_{i}\)</span> given the previous tokens <span class="math inline">\(t_1, \dots, t_{i-1}\)</span>, e.g.&nbsp;using the <em>greedy decoding</em> sampling method:</p>
<p><span id="eq-chap2-greedy-decoding"><span class="math display">\[t_i^* = \underset{t \in \mathcal{V}}{\arg\,\max}\;P(\,\cdot\,|t_1, \ldots, t_{i-1}) \tag{2.11}\]</span></span></p>
<p>This sampling process can be repeated autoregressively, i.e.&nbsp;by adding the selected token <span class="math inline">\(t_i^*\)</span> to the input sequence, until a special <em>end-of-sequence</em> token is generated, or until a maximum sequence length is reached.</p>
<p>We now turn to the additional components required to convert the generic transformer model presented in the previous section into a language model able to process and generate sequences of tokens. <a href="#fig-chap2-transformer-lm" class="quarto-xref">Figure&nbsp;<span>2.2</span></a> shows a stylized view of a transformer LM.</p>
<div id="fig-chap2-transformer-lm" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="t">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-chap2-transformer-lm-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../figures/chap-2-background/transformer_lm.webp" class="img-fluid figure-img" data-fig-pos="t">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-chap2-transformer-lm-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.2: A transformer language model predicting the next word given a prefix.
</figcaption>
</figure>
</div>
<p><span class="paragraph">Embedding layer.</span> The first component of a transformer language model is the <em>embedding layer</em>, which maps input tokens to continuous vector representations, known as <em>embeddings</em>. Word embeddings such as Word2Vec <span class="citation" data-cites="mikolov-etal-2013-linguistic">(<a href="references.html#ref-mikolov-etal-2013-linguistic" role="doc-biblioref">Mikolov et al., 2013</a>)</span> and GloVe <span class="citation" data-cites="pennington-etal-2014-glove">(<a href="references.html#ref-pennington-etal-2014-glove" role="doc-biblioref">Pennington et al., 2014</a>)</span> revolutionized the field of natural language processing by exploiting <em>distributional semantics</em>, i.e.&nbsp;the fact that words which frequently appear in similar contexts should have similar meaning <span class="citation" data-cites="harris-1954-distributional">(<a href="references.html#ref-harris-1954-distributional" role="doc-biblioref">Harris, 1954</a>)</span>, to learn word representations end-to-end using gradient descent. In transformers, the <em>token embedding matrix</em> <span class="math inline">\(\mathbf{E} \in \mathbb{R}^{|\mathcal{V}| \times d}\)</span>, where <span class="math inline">\(d\)</span> is the size of the embedding vectors, and <span class="math inline">\(|\mathcal{V}|\)</span> is the vocabulary size, is learned jointly with the rest of model parameters during training. The embedding layer maps each token <span class="math inline">\(t_i\)</span> in the input sequence to its corresponding vector <span class="math inline">\(\mathbf{z}_i = \mathbf{E}[t_i]\)</span>. The resulting sequence of embeddings <span class="math inline">\(\mathbf{Z} \in \mathbb{R}^{S \times d}\)</span> corresponds to the input to the first transformer block. It is important to note that representations produced by <span class="math inline">\(\mathbf{E}\)</span> are not contextualized, i.e.&nbsp;the same token <span class="math inline">\(t_i\)</span> will always be mapped to the same vector <span class="math inline">\(\mathbf{z}_i\)</span>, regardless of its meaning in the given sequence. For example, the word <em>ring</em> will always be mapped to the same vector, regardless of whether it is used as a noun or a verb. The transformer blocks are used to contextualize these representations, i.e.&nbsp;produce different vectors for the same token depending on the remainder of the sequence.</p>
<p><span class="paragraph">Positional encodings.</span> While the sequential nature of language is an important factor in how we produce and process linguistic information, transformer models do not explicitly account for ordering across elements of the input sequence. For this reason, <em>positional encodings</em> injecting information about the position of each token in the sequence are commonly used in transformer-based language models. The most basic positional encoding is a fixed sinusoidal encoding <span class="citation" data-cites="vaswani-etal-2017-attention">(<a href="references.html#ref-vaswani-etal-2017-attention" role="doc-biblioref">Vaswani et al., 2017</a>)</span>, which is added directly to the input embeddings. Recent models, however, employ rotary position embeddings, allowing for the encoding of both absolute and relative positions between tokens, and allowing the model to generalize to longer contexts beyond those seen during training <span class="citation" data-cites="su-etal-2024-roformer">(<a href="references.html#ref-su-etal-2024-roformer" role="doc-biblioref">Su et al., 2024</a>)</span>.</p>
<p><span class="paragraph">Causal self-attention.</span> The self-attention mechanism in transformer language models is <em>causal</em>, meaning that the attention weights for each token <span class="math inline">\(t_i\)</span> are computed only over the tokens preceding it in the sequence, i.e.&nbsp;<span class="math inline">\(t_1, \ldots, t_{i-1}\)</span>. This ensures that the model can only attend to past tokens when predicting the next token, preserving the auto-regressive nature of the model. The causal self-attention mechanism is implemented by masking out future tokens in the attention computation, ensuring that <span class="math inline">\(\alpha^h\)</span> is computed only for <span class="math inline">\(j \leq i\)</span> in <a href="#eq-chap2-single-head-attn" class="quarto-xref">Equation&nbsp;<span>2.4</span></a>, and that only representations <span class="math inline">\(Z_{\leq i}\)</span> are used to compute the key vector in <a href="#eq-chap2-attn-weight" class="quarto-xref">Equation&nbsp;<span>2.5</span></a>.</p>
<p><span class="paragraph">Prediction head.</span> The prediction head of a transformer language models consists of a so-called <em>unembedding</em> matrix <span class="math inline">\(\mathbf{W}_{U} \in \mathbb{R}^{d \times |\mathcal{V}|}\)</span> mirroring the initial embedding operation, sometimes accompanied by a bias. The last residual stream state <span class="math inline">\(\mathbf{z}_S^L\)</span>, where <span class="math inline">\(L\)</span> is the number of transformer blocks and <span class="math inline">\(S\)</span> is the sequence length, gets transformed by this linear map converting the representation into a next-token distribution of logits, which is turned into a probability distribution via the softmax function:</p>
<p><span id="eq-chap2-next-token-prob"><span class="math display">\[P(\,\cdot\,|t_1, \ldots, t_{i-1}) = \text{softmax}(\mathbf{z}_i^L \mathbf{W}_{U}) \tag{2.12}\]</span></span></p>
<p>In light of the residual stream view presented in <a href="#sec-chap2-nlm-transformers" class="quarto-xref"><span>Section 2.1.2</span></a>, showing that different model components read from and write to the residual stream, it is natural to believe that the predictions derived by applying the unembedding matrix to the final residual stream state <span class="math inline">\(\mathbf{z}_S^L\)</span> are the product of an <em>iterative refinement</em> across model components <span class="citation" data-cites="jastrzebski-etal-2018-residual">(<a href="references.html#ref-jastrzebski-etal-2018-residual" role="doc-biblioref">Jastrzebski et al., 2018</a>)</span>. The <em>logit lens</em> method <span class="citation" data-cites="logitlens">(<a href="references.html#ref-logitlens" role="doc-biblioref">nostalgebraist, 2020</a>)</span>, which we study for error detection in <a href="chap-10-unsup-wqe.html" class="quarto-xref"><span>Chapter 10</span></a>, exploits this intuition to analyze how the model refines the prediction throughout the forward pass, by projecting intermediate residual stream states <span class="math inline">\(\mathbf{z}_S^l\)</span>, with <span class="math inline">\(l &lt; L\)</span>, to the vocabulary space using <span class="math inline">\(\mathbf{W}_{U}\)</span>.</p>
<p><span class="paragraph">Language model pre-training.</span> Modern language models such as those employed in this thesis are typically <em>pre-trained</em> on large web corpora spanning billions or trillions of tokens using the next-token prediction objective, i.e.&nbsp;minimizing the cross-entropy loss between the next-token distribution predicted by the model and the next observed token. This frames the language model training problem as an instance of supervised learning, which we presented in <a href="#sec-chap2-nlm-supervised" class="quarto-xref"><span>Section 2.1.1</span></a>. Formally, given a minibatch <span class="math inline">\(D_t\)</span> of corpus <span class="math inline">\(\mathcal{D}\)</span> composed by sequences of tokens <span class="math inline">\(\mathbf{X_k} = \langle t_1, \ldots, t_{S_k} \rangle\)</span>, the loss for a single training step is computed as:</p>
<p><span id="eq-chap2-celoss"><span class="math display">\[\mathcal{L}_{\text{step}} = -\frac{1}{|D_t|} \sum_{\mathbf{X_k} \in D_t} \sum_{i=1}^{S_k} \log P(t_i|t_1, \ldots, t_{i-1}) \tag{2.13}\]</span></span></p>
<p>Concretely, this corresponds to maximizing the likelihood of the observed tokens given the context provided by the preceding tokens, while minimizing the likelihood of all other incorrect tokens.</p>
<p><span class="paragraph">Language model post-training.</span> After pre-training, language models can be used for generating text given some context, but mostly lack the ability to perform specific tasks without being provided explicit examples, or respond to queries as conversational agents. For this reasons, all language models used for our experiments underwent additional <em>supervised fine-tuning</em> (SFT, also known as instruction tuning), allowing them to learn input-output mappings for realistic user queries beyond natural text occurrences in the pre-training corpus <span class="citation" data-cites="howard-ruder-2018-universal sanh2022multitask">(<a href="references.html#ref-howard-ruder-2018-universal" role="doc-biblioref">Howard and Ruder, 2018</a>; <a href="references.html#ref-sanh2022multitask" role="doc-biblioref">Sanh et al., 2022</a>)</span>. The fine-tuning process still involves the same <span class="math inline">\(\mathcal{L}_{\text{step}}\)</span> loss function over a smaller, curated set of demonstrations. Some of the models we study—such as the Gemma 2 models from <a href="chap-7-sae-litmt.html" class="quarto-xref"><span>Chapter 7</span></a> or the Zephyr model from <a href="chap-5-mirage.html" class="quarto-xref"><span>Chapter 5</span></a> —underwent an additional <em>reinforcement learning from human feedback</em> (RLHF) step, in which the model is fine-tuned to maximize the likelihood of human preferences over pairs of model generations, using a reward model trained on human preferences. This process is typically performed using Proximal Policy Optimization [PPO;<span class="citation" data-cites="schulman-etal-2017-proximal">Schulman et al. (<a href="references.html#ref-schulman-etal-2017-proximal" role="doc-biblioref">2017</a>)</span>] or similar reinforcement learning algorithms. Unless otherwise specified, we use the term <em>language model</em> to refer to transformer language models that were first pre-trained and then fine-tuned, representing the main focus of this thesis.</p>
</section>
</section>
<section id="sec-chap2-attrib" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="sec-chap2-attrib"><span class="header-section-number">2.2</span> Explaining Predictions with Input Attribution</h2>
<p>Contrary to linear models, where learned coefficients directly correspond to the influence of their respective features towards predictions, neural networks’ outcomes cannot be directly interpreted due to the presence of multiple nonlinearities across layers, rendering the attribution of model prediction to individual input features non-trivial. <em>Input attribution</em> methods, also known as <em>feature attribution</em>, were introduced to address this issue by providing a principled way to assign importance scores to input features, clarifying the rationales behind model decisions <span class="citation" data-cites="zeiler-etal-2011-adaptive">(<a href="references.html#ref-zeiler-etal-2011-adaptive" role="doc-biblioref">Zeiler et al., 2011</a>)</span>.</p>
<p>Formally, for a model <span class="math inline">\(\mathbf{f} \in \mathcal{F}: \mathcal{X} \to \mathcal{Y}\)</span>, given an input <span class="math inline">\(\mathbf{x} \in \mathcal{X}\)</span>, we can define the attribution method <span class="math inline">\(\gamma\)</span> as a functional:</p>
<p><span class="math display">\[\gamma: \mathcal{X} \times \mathcal{F} \to \mathbb{R}^{|\mathcal{X}|}\]</span></p>
<p>so that <span class="math inline">\(\mathbf{a}_{\mathbf{f}(\mathbf{x})} = \gamma(\mathbf{x}, \mathbf{f})\)</span> is a vector of attribution scores quantifying the influence of each element of <span class="math inline">\(\mathbf{x}\)</span> on the model predictive distribution <span class="math inline">\(\mathbf{f}(\mathbf{x})\)</span>, with higher scores representing greater importance <span class="citation" data-cites="fel-2024-sparks">(<a href="references.html#ref-fel-2024-sparks" role="doc-biblioref">Fel, 2024</a>)</span>. It is worth noting that attribution methods can rely on one or more specific outcomes <span class="math inline">\(\mathbf{y} \in \mathcal{Y}\)</span> from the predictive distribution <span class="math inline">\(\mathbf{f}(\mathbf{x})\)</span>, such as perturbation-based approaches <span class="citation" data-cites="covert-etal-2021-explaining">(<a href="references.html#ref-covert-etal-2021-explaining" role="doc-biblioref">Covert et al., 2021</a>)</span>, or simply rely on the flow of information within the model to identify important input elements <span class="citation" data-cites="abnar-zuidema-2020-quantifying">(<a href="references.html#ref-abnar-zuidema-2020-quantifying" role="doc-biblioref">Abnar and Zuidema, 2020</a>)</span>. We call the former methods <em>target-dependent</em>, and we discuss them further in <a href="chap-4-pecore.html" class="quarto-xref"><span>Chapter 4</span></a>.</p>
<section id="sec-chap2-attrib-categories" class="level3" data-number="2.2.1">
<h3 data-number="2.2.1" class="anchored" data-anchor-id="sec-chap2-attrib-categories"><span class="header-section-number">2.2.1</span> Attribution Method Categories</h3>
<p>We now briefly summarize common families of input attribution methods, which are employed throughout the first part of this thesis. An in-depth overview of input attribution techniques for natural language processing can be found in <span class="citation" data-cites="madsen-etal-2022-posthoc">Madsen et al. (<a href="references.html#ref-madsen-etal-2022-posthoc" role="doc-biblioref">2022</a>)</span>.</p>
<p><span class="paragraph">Gradient-based attribution</span> For neural network models like transformer LMs, gradients are a natural source of input saliency which can be exploited for attribution purposes <span class="citation" data-cites="simonyan-etal-2014-saliency li-etal-2016-visualizing">(<a href="references.html#ref-simonyan-etal-2014-saliency" role="doc-biblioref">Simonyan et al., 2014</a>; <a href="references.html#ref-li-etal-2016-visualizing" role="doc-biblioref">Li et al., 2016</a>)</span>. A simple gradient-based attribution corresponds to a first-order Taylor expansion of the model at a point <span class="math inline">\(\mathbf{x}\)</span>, expressed as <span class="math inline">\(\nabla \mathbf{f}(\mathbf{x}) \cdot \mathbf{x} + \mathbf{b}\)</span>. The resulting gradient <span class="math inline">\(\nabla_\mathbf{x}^c {\mathbf{f}}\)</span> captures intuitively the <em>sensitivity</em> of the model prediction <span class="math inline">\(c\)</span> to each element in the input. In the case of transformer LMs, <span class="math inline">\(\nabla_\mathbf{x}^{t^*} {\mathbf{f}} \in \mathbb{R}^{S \times d}\)</span>, i.e.&nbsp;every dimension of the input embedding is associated with a attribution score, and the logit of the top predicted token <span class="math inline">\(t^*\)</span> is used as differentiation target for gradient computation.<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> These scores are generally aggregated at a token level to obtain a more intuitive overview of the influence of individual tokens. This is commonly done by taking the <span class="math inline">\(L^p\)</span> norm of the gradient vector:</p>
<p><span id="eq-chap2-gradient"><span class="math display">\[\text{Grad}_{\,\mathbf{f}(\mathbf{x}) \leftarrow t^*} = \|\nabla_{\mathbf{x}}^{t^*} \mathbf{f}\|_p \in \mathbb{R}^{S} \tag{2.14}\]</span></span></p>
<p><a href="#fig-chap2-gradient-attrib" class="quarto-xref">Figure&nbsp;<span>2.3</span></a> shows an example of gradient attribution on a language model. By taking the dot product between the gradient vector and the input embedding <span class="math inline">\({\nabla_{\mathbf{x}}^{t^*} \mathbf{f}\cdot \mathbf{x}}\)</span>, known as the <em>gradient <span class="math inline">\(\times\)</span> input</em> method, this sensitivity information can be converted to an importance estimate. More elaborate gradient-based attribution methods employ perturbations of the input embedding <span class="citation" data-cites="sundararajan-etal-2017-ig smilkov-etal-2017-smoothgrad">(<a href="references.html#ref-sundararajan-etal-2017-ig" role="doc-biblioref">Sundararajan et al., 2017</a>; <a href="references.html#ref-smilkov-etal-2017-smoothgrad" role="doc-biblioref">Smilkov et al., 2017</a>)</span> or ad-hoc gradient propagation rules <span class="citation" data-cites="bach-etal-2015-pixel achtibat-etal-2024-attnlrp">(<a href="references.html#ref-bach-etal-2015-pixel" role="doc-biblioref">Bach, 2015</a>; <a href="references.html#ref-achtibat-etal-2024-attnlrp" role="doc-biblioref">Achtibat et al., 2024</a>)</span> to filter noisy gradient information.</p>
<div id="fig-chap2-gradient-attrib" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="t">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-chap2-gradient-attrib-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../figures/chap-2-background/gradient_attrib.webp" class="img-fluid figure-img" data-fig-pos="t">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-chap2-gradient-attrib-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.3: Gradient-based attribution in a neural language model.
</figcaption>
</figure>
</div>
<p>Gradient-based attribution methods are heavily used in the investigations of <a href="chap-3-inseq.html" class="quarto-xref"><span>Chapter 3</span></a>, <a href="chap-4-pecore.html" class="quarto-xref"><span>Chapter 4</span></a> and <a href="chap-5-mirage.html" class="quarto-xref"><span>Chapter 5</span></a>, representing the majority of methods supported by the Inseq toolkit and the most effective approaches for contextual cues imputation in the <span class="smallcaps">PECoRe</span> framework. Notably, gradient attribution can be exploited in a similar way to identify the importance of intermediate states <span class="math inline">\(\mathbf{z}\)</span> in the model, as opposed to input representations <span class="math inline">\(\mathbf{x}\)</span>, i.e.&nbsp;using <span class="math inline">\(\nabla_{\mathbf{z}}^{t^*} \mathbf{f}\)</span>. The CAT method proposed in <a href="chap-3-inseq.html" class="quarto-xref"><span>Chapter 3</span></a> case study adopts this attribution-based approach to locate factual knowledge across LM layers.</p>
<p><span class="paragraph">Perturbation-based attribution</span> Another popular family of approaches estimates input importance by adding noise or ablating input elements and measuring the resulting impact on model predictions. For instance, the input token <span class="math inline">\(w_j\)</span> at position <span class="math inline">\(j\)</span> can be removed, and the resulting probability difference <span class="math inline">\(p(t^*|t_{&lt;i}) - p(t_{\setminus w_j}^*|t_{&lt;i})\)</span>, where <span class="math inline">\(t^*\)</span> is the predicted token for current sequence position <span class="math inline">\(i\)</span> and <span class="math inline">\(j &lt; i\)</span>, can be used as an estimate for its importance. If the logit or probability given to <span class="math inline">\(w\)</span> does not change, we conclude that the <span class="math inline">\(i\)</span>-th token has no influence. A multitude of perturbation-based attribution methods exist in the literature, such as those based on local surrogate models such as LIME <span class="citation" data-cites="ribeiro-etal-2016-lime">(<a href="references.html#ref-ribeiro-etal-2016-lime" role="doc-biblioref">Ribeiro et al., 2016</a>)</span>, or those derived from game theory like SHAP <span class="citation" data-cites="lundberg-lee-2017-shap">(<a href="references.html#ref-lundberg-lee-2017-shap" role="doc-biblioref">Lundberg and Lee, 2017</a>)</span>. Notably, some architecture-specific methods such as Value Zeroing <span class="citation" data-cites="mohebbi-etal-2023-quantifying">(<a href="references.html#ref-mohebbi-etal-2023-quantifying" role="doc-biblioref">Mohebbi et al., 2023</a>)</span> have been proposed to mitigate the disruptive impact of perturbations on model behaviors. A comprehensive framework unifying various perturbation-based approaches is presented by <span class="citation" data-cites="covert-etal-2021-explaining">Covert et al. (<a href="references.html#ref-covert-etal-2021-explaining" role="doc-biblioref">2021</a>)</span>.</p>
<p><span class="paragraph">Context mixing for attribution</span> Model internals such as the attention weights <span class="math inline">\(\alpha\)</span> presented in <a href="#sec-chap2-nlm-transformers" class="quarto-xref"><span>Section 2.1.2</span></a> were initially proposed as possible explanations for model behavior <span class="citation" data-cites="bahdanau-etal-2015-neural">(<a href="references.html#ref-bahdanau-etal-2015-neural" role="doc-biblioref">Bahdanau et al., 2015</a>)</span>, but were found unfaithful in reflecting the actual predictive behavior of language models <span class="citation" data-cites="jain-wallace-2019-attention bastings-filippova-2020-elephant">(<a href="references.html#ref-jain-wallace-2019-attention" role="doc-biblioref">Jain and Wallace, 2019</a>; <a href="references.html#ref-bastings-filippova-2020-elephant" role="doc-biblioref">Bastings and Filippova, 2020</a>)</span>. This is because, contrary to other approaches, they only accounted for the importance of specific model components, rather than a more general notion of saliency across the full model. However, recent methods have proposed more refined estimates of token contributions exploiting internals to quantify the information flow within LMs. Some of these alternatives include the use of the norm of value-weighted vectors and output-value-weighted vectors <span class="citation" data-cites="kobayashi-etal-2020-attention kobayashi-etal-2021-incorporating">(<a href="references.html#ref-kobayashi-etal-2020-attention" role="doc-biblioref">Kobayashi et al., 2020</a>; <a href="references.html#ref-kobayashi-etal-2021-incorporating" role="doc-biblioref">Kobayashi et al., 2021</a>)</span>, or the use of vectors’ distances to estimate token contributions <span class="citation" data-cites="ferrando-etal-2022-measuring">(<a href="references.html#ref-ferrando-etal-2022-measuring" role="doc-biblioref">Ferrando et al., 2022</a>)</span>. These methods result in a set of attribution scores <span class="math inline">\(\mathbf{a}_{\mathbf{f}(\mathbf{x})} \in \mathbb{R}^{S \times L}\)</span>, marking the contribution of position-specific representation across all layers <span class="math inline">\(1, \ldots, L\)</span> of the model. These per-layer attributions reflecting context mixing patterns are often aggregated using techniques such as <em>rollout</em> <span class="citation" data-cites="abnar-zuidema-2020-quantifying">(<a href="references.html#ref-abnar-zuidema-2020-quantifying" role="doc-biblioref">Abnar and Zuidema, 2020</a>)</span>, resulting in one score per input token participating in the attention operation. Such context mixing approaches have shown competitive faithfulness compared to best gradient and perturbation-based methods, despite employing only a single forward pass to estimate contributions.</p>
<p><span class="paragraph">Contrastive input attribution</span> An important limitation of input attribution methods for interpreting language models is that attributed output tokens belong to a large vocabulary space, often having semantically equivalent tokens competing for probability mass in next-word prediction <span class="citation" data-cites="holtzman-etal-2021-surface">(<a href="references.html#ref-holtzman-etal-2021-surface" role="doc-biblioref">Holtzman et al., 2021</a>)</span>. In this context, attribution scores are likely to misrepresent several overlapping factors such as grammatical correctness and semantic appropriateness driving the model prediction. Recent work addresses this issue by proposing a contrastive formulation of such methods, producing counterfactual explanations for why the model predicts token <span class="math inline">\(t^*\)</span> instead of an alternative token <span class="math inline">\(t^\sim\)</span>. <span class="citation" data-cites="yin-neubig-2022-interpreting">Yin and Neubig (<a href="references.html#ref-yin-neubig-2022-interpreting" role="doc-biblioref">2022</a>)</span> extend the vanilla gradient method of <a href="#eq-chap2-gradient" class="quarto-xref">Equation&nbsp;<span>2.14</span></a> to the contrastive setting as:</p>
<p><span id="eq-chap2-contrast-gradient"><span class="math display">\[\text{ContGrad}_{\,\mathbf{f}(\mathbf{x}) \leftarrow t^*, t^\sim} = \nabla_{\mathbf{x}}^{t^* - t^\sim} \mathbf{f} \tag{2.15}\]</span></span></p>
<p>We employ this formulation in the <span class="smallcaps">PECoRe</span> framework in <a href="chap-4-pecore.html" class="quarto-xref"><span>Chapter 4</span></a> and its extension of <a href="chap-5-mirage.html" class="quarto-xref"><span>Chapter 5</span></a> to identify salient context cues for generated tokens that were highly influenced by context.</p>
</section>
<section id="sec-chap2-attrib-eval" class="level3" data-number="2.2.2">
<h3 data-number="2.2.2" class="anchored" data-anchor-id="sec-chap2-attrib-eval"><span class="header-section-number">2.2.2</span> Evaluating and Using Attribution Methods</h3>
<p><span class="paragraph">Plausibility and Faithfulness</span> The evaluation of input attribution methods can be operationalized in terms of various desiderata. <em>Plausibility</em>, also referred to as “human-interpretability” <span class="citation" data-cites="lage-etal-2019-evaluation">(<a href="references.html#ref-lage-etal-2019-evaluation" role="doc-biblioref">Lage et al., 2019</a>)</span>, is a measure of <em>“how convincing the interpretation is to humans”</em> <span class="citation" data-cites="jacovi-goldberg-2020-towards">(<a href="references.html#ref-jacovi-goldberg-2020-towards" role="doc-biblioref">Jacovi and Goldberg, 2020</a>)</span>, i.e.&nbsp;how well the salient tokens identified by an attribution method are in agreement with those selected by human annotators. It is important to note that plausibility does not imply <strong>faithfulness</strong>, i.e.&nbsp;how accurately the rationale reflects the true reasoning process of the model <span class="citation" data-cites="wiegreffe-pinter-2019-attention">(<a href="references.html#ref-wiegreffe-pinter-2019-attention" role="doc-biblioref">Wiegreffe and Pinter, 2019</a>)</span>, since a good explanation of model behavior might not align with human intuition. Consider the following sentence from the BLiMP corpus <span class="citation" data-cites="warstadt-etal-2020-blimp-benchmark">(<a href="references.html#ref-warstadt-etal-2020-blimp-benchmark" role="doc-biblioref">Warstadt et al., 2020</a>)</span>.</p>
<blockquote class="blockquote">
<p><span class="math inline">\(\mathbf{x}\)</span> = A <strong>report</strong> about the Impressionists <u><strong>has</strong>/<span class="math inline">\(*\)</span>have</u> won the competition.</p>
</blockquote>
<p>For the sentence to be grammatically correct, the verb <em>to have</em> must be correctly inflected as <em>has</em> to agree with the preceding noun <em>report</em>. Hence, to evaluate the plausibility of a language model for this example, the model is provided with the prefix <span class="math inline">\(\mathbf{x}'\)</span> =“A report about the Impressionists”. Then, attribution scores are computed for every input token towards the prediction of <em>has</em> as the next token. Finally, we verify whether these scores identify the token <em>report</em> as the most important to predict <em>has</em>. We note that the selection of the pair <em>report</em>-<em>has</em> in the canonical procedure described above is entirely based on grammatical correctness, and other potential pairs not matching these constraints are not considered (e.g.&nbsp;the usage of <em>report</em> to predict <em>writing</em> instead of <em>has</em> as a likely continuation). This common procedure might also cause reasonable behaviors to be labeled as implausible. For example, the indefinite article <em>A</em> might be identified as the most important token to predict <em>has</em> since it is forcibly followed by a singular noun and can co-occur with <em>has</em> more frequently than <em>report</em> in the model’s training data. These limitations in the standard hypothesis-driven approach to plausibility evaluation motivate our proposal for <span class="smallcaps">PECoRe</span> as a data-driven alternative in <a href="chap-4-pecore.html" class="quarto-xref"><span>Chapter 4</span></a>.</p>
<p><span class="paragraph">Limitations of input attribution methods</span> While input attribution methods are commonly used to debug failure cases and identify biases in models’ predictions <span class="citation" data-cites="mccoy-etal-2019-right">(<a href="references.html#ref-mccoy-etal-2019-right" role="doc-biblioref">McCoy et al., 2019</a>)</span>, popular approaches were shown to be insensitive to variations in the model and data generating process <span class="citation" data-cites="adebayo-etal-2018-sanity sixt-etal-2020-explanations">(<a href="references.html#ref-adebayo-etal-2018-sanity" role="doc-biblioref">Adebayo et al., 2018</a>; <a href="references.html#ref-sixt-etal-2020-explanations" role="doc-biblioref">Sixt et al., 2020</a>)</span>, to disagree with each others’ predictions <span class="citation" data-cites="atanasova-etal-2020-diagnostic crabbe-vanderschaar-2023-evaluating krishna-etal-2024-disagreement">(<a href="references.html#ref-atanasova-etal-2020-diagnostic" role="doc-biblioref">Atanasova et al., 2020</a>; <a href="references.html#ref-crabbe-vanderschaar-2023-evaluating" role="doc-biblioref">Crabbé and Schaar, 2023</a>; <a href="references.html#ref-krishna-etal-2024-disagreement" role="doc-biblioref">Krishna et al., 2024</a>)</span> and to show limited capacity in detecting unseen spurious correlations <span class="citation" data-cites="adebayo-etal-2020-debugging adebayo-etal-2022-posthoc">(<a href="references.html#ref-adebayo-etal-2020-debugging" role="doc-biblioref">Adebayo et al., 2020</a>; <a href="references.html#ref-adebayo-etal-2022-posthoc" role="doc-biblioref">Adebayo et al., 2022</a>)</span>. Importantly, popular methods were found provably unreliable at predicting counterfactual model behavior in realistic settings <span class="citation" data-cites="bilodeau-etal-2024-impossibility">(<a href="references.html#ref-bilodeau-etal-2024-impossibility" role="doc-biblioref">Bilodeau et al., 2024</a>)</span>. Apart from theoretical limitations, perturbation-based approaches also suffer from out-of-distribution predictions induced by unrealistic noised or ablated inputs, and from high computational cost of targeted ablations for granular input elements.</p>
<p><span class="paragraph">Tools for input attribution</span> The <code>captum</code> library <span class="citation" data-cites="kokhlikyan-etal-2020-captum">(<a href="references.html#ref-kokhlikyan-etal-2020-captum" role="doc-biblioref">Kokhlikyan et al., 2020</a>)</span> is part of the Pytorch ecosystem providing access to several gradient and perturbation-based input attribution methods for any Pytorch-based model, with the recent addition of utilities for simplifying attribution analyses of generative LMs <span class="citation" data-cites="miglani-etal-2023-using">(<a href="references.html#ref-miglani-etal-2023-using" role="doc-biblioref">Miglani et al., 2023</a>)</span>. Several <code>captum</code>-based tools provide convenient APIs for input attribution of transformer-based models, notably Transformers Interpret <span class="citation" data-cites="pierse-2021-transformers">(<a href="references.html#ref-pierse-2021-transformers" role="doc-biblioref">Pierse, 2021</a>)</span>, <code>ferret</code> <span class="citation" data-cites="attanasio-etal-2023-ferret">(<a href="references.html#ref-attanasio-etal-2023-ferret" role="doc-biblioref">Attanasio et al., 2023</a>)</span> and <code>Ecco</code> <span class="citation" data-cites="alammar-2021-ecco">(<a href="references.html#ref-alammar-2021-ecco" role="doc-biblioref">Alammar, 2021</a>)</span>, which are mainly centered around language classification tasks. <code>SHAP</code> <span class="citation" data-cites="lundberg-lee-2017-shap">(<a href="references.html#ref-lundberg-lee-2017-shap" role="doc-biblioref">Lundberg and Lee, 2017</a>)</span> is a popular toolkit mainly centered on perturbation-based input attribution methods and model-agnostic explanations for various data modalities. The <code>saliency</code> library<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> provides framework-agnostic implementations for mainly gradient-based input attribution methods, while <code>LIT</code> <span class="citation" data-cites="tenney-etal-2020-language">(<a href="references.html#ref-tenney-etal-2020-language" role="doc-biblioref">Tenney et al., 2020</a>)</span> is a framework-agnostic tool providing a convenient set of utilities and an intuitive interface for interpretability studies spanning input attribution, concept-based explanations and counterfactual behavior evaluation. It notably includes a visual tool for debugging complex LLM prompts <span class="citation" data-cites="tenney-etal-2024-interactive">(<a href="references.html#ref-tenney-etal-2024-interactive" role="doc-biblioref">Tenney et al., 2024</a>)</span>. More recent low-level interpretability tools such as <code>nnsight</code> <span class="citation" data-cites="fiottokaufman-etal-2024-nnsight">(<a href="references.html#ref-fiottokaufman-etal-2024-nnsight" role="doc-biblioref">Fiotto-Kaufman et al., 2025</a>)</span> also support attribution, without explicitly providing abstractions to facilitate its usage. <code>inseq</code>, which we introduce in <a href="chap-3-inseq.html" class="quarto-xref"><span>Chapter 3</span></a> as part of this thesis’ contributions, is one of the most popular tools for input attribution of generative LMs, supporting advanced approaches for contrastive context attribution <span class="citation" data-cites="sarti-etal-2024-quantifying">(<a href="references.html#ref-sarti-etal-2024-quantifying" role="doc-biblioref">Sarti et al., 2024</a>)</span> and context mixing evaluation.</p>
</section>
</section>
<section id="sec-chap2-steer" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="sec-chap2-steer"><span class="header-section-number">2.3</span> Conditioning Language Model Generations</h2>
<p>This section describes the two main families of approaches for conditioning the behavior of language models during text generation. First, we present methods for modifying the input context by providing relevant information retrieved from external sources, or demonstrations of desired behavior, which we use in <a href="chap-5-mirage.html" class="quarto-xref"><span>Chapter 5</span></a>, <a href="chap-6-ramp.html" class="quarto-xref"><span>Chapter 6</span></a>, <a href="chap-7-sae-litmt.html" class="quarto-xref">and&nbsp;<span>7</span></a>. Then, we discuss approaches for modifying the model’s internal representations to achieve targeted interventions in the generation process, which we compare to prompting methods in <a href="chap-7-sae-litmt.html" class="quarto-xref"><span>Chapter 7</span></a>.</p>
<section id="sec-chap2-steer-context" class="level3" data-number="2.3.1">
<h3 data-number="2.3.1" class="anchored" data-anchor-id="sec-chap2-steer-context"><span class="header-section-number">2.3.1</span> Controlling Input Context</h3>
<p>Large language models have become widely popular due to their ability to adjust their predictions in light of few examples or relevant information provided in an input context (<em>prompt</em>), without requiring additional training <span class="citation" data-cites="brown-etal-2020-language">(<a href="references.html#ref-brown-etal-2020-language" role="doc-biblioref">Brown et al., 2020</a>)</span>. Prompting LLMs to exploit their <em>in-context learning</em> skills has become pervasive in the NLP community, with much effort devoted to designing effective prompts for various tasks <span class="citation" data-cites="dong-etal-2024-survey">(<a href="references.html#ref-dong-etal-2024-survey" role="doc-biblioref">Dong et al., 2024</a>)</span>.</p>
<p><em>Few-shot prompting</em> is an effective approach to adapt LLMs to new tasks by providing a few demonstrations of the desired behavior in the input context. For example, to perform a translation, a few source language examples can be provided in the prompt with their respective target language translations, and the model is expected to translate new source entries used as queries (<a href="#fig-chap2-icl-rag" class="quarto-xref">Figure&nbsp;<span>2.4</span></a>, left). <em>Zero-shot prompting</em> is a more challenging task, where the model is expected to perform well on a new task without any demonstrations, relying solely on its pre-trained knowledge. While effective, several studies highlighted the brittleness of prompting to unexpected factors such as the order of provided examples <span class="citation" data-cites="lu-etal-2022-fantastically">(<a href="references.html#ref-lu-etal-2022-fantastically" role="doc-biblioref">Lu et al., 2022</a>)</span>. In this thesis, we use few-shot prompting in our attribute-controlled translation experiments of <a href="chap-6-ramp.html" class="quarto-xref"><span>Chapter 6</span></a> and our literary translation experiments of <a href="chap-7-sae-litmt.html" class="quarto-xref"><span>Chapter 7</span></a>.</p>
<div id="fig-chap2-icl-rag" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="t">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-chap2-icl-rag-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../figures/chap-2-background/icl_rag.webp" class="img-fluid figure-img" data-fig-pos="t">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-chap2-icl-rag-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.4: <strong>Left:</strong> Few-shot prompting for English<span class="math inline">\(\rightarrow\)</span>Italian translation. <strong>Right:</strong> Retrieval-augmented generation for factual question answering. Relevant paragraphs are dynamically retrieved and infilled in the prompt using their similarity to the query to improve answer quality.
</figcaption>
</figure>
</div>
<p><em>Retrieval-augmented generation</em> (RAG) is a different approach for conditioning generation where the model is provided with relevant context paragraphs retrieved on-the-fly from an external dataset, such as Wikipedia or a domain-specific corpus. This context is then used to inform the model’s predictions, allowing it to generate more accurate and relevant responses without relying solely on its potentially faulty pre-training knowledge (<a href="#fig-chap2-icl-rag" class="quarto-xref">Figure&nbsp;<span>2.4</span></a>, right). RAG has been shown to be effective in improving the factual accuracy of model outputs and reducing hallucinations <span class="citation" data-cites="lewis-etal-2020-rag petroni-etal-2020-how">(<a href="references.html#ref-lewis-etal-2020-rag" role="doc-biblioref">Lewis et al., 2020</a>; <a href="references.html#ref-petroni-etal-2020-how" role="doc-biblioref">Petroni et al., 2020</a>)</span>. However, it is not directly obvious which retrieved paragraphs are motivating the model’s predictions, a challenge we address via input attribution in <a href="chap-5-mirage.html" class="quarto-xref"><span>Chapter 5</span></a>. <a href="chap-6-ramp.html" class="quarto-xref"><span>Chapter 6</span></a> also employs a similarity retrieval component to control the examples selected for few-shot prompting, showing that example selection leads to better performances in machine translation with LLMs.</p>
</section>
<section id="sec-chap2-steer-activations" class="level3" data-number="2.3.2">
<h3 data-number="2.3.2" class="anchored" data-anchor-id="sec-chap2-steer-activations"><span class="header-section-number">2.3.2</span> Controlling Model Representations</h3>
<p>Techniques for conditioning model behavior by modifying the model’s internal representations are commonly referred to as <em>steering</em> methods, and often exploit the linear structure of model activations to achieve simple targeted interventions. Indeed, the <em>linear representation hypothesis</em> states that latent properties of interest—for example, the tone of a response—are encoded as linear subspaces of the representation space in language model activation <span class="citation" data-cites="park-etal-2023-linear">(<a href="references.html#ref-park-etal-2023-linear" role="doc-biblioref">Park et al., 2023</a>)</span>. Such property was already observed in early work on word embeddings <span class="citation" data-cites="mikolov-etal-2013-linguistic">(<a href="references.html#ref-mikolov-etal-2013-linguistic" role="doc-biblioref">Mikolov et al., 2013</a>)</span>, where the direction of the vector between two words was shown to encode their semantic relationship, e.g.&nbsp;<span class="math inline">\(\mathbf{z}_{\text{king}} - \mathbf{z}_{\text{man}} + \mathbf{z}_{\text{woman}} \approx \mathbf{z}_{\text{queen}}\)</span>.</p>
<p>Recent work highlighted the effectiveness of linear interventions on language models representations using directions identified by a <em>probing classifier</em>, i.e.&nbsp;a model <span class="math inline">\(\mathbf{p}: \mathbb{R}^{d} \to \mathcal{C}\)</span> trained to predict a specific property of interest <span class="math inline">\(c \in \mathcal{C}\)</span> from the intermediate representation of a trained transformer LM <span class="citation" data-cites="kohn-2015-whats gupta-etal-2015-distributional belinkov-2022-probing">(<a href="references.html#ref-kohn-2015-whats" role="doc-biblioref">Köhn, 2015</a>; <a href="references.html#ref-gupta-etal-2015-distributional" role="doc-biblioref">Gupta et al., 2015</a>; see <a href="references.html#ref-belinkov-2022-probing" role="doc-biblioref">Belinkov, 2022</a> for a review)</span>. For instance, adding negative multiples of the sentiment direction (<span class="math inline">\(\mathbf{c}_\text{sent}\)</span>) to the residual stream, i.e.&nbsp;modifying the activation <span class="math inline">\(\mathbf{z}^l\)</span> as <span class="math inline">\({\tilde{\mathbf{z}}^l \leftarrow \mathbf{z}^l - \alpha \mathbf{c}_\text{sent}}\)</span>, where here <span class="math inline">\(\alpha\)</span> is a pre-selected <em>steering coefficient</em> controlling the intensity of the intervention, is sufficient to generate a text exhibiting the opposite sentiment label <span class="citation" data-cites="hollinsworth-etal-2024-language">(<a href="references.html#ref-hollinsworth-etal-2024-language" role="doc-biblioref">Tigges et al., 2024</a>)</span>. This simple procedure, known as <em>activation addition</em>, has become popular for conditioning desired attributes in model generations, including multiple properties at once <span class="citation" data-cites="scalena-etal-2024-multi">(<a href="references.html#ref-scalena-etal-2024-multi" role="doc-biblioref">Scalena et al., 2024</a>)</span>. Some of its variants omit probing classifiers and employ other unsupervised methods for computing feature directions, such as K-Means clustering of representations for examples showing a desired property <span class="citation" data-cites="zou-etal-2024-enhancing">(<a href="references.html#ref-zou-etal-2024-enhancing" role="doc-biblioref">Zou et al., 2024</a>)</span>, or mean difference between representations for positive and negative sets of demonstrations <span class="citation" data-cites="marks-tegmark-2024-geometry arditi-etal-2024-refusal">(<a href="references.html#ref-marks-tegmark-2024-geometry" role="doc-biblioref">Marks and Tegmark, 2024</a>; <a href="references.html#ref-arditi-etal-2024-refusal" role="doc-biblioref">Arditi et al., 2024</a>)</span>.</p>
<p><span class="citation" data-cites="wu-etal-2024-reft">Wu et al. (<a href="references.html#ref-wu-etal-2024-reft" role="doc-biblioref">2024</a>)</span> describe a broader framework for representation steering, proposing the use of <em>learnable interventions</em> for conditioning generation at specific steps with variable intensity. Formally, an intervention <span class="math inline">\(I\)</span> can be defined as a tuple composed by an <em>intervention function</em> <span class="math inline">\(\xi: \mathbb{R}^d \to \mathbb{R}^d\)</span> with learnable parameters, a set of input positions <span class="math inline">\(P \subseteq \{1, \dots, S\}\)</span> that the intervention is applied to and the layer <span class="math inline">\(l\)</span> at which the intervention is applied. This framework, dubbed <em>representation fine-tuning</em> (<span class="smallcaps">ReFT</span>), allows to learn interventions overriding <span class="math inline">\(\mathbf{z}^l\)</span> as:</p>
<p><span id="eq-chap2-reft-intervention"><span class="math display">\[z^l_i = \begin{cases}
\xi(\mathbf{z}^l_i), &amp; \text{if}\; i \in P \\
\mathbf{z}^l_i, &amp; \text{otherwise}
\end{cases}
\tag{2.16}\]</span></span></p>
<p>The intervention function can be learned by minimizing the normal cross-entropy loss with a next token prediction objective, optimizing only the parameters of the intervention function. Activation addition (ActAdd) can then be described as a special case in this broader framework, where the intervention function <span class="math inline">\(\xi\)</span> is constant and applied at all generation steps. In the experiments of <a href="chap-7-sae-litmt.html" class="quarto-xref"><span>Chapter 7</span></a>, we use ActAdd and <span class="smallcaps">ReFT</span> as baselines for our proposed steering method.</p>
<p>The final steering approach we discuss in this section involves the use of <em>sparse autoencoders</em>[SAEs; <span class="citation" data-cites="huben-etal-2024-sparse">Huben et al. (<a href="references.html#ref-huben-etal-2024-sparse" role="doc-biblioref">2024</a>)</span>] for conditioning model behavior. SAEs have become widely adopted for analyzing the representations learned by transformer LMs thanks to their ability to address <em>polysemanticity</em>, i.e.&nbsp;the entanglement of multiple concepts within learned model representations. Indeed, neurons in transformer LMs were observed to activate on diverse and semantically distinct contexts, with concepts being encoded in a distributed manner across multiple units <span class="citation" data-cites="smolensky-1986-neural olah-2023-distributed">(<a href="references.html#ref-smolensky-1986-neural" role="doc-biblioref">Smolensky, 1986</a>; <a href="references.html#ref-olah-2023-distributed" role="doc-biblioref">Olah, 2023</a>)</span>. In light of this, and given the disparity between the relatively low-dimensional representations learned by transformer LMs and the vast array of abilities they acquire during training, latent concept representations were speculated to be encoded in <em>superposition</em> across various model units <span class="citation" data-cites="arora-etal-2018-linear">(<a href="references.html#ref-arora-etal-2018-linear" role="doc-biblioref">Arora et al., 2018</a>)</span>, i.e.&nbsp;that multiple neurons jointly encode the presence of a single concept (<a href="#fig-chap2-sae" class="quarto-xref">Figure&nbsp;<span>2.5</span></a>, left). A concrete example of this phenomenon is given by <span class="citation" data-cites="elhage-etal-2022-toy">Elhage et al. (<a href="references.html#ref-elhage-etal-2022-toy" role="doc-biblioref">2022</a>)</span>, where superposition is observed in presence of a long tail of sparse concepts in the training dataset.</p>
<div id="fig-chap2-sae" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="t">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-chap2-sae-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../figures/chap-2-background/sae.webp" class="img-fluid figure-img" data-fig-pos="t">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-chap2-sae-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.5: <strong>Left:</strong> Concepts encoded in a 2-dimensional parameter space. (a) Polysemanticity can be observed when concept do not align with the standard basis, i.e.&nbsp;they are encoded jointly by multiple units. (b) If concepts align perfectly with neurons, these neurons are <em>monosemantic</em>. (c) When the number of concepts exceeds the number of parameters, polysemanticity is inevitable and <em>superposition</em> is observed. <strong>Right:</strong> Sparse autoencoder (SAE) trained to reconstruct a model’s internal representations <span class="math inline">\(\mathbf{z}\)</span>. Interpretable SAE concepts are found in rows of <span class="math inline">\(\mathbf{W}_{\text{dec}}\)</span>. Biases are omitted for clarity.
</figcaption>
</figure>
</div>
<p>A possible strategy to disentangle concepts in superposition involves finding an overcomplete feature basis via dictionary learning <span class="citation" data-cites="olshausen-field-1997-sparse donoho-elad-2003-optimally">(<a href="references.html#ref-olshausen-field-1997-sparse" role="doc-biblioref">Olshausen and Field, 1997</a>; <a href="references.html#ref-donoho-elad-2003-optimally" role="doc-biblioref">Donoho and Elad, 2003</a>)</span>. SAEs are simple autoencoder neural networks, i.e.&nbsp;models trained to reconstruct their input, that can be trained to reconstruct internal representations <span class="math inline">\(\mathbf{z} \in \mathbb{R}^{d}\)</span> of a neural network exhibiting superposition. Their training objective encourages the model to learn a sparse coding of the input representation through an ad-hoc loss term, resulting in a sparse dictionary of learned concepts. <span class="citation" data-cites="huben-etal-2024-sparse">Huben et al. (<a href="references.html#ref-huben-etal-2024-sparse" role="doc-biblioref">2024</a>)</span> and <span class="citation" data-cites="bricken-etal-2023-monosemanticity">Bricken et al. (<a href="references.html#ref-bricken-etal-2023-monosemanticity" role="doc-biblioref">2023</a>)</span> propose training SAEs on transformer LM representations using the form:</p>
<p><span id="eq-chap2-sae"><span class="math display">\[
\begin{aligned}
    \text{SAE}(\mathbf{z}) &amp;= h(\mathbf{z})\,\mathbf{W}_{\text{dec}} + \mathbf{b}_{\text{dec}} \\
    \text{with}\; h(\mathbf{z}) &amp;= \sigma\big((\mathbf{z} - \mathbf{b}_{\text{dec}})\mathbf{W}_{\text{enc}} + \mathbf{b}_{\text{enc}}\big) \\
\end{aligned}
\tag{2.17}\]</span></span></p>
<p>using the loss function:</p>
<p><span id="eq-chap2-sae-loss"><span class="math display">\[\mathcal{L}(\mathbf{z}) = \|\mathbf{z} - \text{SAE}(\mathbf{z})\|_2^2 + \alpha \|h(\mathbf{z})\|_1 \tag{2.18}\]</span></span></p>
<p>where <span class="math inline">\(\sigma\)</span> is a non-linear activation function, <span class="math inline">\(\mathbf{W}_{\text{enc}}\)</span> and <span class="math inline">\(\mathbf{W}_{\text{dec}}\)</span> are the encoder and decoder learned weight matrices, respectively, and <span class="math inline">\(\alpha\)</span> is a hyperparameter controlling the sparsity of the learned representation. The first term in <a href="#eq-chap2-sae-loss" class="quarto-xref">Equation&nbsp;<span>2.18</span></a> is the <em>reconstruction term</em>, accounting for the quality of reconstruction, while the second term is the <em>sparsity term</em>, which promotes sparsity. The SAE architecture is illustrated in <a href="#fig-chap2-sae" class="quarto-xref">Figure&nbsp;<span>2.5</span></a> (right).</p>
<p>If <span class="math inline">\(h(\mathbf{z}) \in \mathbb{R}^{m}\)</span> and <span class="math inline">\(m \gg d\)</span>, <span class="math inline">\(\mathbf{z}\)</span> can be approximated as a sparse linear combination of the learned rows in the dictionary <span class="math inline">\({\mathbf{W}_{\text{dec}} \in \mathbb{R}^{m \times d}}\)</span>, ideally representing monosemantic concepts. Similarly to activation addition, these concepts can be used to steer model behavior by scaling them using a steering coefficient before reconstruction, resulting in a modified representation <span class="math inline">\(\tilde{\mathbf{z}}\)</span>. We use a similar approach in our SAE-based steering method we present in <a href="chap-7-sae-litmt.html" class="quarto-xref"><span>Chapter 7</span></a>.</p>
</section>
</section>
<section id="sec-chap2-mt" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="sec-chap2-mt"><span class="header-section-number">2.4</span> Machine Translation</h2>
<p>Machine translation is a long-standing task in natural language processing, with the goal of automatically translating text from a <em>source</em> language to another <em>target</em> language. In this section, we provide a brief overview of the evolution of machine translation approaches, describe how transformer LM architectures are commonly used for machine translation, and how such models can handle multiple languages and contextual information.</p>
<p>The history of machine translation can be summarized in three main phases. Between the 1960s and the 1980s, the first successes of machine translation were attained by <strong>rule-based</strong> systems exploiting various techniques, ranging from direct translation using dictionaries with a set of reordering rules to ambitious methods aiming to exploit an <em>interlingua</em> to act as a bridge when mapping meaning across languages <span class="citation" data-cites="hutchins-2001-machine">(<a href="references.html#ref-hutchins-2001-machine" role="doc-biblioref">Hutchins, 2001</a>)</span>. As for most rule-based methods, however, these approaches were limited by the need of ad-hoc rules, which could hardly account for less frequent and challenging settings. From the 1990s onwards, the <strong>statistical</strong> paradigm took foot by exploiting large bilingual corpora made available by the birth of the World Wide Web to train statistical language models parametrized as tables of co-occurrence probabilities <span class="citation" data-cites="och-etal-1999-improved">(<a href="references.html#ref-och-etal-1999-improved" role="doc-biblioref">Och et al., 1999</a>)</span>, with popular approaches aiming to segment challenging sentences into simpler phrases for ease of translation via co-occurrences <span class="citation" data-cites="koehn-etal-2003-statistical">(<a href="references.html#ref-koehn-etal-2003-statistical" role="doc-biblioref">Koehn et al., 2003</a>)</span> or syntactic analysis <span class="citation" data-cites="hadiwinoto-2017-book">(<a href="references.html#ref-hadiwinoto-2017-book" role="doc-biblioref">Hadiwinoto, 2017</a>)</span>. In 2013, the advent of word embeddings coincided with the first MT systems based on continuous language representations parametrized by neural networks <span class="citation" data-cites="kalchbrenner-blunsom-2013-recurrent">(<a href="references.html#ref-kalchbrenner-blunsom-2013-recurrent" role="doc-biblioref">Kalchbrenner and Blunsom, 2013</a>)</span>, marking the advent of the <strong>neural</strong> MT (NMT) paradigm that remains the current state-of-the-art for machine translation. While the architecture of NMT systems has barely changed since the introduction of the transformer, as for most NLP tasks the introduction of large pre-trained language models has led to general-purpose models able to handle various translation-related task via light tuning and ad-hoc prompting <span class="citation" data-cites="alves-etal-2024-tower">(<a href="references.html#ref-alves-etal-2024-tower" role="doc-biblioref">Alves et al., 2024</a>)</span>.</p>
<p>Provided that machine translation involves the generation of a sequence of translated target tokens, it is straightforward to see how such task can fit well into the sequence-to-sequence framework adopted by neural language models. Given a sequence of tokens <span class="math inline">\(\mathbf{x} = (x_1, x_2, \ldots, x_{S_s})\)</span> in the source language <span class="math inline">\(s\)</span>, a language model can be trained to generate a sequence of target tokens <span class="math inline">\(\mathbf{y} = (y_1, y_2, \ldots, y_{S_t})\)</span> in the target language <span class="math inline">\(t\)</span> using the classic cross-entropy loss function. The transformer module we presented in <a href="#sec-chap2-nlm-lm" class="quarto-xref"><span>Section 2.1.3</span></a> corresponds to the decoder-only architecture currently preferred for language modeling, involving a single stack of blocks. However, the original model proposed by <span class="citation" data-cites="vaswani-etal-2017-attention">Vaswani et al. (<a href="references.html#ref-vaswani-etal-2017-attention" role="doc-biblioref">2017</a>)</span> followed the traditional <em>encoder-decoder</em> structure adopted in MT, with an additional dedicated component for encoding source information and influencing the generation of target tokens.</p>
<p>The encoder-decoder transformer architecture for machine translation is illustrated in <a href="#fig-chap2-encdec" class="quarto-xref">Figure&nbsp;<span>2.6</span></a>. The encoder processes the source sentence <span class="math inline">\(\mathbf{x}\)</span> and produces a sequence of contextualized representations <span class="math inline">\(\mathbf{Z}^{L_{\text{enc}}}_{\text{enc}} \in \mathbb{R}^{S_s \times d_\text{enc}}\)</span> capturing the meaning of the source sentence. When generating the <span class="math inline">\(i\)</span>-th token in the target sentence, every block of the decoder then attends to the target prefix <span class="math inline">\(\mathbf{y}_{&lt;i}\)</span> using the self-attention module (MHSA) presented in <a href="#sec-chap2-nlm-transformers" class="quarto-xref"><span>Section 2.1.2</span></a>, and complements this with a <em>multi-head cross-attention</em> (MHCA) mechanism integrating information from encoder representations <span class="math inline">\(\mathbf{Z}^{L_{\text{enc}}}_{\text{enc}}\)</span>. Functionally, the cross-attention module is identical to self-attention, but employs encoder representations to generate key and value vectors, while the query vectors are generated from the decoder representations.</p>
<div id="fig-chap2-encdec" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="t">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-chap2-encdec-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../figures/chap-2-background/nmt_encoder_decoder.webp" class="img-fluid figure-img" data-fig-pos="t">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-chap2-encdec-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.6: Transformer encoder-decoder architecture for neural machine translation. The encoder processes the source sentence and produces a sequence of contextualized representations, while the decoder generates the target sentence using causal self-attention (MHSA) and cross-attention (MHCA) mechanisms. The last decoder state is projected to the vocabulary space by the prediction head, and the next word is selected.
</figcaption>
</figure>
</div>
<p>While encoder-decoder transformers were traditionally trained from scratch on the machine translation task, the current state-of-the-art adapts pre-trained decoder-only LLMs with ad-hoc supervised tuning <span class="citation" data-cites="cui-etal-2025-multilingual rei-etal-2024-tower xu-etal-2024-paradigm">(<a href="references.html#ref-cui-etal-2025-multilingual" role="doc-biblioref">Cui et al., 2025</a>; <a href="references.html#ref-rei-etal-2024-tower" role="doc-biblioref">Rei et al., 2024</a>; <a href="references.html#ref-xu-etal-2024-paradigm" role="doc-biblioref">Xu et al., 2024</a>)</span>. Our experiments reflect this paradigm shift: initial MT experiments in <a href="chap-4-pecore.html" class="quarto-xref"><span>Chapter 4</span></a>, <a href="chap-8-divemt.html" class="quarto-xref"><span>Chapter 8</span></a> and <a href="chap-9-qe4pe.html" class="quarto-xref"><span>Chapter 9</span></a> employ traditional encoder-decoder, single-purpose translation models, while in <a href="chap-6-ramp.html" class="quarto-xref"><span>Chapter 6</span></a> and <a href="chap-7-sae-litmt.html" class="quarto-xref"><span>Chapter 7</span></a> we generate translations by prompting general-purpose LLMs. Finally, <a href="chap-10-unsup-wqe.html" class="quarto-xref"><span>Chapter 10</span></a> evaluates methods on both model types.</p>
<p><span class="paragraph">Multilingual machine translation</span> Even before the advent of LLM-based translation systems, an important trend in MT research involved the training of massively multilingual MT (MMT) models capable of producing direct translations across hundreds of translation directions <span class="citation" data-cites="aharoni-etal-2019-massively">(<a href="references.html#ref-aharoni-etal-2019-massively" role="doc-biblioref">Aharoni et al., 2019</a>)</span>. Such approach was shown to bring improvements over previous methods requiring an intermediate translation step into a high-resource <em>pivot language</em> when two less-resourced languages were used as source and target <span class="citation" data-cites="kim-etal-2019-pivot">(<a href="references.html#ref-kim-etal-2019-pivot" role="doc-biblioref">Kim et al., 2019</a>)</span>. MMT models are typically trained on large multilingual web corpora with similarity-matched sentence pairs in different languages <span class="citation" data-cites="schwenk-etal-2021-ccmatrix">(<a href="references.html#ref-schwenk-etal-2021-ccmatrix" role="doc-biblioref">Schwenk et al., 2021</a>)</span>, using special <em>language tags</em> such as <code>&lt;eng_Latn&gt;</code> as prefixes to mark source and target languages. After training, a translation into a specific language can be produced by prepending the respective language tag to the target sequence, biasing model generation towards tokens matching that language. This thesis makes ample use of encoder-decoder MMT models, such as mBART-50 <span class="citation" data-cites="tang-etal-2021-multilingual">(<a href="references.html#ref-tang-etal-2021-multilingual" role="doc-biblioref">Tang et al., 2021</a>)</span>, trained to translate from English to 50 languages (one-to-many MMT), M2M-100 <span class="citation" data-cites="fan-etal-2021-beyond">(<a href="references.html#ref-fan-etal-2021-beyond" role="doc-biblioref">Fan et al., 2021</a>)</span>, with many-to-many translation between 100 languages, and finally No Language Left Behind [NLLB; <span class="citation" data-cites="nllb-2024-scaling">NLLB Team et al. (<a href="references.html#ref-nllb-2024-scaling" role="doc-biblioref">2024</a>)</span>], covering 200 languages in all directions. Decoder-only LLMs are generally trained on variable amounts of multilingual data<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a>, and hence exhibit some degree of multilingual ability without additional MT tuning.</p>
<p><span class="paragraph">Context-aware machine translation</span> Inter-sentential context is often fundamental for resolving discourse-level ambiguities during translation <span class="citation" data-cites="muller-etal-2018-large bawden-etal-2018-evaluating voita-etal-2019-good fernandes-etal-2023-translation">(<a href="references.html#ref-muller-etal-2018-large" role="doc-biblioref">Müller et al., 2018</a>; <a href="references.html#ref-bawden-etal-2018-evaluating" role="doc-biblioref">Bawden et al., 2018</a>; <a href="references.html#ref-voita-etal-2019-good" role="doc-biblioref">Voita et al., 2019</a>; <a href="references.html#ref-fernandes-etal-2023-translation" role="doc-biblioref">Fernandes et al., 2023b</a>)</span>. Traditional MT systems were trained at <em>segment level</em> due to their limited ability in handling long context, potentially losing important contextual information that spans beyond sentence boundaries, resulting in lower performances in realistic settings <span class="citation" data-cites="laubli-etal-2018-machine toral-etal-2018-attaining">(<a href="references.html#ref-laubli-etal-2018-machine" role="doc-biblioref">Läubli et al., 2018</a>; <a href="references.html#ref-toral-etal-2018-attaining" role="doc-biblioref">Toral et al., 2018a</a>)</span>. <em>Context-aware MT</em> approaches aimed to address this limitation by incorporating document-level information to improve translation quality and consistency, leading to improved performance when translating cohesive discourse phenomena such as anaphora resolution, lexical cohesion, and maintaining consistent terminology within a document <span class="citation" data-cites="voita-etal-2018-context maruf-haffari-2018-document">(<a href="references.html#ref-voita-etal-2018-context" role="doc-biblioref">Voita et al., 2018</a>; <a href="references.html#ref-maruf-haffari-2018-document" role="doc-biblioref">Maruf and Haffari, 2018</a>)</span>. Initial context-aware approaches for NMT employed methods ranging from concatenating multiple source sentences to employing hierarchical attention mechanisms that explicitly model document structure <span class="citation" data-cites="miculicich-etal-2018-document zhang-etal-2018-improving">(<a href="references.html#ref-miculicich-etal-2018-document" role="doc-biblioref">Miculicich et al., 2018</a>; <a href="references.html#ref-zhang-etal-2018-improving" role="doc-biblioref">Zhang et al., 2018</a>)</span>. We use one such methods, namely concatenating context and current source text using a special <code>&lt;brk&gt;</code> tag, for the NMT models we analyze in <a href="chap-4-pecore.html" class="quarto-xref"><span>Chapter 4</span></a>. Recent LLM-based translation systems can naturally process longer contexts and maintain better consistency across document boundaries <span class="citation" data-cites="wang-etal-2023-document-level briakou-etal-2024-translating">(<a href="references.html#ref-wang-etal-2023-document-level" role="doc-biblioref">Wang et al., 2023</a>; <a href="references.html#ref-briakou-etal-2024-translating" role="doc-biblioref">Briakou et al., 2024</a>)</span>.</p>
</section>
<section id="sec-chap2-pe" class="level2" data-number="2.5">
<h2 data-number="2.5" class="anchored" data-anchor-id="sec-chap2-pe"><span class="header-section-number">2.5</span> MT Post-Editing and Evaluation</h2>
<p>The landscape of machine translation has undergone a fundamental transformation in recent decades, shifting from a tool primarily designed for professional translators to a technology accessed by millions of lay users worldwide <span class="citation" data-cites="savoldi-etal-2025-translation">(<a href="references.html#ref-savoldi-etal-2025-translation" role="doc-biblioref">Savoldi et al., 2025</a>)</span>. In this section, we review MT post-editing tools and practices, and discuss how MT outputs are evaluated by means of automatic metrics and human annotators.</p>
<section id="post-editing-mt" class="level3" data-number="2.5.1">
<h3 data-number="2.5.1" class="anchored" data-anchor-id="post-editing-mt"><span class="header-section-number">2.5.1</span> Post-editing MT</h3>
<p>Since the inception of MT technologies in professional translation workflow, human post-editing has been a crucial step to ensure quality and mitigate potential critical errors, especially for low-resource settings <span class="citation" data-cites="wagner-1983-rapid church-hovy-1993-good">(<a href="references.html#ref-wagner-1983-rapid" role="doc-biblioref">Wagner, 1983</a>; <a href="references.html#ref-church-hovy-1993-good" role="doc-biblioref">Church and Hovy, 1993</a>)</span>. The industry distinguishes between two primary post-editing levels: <em>light post-editing</em>, which focuses on correcting only critical errors affecting comprehension while tolerating stylistic imperfections, and <em>full post-editing</em>, which aims to achieve human translation quality standards. The choice between these approaches involves trade-offs between effort investment and quality requirements, with light post-editing being faster while maintaining acceptable quality for many use cases <span class="citation" data-cites="plitt-masselot-2010-productivity">(<a href="references.html#ref-plitt-masselot-2010-productivity" role="doc-biblioref">Plitt and Masselot, 2010</a>)</span>. Seminal post-editing studies highlighted an increase in translators’ productivity following MT adoption <span class="citation" data-cites="guerberof-2009-productivity green-etal-2013-efficacy laubli-etal-2013-assessing plitt-masselot-2010-productivity parra-escartin-arcedillo-2015-machine">(<a href="references.html#ref-guerberof-2009-productivity" role="doc-biblioref">Guerberof, 2009</a>; <a href="references.html#ref-green-etal-2013-efficacy" role="doc-biblioref">Green et al., 2013</a>; <a href="references.html#ref-laubli-etal-2013-assessing" role="doc-biblioref">Läubli et al., 2013</a>; <a href="references.html#ref-plitt-masselot-2010-productivity" role="doc-biblioref">Plitt and Masselot, 2010</a>; <a href="references.html#ref-parra-escartin-arcedillo-2015-machine" role="doc-biblioref">Parra Escartín and Arcedillo, 2015</a>)</span>. However, they also struggled to identify generalizable findings due to confounding factors like output quality, content domains, and high variance across language pairs and human subjects. With the advent of NMT, productivity gains of the new approach were extensively compared to those of statistical MT <span class="citation" data-cites="castilho-etal-2017-neural bentivogli-etal-2016-neural toral-etal-2018-postediting laubli-etal-2019-post">(<a href="references.html#ref-castilho-etal-2017-neural" role="doc-biblioref">Castilho et al., 2017</a>; <a href="references.html#ref-bentivogli-etal-2016-neural" role="doc-biblioref">Bentivogli et al., 2016</a>; <a href="references.html#ref-toral-etal-2018-postediting" role="doc-biblioref">Toral et al., 2018b</a>; <a href="references.html#ref-laubli-etal-2019-post" role="doc-biblioref">Läubli et al., 2019</a>)</span>. Initial results were promising for NMT due to its better fluency and overall results. Moreover, translators were shown to prefer NMT over SMT for post-editing, although a pronounced productivity increase was not always present. In more recent times, various works explored the usage of adaptive MT systems that learn from post-editing feedback in real-time <span class="citation" data-cites="turchi-etal-2017-continuous karimova-etal-2018-user">(<a href="references.html#ref-turchi-etal-2017-continuous" role="doc-biblioref">Turchi et al., 2017</a>; <a href="references.html#ref-karimova-etal-2018-user" role="doc-biblioref">Karimova et al., 2018</a>)</span>, with the goal of progressively reducing repetitive corrections and adapting to translator preferences. Notably, recent estimates confirm that human-machine collaboration can match or even exceed the quality of human-only translations, with potential cost reductions estimated at around 60% the price of full human post-editing <span class="citation" data-cites="liu-etal-2024-beyond-human">(<a href="references.html#ref-liu-etal-2024-beyond-human" role="doc-biblioref">Liu et al., 2024</a>)</span>.</p>
<p>The main metric of evaluation for post-editing in the industry is <em>productivity</em>, often operationalized as the amount of source characters or word revised per minute. On the other hand, post-editing research often complements productivity measurements with <em>editing effort</em> alongside its <em>temporal</em>, <em>technical</em> and <em>cognitive</em> components <span class="citation" data-cites="krings-2001-repairing">(<a href="references.html#ref-krings-2001-repairing" role="doc-biblioref">Krings, 2001</a>)</span>, corresponding to editing time, number of keystrokes and pauses between keystrokes during the editing process, respectively. Importantly, the cognitive and temporal demands of post-editing were found to vary significantly depending on various factors, such as error types and user expertise. For example, <span class="citation" data-cites="daems-etal-2017-identifying">Daems et al. (<a href="references.html#ref-daems-etal-2017-identifying" role="doc-biblioref">2017</a>)</span> found that certain error categories have disproportionate impacts on post-editing effort, with adequacy errors often requiring more cognitive resources than fluency errors, even though the latter may be more immediately apparent to users <span class="citation" data-cites="martindale-carpuat-2018-fluency">(<a href="references.html#ref-martindale-carpuat-2018-fluency" role="doc-biblioref">Martindale and Carpuat, 2018</a>)</span>. Domain-specific considerations further complicate this landscape, as technical domains may tolerate certain stylistic variations while requiring precise terminology, whereas literary translation may prioritize creative renditions of meaning <span class="citation" data-cites="guerberof-toral-2022-creativity">(<a href="references.html#ref-guerberof-toral-2022-creativity" role="doc-biblioref">Guerberof-Arenas and Toral, 2022</a>)</span>.</p>
<p>Professional translators typically post-edit texts through <em>computer-assisted translation</em> (CAT) tools, which are interfaces designed to enhance human translators’ productivity by providing access to keyboard shortcuts, quality estimation (which we discuss in <a href="#sec-chap2-qe" class="quarto-xref"><span>Section 2.6</span></a>) and other assistive technologies <span class="citation" data-cites="bowker-2002-computer">(<a href="references.html#ref-bowker-2002-computer" role="doc-biblioref">Bowker, 2002</a>)</span>. A common functionality of CATs is the integration of <em>translation memories</em> (TMs), which are bilingual databases storing previously translated content that can be retrieved and reused for similar segments, mimicking the functioning of early example-based MT systems <span class="citation" data-cites="garcia-2009-beyond">(<a href="references.html#ref-garcia-2009-beyond" role="doc-biblioref">Garcia, 2009</a>)</span>. Additional features often include terminology management systems (<em>termbases</em>) for maintaining consistency in technical terms and brand names, automatic text segmentation, and quality assurance modules such as spellcheckers for detecting errors and inconsistencies. Modern CAT tools have evolved from standalone desktop software to cloud-based platforms accessible via web browsers <span class="citation" data-cites="moran-etal-2014-towards federico-etal-2014-matecat-open">(<a href="references.html#ref-moran-etal-2014-towards" role="doc-biblioref">Moran et al., 2014</a>; <a href="references.html#ref-federico-etal-2014-matecat-open" role="doc-biblioref">Federico et al., 2014</a>)</span>, with recent surveys indicating that 88% of professional translators use at least one CAT tool for their work.<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a> While many CAT tools nowadays offer multiple advanced features, including LLM-based AI assistants, in our user studies of <a href="chap-8-divemt.html" class="quarto-xref"><span>Chapter 8</span></a> and <a href="chap-9-qe4pe.html" class="quarto-xref"><span>Chapter 9</span></a>, we employ simple research-oriented interfaces with minimal text editing functionalities to ensure equal proficiency across subjects. In <a href="chap-8-divemt.html" class="quarto-xref"><span>Chapter 8</span></a> we employ PET <span class="citation" data-cites="aziz-etal-2012-pet">(<a href="references.html#ref-aziz-etal-2012-pet" role="doc-biblioref">Aziz et al., 2012</a>)</span>, a simple desktop-based post-editing tool supporting various languages, while in <a href="chap-9-qe4pe.html" class="quarto-xref"><span>Chapter 9</span></a> we use a custom-built web interface supporting editing over highlighted error spans.</p>
</section>
<section id="mt-evaluation" class="level3" data-number="2.5.2">
<h3 data-number="2.5.2" class="anchored" data-anchor-id="mt-evaluation"><span class="header-section-number">2.5.2</span> MT Evaluation</h3>
<p>The industrial context had historically an important influence on MT evaluation practices, encouraging researchers to focus on evaluation efficiency, combining automatic metrics with human assessment, and metrics that could provide concrete benefits when employed in professional translation workflows.</p>
<p><span class="paragraph">Automatic MT Metrics.</span> Automatic evaluation metrics for machine translation have been widely adopted since the early 2000s, with the most popular metrics being BLEU <span class="citation" data-cites="papineni-etal-2002-bleu">(<a href="references.html#ref-papineni-etal-2002-bleu" role="doc-biblioref">Papineni et al., 2002</a>)</span>. BLEU is a simple and inexpensive metric measuring lexical similarity between a <em>candidate</em> translation <span class="math inline">\(\hat y\)</span> and its given <em>reference</em> <span class="math inline">\(y\)</span> as the number of <span class="math inline">\(n\)</span>-grams <span class="math inline">\(G_n = {\hat y_1, \dots, \hat y_n, \hat y_2, \dots, \hat y_{n+1}, \dots}\)</span> shared between them, normalized by the total n-gram count:</p>
<p><span class="math display">\[p_n(y, \hat y) = \frac{\sum_{s \in G_n} \min(C(s,\hat y), C(s,y))}{\sum_{s \in G_n} C(s,\hat y)}\]</span></p>
<p>where <span class="math inline">\(C(s, y)\)</span> is the count of n-gram <span class="math inline">\(s\)</span> in sequence <span class="math inline">\(y\)</span>. The complete BLEU score also incorporates a brevity penalty to discourage overly short translations. BLEU is computed at segment-level for an entire corpus of candidate and reference translations, and averaged to obtain a corpus-level score. Multiple variants of BLEU have been proposed to account for length bias, multiple references, with other metrics such as chrF <span class="citation" data-cites="popovic-2015-chrf">(<a href="references.html#ref-popovic-2015-chrf" role="doc-biblioref">Popović, 2015</a>)</span> adopting similar lexicon-based approaches at the character level, or aligning n-grams across the two sequences <span class="citation" data-cites="banerjee-lavie-2005-meteor">(<a href="references.html#ref-banerjee-lavie-2005-meteor" role="doc-biblioref">Banerjee and Lavie, 2005</a>)</span>. Other lexical metrics such as the Translation Error Rate <span class="citation" data-cites="snover-etal-2006-study">(<a href="references.html#ref-snover-etal-2006-study" role="doc-biblioref">Snover et al., 2006</a>)</span> or Word Error Rate (WER) have been used to connect the quality of the candidate sequence to the number of edits required to convert it into the reference, grounding the evaluation in post-editing technical effort. While these metrics provide rapid assessment of translation quality with minimal computational overhead, they suffer from several limitations: sensitivity to lexical variations that may not reflect translation quality differences, poor correlation with human judgments for high-quality neural MT outputs, and limited generalization across different writing systems <span class="citation" data-cites="bugliarello-etal-2020-easier">(<a href="references.html#ref-bugliarello-etal-2020-easier" role="doc-biblioref">Bugliarello et al., 2020</a>)</span>.</p>
<p>Following calls from the MT research community <span class="citation" data-cites="freitag-etal-2022-results">(<a href="references.html#ref-freitag-etal-2022-results" role="doc-biblioref">Freitag et al., 2022</a>)</span>, the limitations of lexical metrics led to the widespread adoption of <em>learned metrics</em> trained to predict translation quality from large amounts of annotated examples. Most of the widely used learned MT metrics employ transformer-based encoder-only pretrained LMs such as BERT <span class="citation" data-cites="devlin-etal-2019-bert">(<a href="references.html#ref-devlin-etal-2019-bert" role="doc-biblioref">Devlin et al., 2019</a>)</span> or the cross-lingual model XLM <span class="citation" data-cites="conneau-lample-2019-cross">(<a href="references.html#ref-conneau-lample-2019-cross" role="doc-biblioref">Conneau and Lample, 2019</a>)</span>. Among the most notable metrics, <span class="smallcaps">Bleurt</span> <span class="citation" data-cites="sellam-etal-2020-bleurt">(<a href="references.html#ref-sellam-etal-2020-bleurt" role="doc-biblioref">Sellam et al., 2020</a>)</span> is a BERT-based model using multi-task loss on synthetic data to perform regression of human quality judgments, while <span class="smallcaps">comet</span> <span class="citation" data-cites="rei-etal-2020-comet">(<a href="references.html#ref-rei-etal-2020-comet" role="doc-biblioref">Rei et al., 2020</a>)</span> feeds source text, candidate and reference translation triples to a dual cross-lingual encoder structure that jointly learns to estimate quality and rank multiple candidate translations. In most of our MT evaluations we employ the <span class="smallcaps">comet</span> metric due to its excellent performance across hundreds of languages, which resulted in top-scoring submissions at multiple WMT metrics shared tasks <span class="citation" data-cites="rei-etal-2020-comet rei-etal-2021-references rei-etal-2022-comet">(<a href="references.html#ref-rei-etal-2020-comet" role="doc-biblioref">Rei et al., 2020</a>; <a href="references.html#ref-rei-etal-2021-references" role="doc-biblioref">Rei et al., 2021</a>; <a href="references.html#ref-rei-etal-2022-comet" role="doc-biblioref">Rei et al., 2022a</a>)</span>.<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a> However, learned metrics introduce their own challenges, including non-trivial computational requirements, potential biases inherited from training data, and questions about generalization to out-of-domain content <span class="citation" data-cites="amrhein-sennrich-2022-identifying">(<a href="references.html#ref-amrhein-sennrich-2022-identifying" role="doc-biblioref">Amrhein and Sennrich, 2022</a>)</span></p>
<p><span class="paragraph">Human evaluation of MT.</span> Human evaluation, despite its challenges due to inconsistencies across annotators, cultural and linguistic biases, and high costs, remains the gold standard for assessing machine translation quality, providing crucial insights that automatic metrics may fail to capture <span class="citation" data-cites="freitag-etal-2021-experts">(<a href="references.html#ref-freitag-etal-2021-experts" role="doc-biblioref">Freitag et al., 2021</a>)</span>. Historically, human assessment of MT was centered around the notions of <em>adequacy</em> (also accuracy or fidelity), <em>comprehensibility</em> and <em>fluency</em> (or grammaticality) <span class="citation" data-cites="white-etal-1994-arpa callison-burch-etal-2007-meta">(<a href="references.html#ref-white-etal-1994-arpa" role="doc-biblioref">White et al., 1994</a>; <a href="references.html#ref-callison-burch-etal-2007-meta" role="doc-biblioref">Callison-Burch et al., 2007</a>)</span>, with adequacy measuring how well the original meaning is conveyed, comprehensibility reflecting how understandable MT is without the original source, and fluency judging whether appropriate target grammar is employed <span class="citation" data-cites="popovic-2020-informative">(<a href="references.html#ref-popovic-2020-informative" role="doc-biblioref">Popović, 2020</a>)</span>. MT evaluation campaigns since 2017 adopted a <em>continuous direct assessment</em> (DA) of translation quality using scalar ratings— for example, using a 0-100 scale as in <span class="citation" data-cites="graham-etal-2013-continuous">Graham et al. (<a href="references.html#ref-graham-etal-2013-continuous" role="doc-biblioref">2013</a>)</span> —or comparative ranking of multiple system outputs <span class="citation" data-cites="bojar-etal-2017-findings">(<a href="references.html#ref-bojar-etal-2017-findings" role="doc-biblioref">Bojar et al., 2017</a>)</span>.</p>
<p>More recently, the introduction of the Multidimensional Quality Metric (MQM) <span class="citation" data-cites="burchardt-2013-multidimensional">(<a href="references.html#ref-burchardt-2013-multidimensional" role="doc-biblioref">Lommel et al., 2013</a>)</span> has provided more structured evaluation protocols. MQM is an established framework allowing annotators to identify and categorize specific spans in a translated text as accuracy, fluency, and style issues, and assign them a level of severity (typically, a 3-way classification into <em>minor/major/critical</em>). <span class="citation" data-cites="freitag-etal-2021-experts">Freitag et al. (<a href="references.html#ref-freitag-etal-2021-experts" role="doc-biblioref">2021</a>)</span> experiments with various scoring configurations, resulting in the scoring formula:</p>
<p><span class="math display">\[\text{MQM} = (\text{\# Major Err.} \times 5) + (\text{\# Minor Err.} \times 1) + (\text{\# Punct. Err.} \times 0.1)\]</span></p>
<p>with higher scores corresponding to worse translation, resulting in a high correlation with judgments from expert raters. However, such scheme has been criticized due to its potential length bias, with recent proposals for calibrated and non-linear scoring models accounting for similar issues <span class="citation" data-cites="lommel-etal-2024-multi">(<a href="references.html#ref-lommel-etal-2024-multi" role="doc-biblioref">Lommel et al., 2024</a>)</span>. An example description of MQM error categories and severity levels we employed for our study in <a href="chap-9-qe4pe.html" class="quarto-xref"><span>Chapter 9</span></a> is presented in <a href="chap-9-qe4pe.html#tbl-qe4pe-qa-guidelines" class="quarto-xref">Table&nbsp;<span>9.1</span></a>.</p>
<p>Recent evaluation campaigns such as WMT 2024 <span class="citation" data-cites="kocmi-etal-2024-findings">(<a href="references.html#ref-kocmi-etal-2024-findings" role="doc-biblioref">Kocmi et al., 2024a</a>)</span> have increasingly adopted the MQM protocol for their evaluation, emphasizing in particular the importance of expert vs.&nbsp;non-expert annotators, with studies showing that translation professionals provide more consistent and reliable judgments compared to crowd-sourced annotations <span class="citation" data-cites="freitag-etal-2021-experts">(<a href="references.html#ref-freitag-etal-2021-experts" role="doc-biblioref">Freitag et al., 2021</a>)</span>. The advent of large language models has introduced new challenges for human evaluation, as the quality gap between human and machine translation continues to narrow, requiring more fine-grained assessment criteria and larger annotator pools to achieve reliable results <span class="citation" data-cites="kocmi-etal-2024-findings">(<a href="references.html#ref-kocmi-etal-2024-findings" role="doc-biblioref">Kocmi et al., 2024a</a>)</span>. The main limiting factor towards the diffusion of the MQM evaluation protocol is its cost, since it involves a thorough annotation of error spans. Recently, the Error Span Annotation (ESA) protocol <span class="citation" data-cites="kocmi-etal-2024-error">(<a href="references.html#ref-kocmi-etal-2024-error" role="doc-biblioref">Kocmi et al., 2024b</a>)</span> was introduced as a potential compromise between DA and MQM ratings, soliciting annotators to provide a 0-100 quality rating only after a light pass of error span identification, without requiring a full MQM error type categorization. The error annotation is intended to prime annotators to ground their quality judgments in empirical evidence, and ESA scores were observed to correlate strongly with MQM ones, while being 32% cheaper to obtain <span class="citation" data-cites="kocmi-etal-2024-error">(<a href="references.html#ref-kocmi-etal-2024-error" role="doc-biblioref">Kocmi et al., 2024b</a>)</span>. For this reason, we adopt a variant of the ESA protocol when conducting the quality assessment phase of our QE4PE study in <a href="chap-9-qe4pe.html" class="quarto-xref"><span>Chapter 9</span></a>. <span class="citation" data-cites="zouhar-etal-2025-ai">Zouhar et al. (<a href="references.html#ref-zouhar-etal-2025-ai" role="doc-biblioref">2025</a>)</span> propose to use a language model to assist in the error span identification process, potentially further reducing the cost and effort involved in the ESA protocol.</p>
</section>
</section>
<section id="sec-chap2-qe" class="level2" data-number="2.6">
<h2 data-number="2.6" class="anchored" data-anchor-id="sec-chap2-qe"><span class="header-section-number">2.6</span> Quality Estimation for MT</h2>
<p>The automatic MT metrics presented in <a href="#sec-chap2-pe" class="quarto-xref"><span>Section 2.5</span></a> require the use of a reference translation to measure the quality of a given candidate. While effective, these metrics cannot be employed to evaluate translation candidates on the fly, for example before presenting them to human post-editors, or as a ranking procedure in advanced decoding strategies <span class="citation" data-cites="rei-etal-2022-searching">(<a href="references.html#ref-rei-etal-2022-searching" role="doc-biblioref">Rei et al., 2022b</a>)</span>. Moreover, the presence of low-quality references can lead to biased evaluations of MT quality that do not reflect the translation quality without tying it to a specific gold standard <span class="citation" data-cites="freitag-etal-2023-results">(<a href="references.html#ref-freitag-etal-2023-results" role="doc-biblioref">Freitag et al., 2023</a>)</span>. <strong>Quality estimation</strong> metrics (QE), also known as <em>reference-free</em> MT metrics, are an alternative category of techniques designed to address these limitations by predicting translation quality without requiring reference translations <span class="citation" data-cites="specia-etal-2018-quality">(<a href="references.html#ref-specia-etal-2018-quality" role="doc-biblioref">Specia et al., 2018</a>)</span>. Contrary to traditional MT evaluation, QE can be performed at various levels of granularity. On the one hand, when operating at the <em>segment</em> or <em>document</em> levels, QE methods typically returns a score between 0 and 1 reflecting the overall quality of the translation, which can be then used to guide post-editors to focus on problematic segments <span class="citation" data-cites="tamchyna-2021-deploying">(<a href="references.html#ref-tamchyna-2021-deploying" role="doc-biblioref">Tamchyna, 2021</a>)</span>. On the other hand, <em>word-level</em> QE metrics can provide more granular information about translation issues, and typically operate by marking individual words with binary <code>OK</code>/<code>BAD</code> labels or, more recently, following the severity scheme introduced by the MQM framework.</p>
<p>Initial approaches to QE were mostly based on the uncertainty extracted from MT models <span class="citation" data-cites="blatz-etal-2004-confidence specia-etal-2009-estimating">(<a href="references.html#ref-blatz-etal-2004-confidence" role="doc-biblioref">Blatz et al., 2004</a>; <a href="references.html#ref-specia-etal-2009-estimating" role="doc-biblioref">Specia et al., 2009</a>)</span>, but with time began focusing on supervised approaches involving ad-hoc model training <span class="citation" data-cites="turchi-etal-2013-coping turchi-etal-2014-adaptive kepler-etal-2019-openkiwi thompson-post-2020-automatic">(<a href="references.html#ref-turchi-etal-2013-coping" role="doc-biblioref">Turchi et al., 2013</a>; <a href="references.html#ref-turchi-etal-2014-adaptive" role="doc-biblioref">Turchi et al., 2014</a>; <a href="references.html#ref-kepler-etal-2019-openkiwi" role="doc-biblioref">Kepler et al., 2019</a>; <a href="references.html#ref-thompson-post-2020-automatic" role="doc-biblioref">Thompson and Post, 2020</a>, <em>inter alia</em>)</span>. Advances in segment- and word-level QE research are regularly assessed in annual WMT campaigns <span class="citation" data-cites="fomicheva-etal-2021-eval4nlp zerva-etal-2022-findings zerva-etal-2024-findings blain-etal-2023-findings">(<a href="references.html#ref-fomicheva-etal-2021-eval4nlp" role="doc-biblioref">Fomicheva et al., 2021</a>; <a href="references.html#ref-zerva-etal-2022-findings" role="doc-biblioref">Zerva et al., 2022</a>; <a href="references.html#ref-zerva-etal-2024-findings" role="doc-biblioref">Zerva et al., 2024</a>; <a href="references.html#ref-blain-etal-2023-findings" role="doc-biblioref">Blain et al., 2023</a>)</span>, where the best-performing QE systems have recently employed transformer-based language models trained to predict quality scores, in a fashion similar to reference-based metrics. In particular, reference-less counterparts to the <span class="smallcaps">comet</span> models were introduced for QE applications, including a smaller model for efficient inference <span class="citation" data-cites="rei-etal-2022-searching">(<a href="references.html#ref-rei-etal-2022-searching" role="doc-biblioref">Rei et al., 2022b</a>)</span>.</p>
<p>More recently, the widespread adoption of the MQM paradigm and the advances in LLM capabilities led to new QE metrics predicting quality at various granularity levels. Notably, <span class="citation" data-cites="kocmi-federmann-2023-gemba">Kocmi and Federmann (<a href="references.html#ref-kocmi-federmann-2023-gemba" role="doc-biblioref">2023</a>)</span> prompt GPT-4 with an annotation scheme mimicking MQM to produce fine-grained quality assessments, from which they derive a segment-level score, while <span class="citation" data-cites="fernandes-etal-2023-devil">Fernandes et al. (<a href="references.html#ref-fernandes-etal-2023-devil" role="doc-biblioref">2023a</a>)</span> develop a similar AutoMQM framework using the PaLM-2 LLM. While these approaches usually employ proprietary models, <span class="citation" data-cites="guerreiro-etal-2024-xcomet">Guerreiro et al. (<a href="references.html#ref-guerreiro-etal-2024-xcomet" role="doc-biblioref">2024</a>)</span> propose a state-of-the-art open-source QE model extending <span class="smallcaps">comet</span> to jointly predict quality estimation at the word and the sentence level, combining sentence-level and word-level error span prediction for improved explainability of results. <span class="smallcaps">xcomet</span> metrics come in a 3.5B (XL) and 10.7B (XXL) size and support both reference-based and reference-less usage, hence enabling usage for quality estimation purposes. Concretely, <span class="smallcaps">xcomet</span> models are transformer encoders fine-tuned from pre-trained XLMR encoders <span class="citation" data-cites="goyal-etal-2021-larger">(<a href="references.html#ref-goyal-etal-2021-larger" role="doc-biblioref">Goyal et al., 2021</a>)</span> using a mix of sentence-level Direct Assessment scores and word-level MQM error spans. We use their resulting systems for our user study of <a href="chap-9-qe4pe.html" class="quarto-xref"><span>Chapter 9</span></a> and our metric comparison in <a href="chap-10-unsup-wqe.html" class="quarto-xref"><span>Chapter 10</span></a>.</p>
<p>Aside from supervised models, a return to <em>unsupervised</em> methods exploiting models uncertainty and their internal mechanisms was brought on by <span class="citation" data-cites="fomicheva-etal-2020-unsupervised">Fomicheva et al. (<a href="references.html#ref-fomicheva-etal-2020-unsupervised" role="doc-biblioref">2020</a>)</span>. In their work, such approaches were shown to rival state-of-the-art supervised QE models in predicting translation quality at the segment level. These methods typically rely on the model’s confidence in its predictions, often using metrics such as predictive probability or the entropy of the predictive distribution to mark low-confidence tokens as potential errors. The appeal of such methods lies in their efficiency, exploiting the knowledge of the MT model for error detection without requiring additional training on expensive human annotations. While such methods have been the object of multiple studies <span class="citation" data-cites="dale-etal-2023-detecting xu-etal-2023-understanding himmi-etal-2024-enhanced leiter-etal-2024-towards">(<a href="references.html#ref-dale-etal-2023-detecting" role="doc-biblioref">Dale et al., 2023</a>; <a href="references.html#ref-xu-etal-2023-understanding" role="doc-biblioref">Xu et al., 2023</a>; <a href="references.html#ref-himmi-etal-2024-enhanced" role="doc-biblioref">Himmi et al., 2024</a>; surveyed by <a href="references.html#ref-leiter-etal-2024-towards" role="doc-biblioref">Leiter et al., 2024</a>)</span>, including a shared task dedicated to explainable QE metrics <span class="citation" data-cites="fomicheva-etal-2021-eval4nlp">(<a href="references.html#ref-fomicheva-etal-2021-eval4nlp" role="doc-biblioref">Fomicheva et al., 2021</a>)</span>, their evaluation was typically focused on segment-level evaluation quality, with word-level error spans being generally obtained by attributing the predictions of supervised segment-level metrics <span class="citation" data-cites="rubino-etal-2021-error rei-etal-2023-inside">(<a href="references.html#ref-rubino-etal-2021-error" role="doc-biblioref">Rubino et al., 2021</a>; <a href="references.html#ref-rei-etal-2023-inside" role="doc-biblioref">Rei et al., 2023</a>)</span>. By contrast, recent work on LLMs evaluates various metrics to detect errors from the generator model, without additional systems involved, both at the sentence <span class="citation" data-cites="fadeeva-etal-2023-lm">(<a href="references.html#ref-fadeeva-etal-2023-lm" role="doc-biblioref">Fadeeva et al., 2023</a>)</span> and at the token level <span class="citation" data-cites="fadeeva-etal-2024-fact">(<a href="references.html#ref-fadeeva-etal-2024-fact" role="doc-biblioref">Fadeeva et al., 2024</a>)</span>. Our evaluation of <a href="chap-10-unsup-wqe.html" class="quarto-xref"><span>Chapter 10</span></a> involves various unsupervised metrics at the word level, employing the edits from our user studies of previous chapters as sources of word-level error spans to evaluate unsupervised word-level QE methods across multiple label sets. A notable technique for unsupervised QE is <strong>Monte Carlo Dropout (MCD)</strong> <span class="citation" data-cites="gal-ghahramani-2016-dropout">(<a href="references.html#ref-gal-ghahramani-2016-dropout" role="doc-biblioref">Gal and Ghahramani, 2016</a>)</span>. The dropout mechanism <span class="citation" data-cites="dropout">(<a href="references.html#ref-dropout" role="doc-biblioref">Srivastava et al., 2014</a>)</span>, commonly used for regularization during training, is employed at inference time by MCD to produce a set of noisy predictions from a unique model, approximating Bayesian inference. For a given input <span class="math inline">\(\mathbf{x}\)</span>, <span class="math inline">\(T\)</span> forward passes are performed through the network. In each pass <span class="math inline">\(t \in T\)</span>, a different random dropout mask <span class="math inline">\(\Theta_t\)</span> is applied on model parameters, resulting in slightly different output probabilities <span class="math inline">\(p(\mathbf{x} \mid \Theta_t)\)</span>. The set of <span class="math inline">\(T\)</span> predictions <span class="math inline">\(\{p(\mathbf{x} \mid \Theta_1), \dots, p(\mathbf{x} \mid \Theta_T)\}\)</span> can be seen as samples from an approximate posterior distribution. These can be used, for example, to quantify model uncertainty as the variance of the set of probabilities for a specific token. We employ such method, showing promising performances in our evaluation of <a href="chap-10-unsup-wqe.html" class="quarto-xref"><span>Chapter 10</span></a>, to produce unsupervised error highlights for our QE4PE user study in <a href="chap-9-qe4pe.html" class="quarto-xref"><span>Chapter 9</span></a>.</p>
<p>From a practical standpoint, QE methods are widely used in the translation industry for triaging automatic translations, with integrations in popular CAT tools to present users with segment-level quality scores <span class="citation" data-cites="tamchyna-2021-deploying">(<a href="references.html#ref-tamchyna-2021-deploying" role="doc-biblioref">Tamchyna, 2021</a>)</span>. While QE usage has been found helpful to increase the confidence and speed of human assessment <span class="citation" data-cites="mehandru-etal-2023-physician zouhar-etal-2025-ai">(<a href="references.html#ref-mehandru-etal-2023-physician" role="doc-biblioref">Mehandru et al., 2023</a>; <a href="references.html#ref-zouhar-etal-2025-ai" role="doc-biblioref">Zouhar et al., 2025</a>)</span>, an incautious usage of these techniques can lead to a misplaced over-reliance on model predictions <span class="citation" data-cites="zouhar-etal-2021-backtranslation">(<a href="references.html#ref-zouhar-etal-2021-backtranslation" role="doc-biblioref">Zouhar et al., 2021</a>)</span>. Moreover, the effectiveness of QE-assisted post-editing depends critically on the accuracy of quality predictions, with inaccurate highlights potentially misleading translators and reducing overall productivity <span class="citation" data-cites="shenoy-etal-2021-investigating">(<a href="references.html#ref-shenoy-etal-2021-investigating" role="doc-biblioref">Shenoy et al., 2021</a>)</span>. Interfaces supporting word-level error highlights were developed for studying MT post-editing <span class="citation" data-cites="coppers-etal-2018-intellingo herbig-etal-2020-mmpe">(<a href="references.html#ref-coppers-etal-2018-intellingo" role="doc-biblioref">Coppers et al., 2018</a>; <a href="references.html#ref-herbig-etal-2020-mmpe" role="doc-biblioref">Herbig et al., 2020</a>)</span> and code reviewing <span class="citation" data-cites="sun-etal-2022-investigating vasconcelos-etal-2025-generation">(<a href="references.html#ref-sun-etal-2022-investigating" role="doc-biblioref">Sun et al., 2022</a>; <a href="references.html#ref-vasconcelos-etal-2025-generation" role="doc-biblioref">Vasconcelos et al., 2025</a>)</span>, with results suggesting that striking the right balance of user-provided information is fundamental to improve the editing experience and prevent cognitive overload. Our user study of <a href="chap-9-qe4pe.html" class="quarto-xref"><span>Chapter 9</span></a> is one of few works going beyond accuracy evaluations to measure the actual impact of word-level QE systems when integrated in human post-editing workflows.</p>


<div id="refs" class="references csl-bib-body" role="list" style="display: none">
<div id="ref-abnar-zuidema-2020-quantifying" class="csl-entry" role="listitem">
Samira Abnar and Willem Zuidema. 2020. <a href="https://doi.org/10.18653/v1/2020.acl-main.385">Quantifying attention flow in transformers</a>. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault, editors, <em>Proceedings of the 58th annual meeting of the association for computational linguistics</em>, pages 4190–4197, Online. Association for Computational Linguistics.
</div>
<div id="ref-achtibat-etal-2024-attnlrp" class="csl-entry" role="listitem">
Reduan Achtibat, Sayed Mohammad Vakilzadeh Hatefi, Maximilian Dreyer, Aakriti Jain, Thomas Wiegand, Sebastian Lapuschkin, and Wojciech Samek. 2024. AttnLRP: Attention-aware layer-wise relevance propagation for transformers. In <em>Proceedings of the 41st international conference on machine learning</em>, Vienna, Austria. JMLR.org.
</div>
<div id="ref-adebayo-etal-2018-sanity" class="csl-entry" role="listitem">
Julius Adebayo, Justin Gilmer, Michael Muelly, Ian Goodfellow, Moritz Hardt, and Been Kim. 2018. <a href="https://proceedings.neurips.cc/paper/2018/file/294a8ed24b1ad22ec2e7efea049b8737-Paper.pdf">Sanity checks for saliency maps</a>. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, <em>Advances in neural information processing systems</em>, volume 31, pages 9505–9515, Montréal, Canada. Curran Associates, Inc.
</div>
<div id="ref-adebayo-etal-2022-posthoc" class="csl-entry" role="listitem">
Julius Adebayo, Michael Muelly, Harold Abelson, and Been Kim. 2022. <a href="https://openreview.net/forum?id=xNOVfCCvDpM">Post hoc explanations may be ineffective for detecting unknown spurious correlation</a>. In <em>International conference on learning representations</em>.
</div>
<div id="ref-adebayo-etal-2020-debugging" class="csl-entry" role="listitem">
Julius Adebayo, Michael Muelly, Ilaria Liccardi, and Been Kim. 2020. Debugging tests for model explanations. In <em>Proceedings of the 34th international conference on neural information processing systems</em>, Red Hook, NY, USA. Curran Associates Inc.
</div>
<div id="ref-aharoni-etal-2019-massively" class="csl-entry" role="listitem">
Roee Aharoni, Melvin Johnson, and Orhan Firat. 2019. <a href="https://doi.org/10.18653/v1/N19-1388">Massively multilingual neural machine translation</a>. In Jill Burstein, Christy Doran, and Thamar Solorio, editors, <em>Proceedings of the 2019 conference of the north <span>A</span>merican chapter of the association for computational linguistics: Human language technologies, volume 1 (long and short papers)</em>, pages 3874–3884, Minneapolis, Minnesota. Association for Computational Linguistics.
</div>
<div id="ref-alammar-2021-ecco" class="csl-entry" role="listitem">
J Alammar. 2021. <a href="https://doi.org/10.18653/v1/2021.acl-demo.30">Ecco: An open source library for the explainability of transformer language models</a>. In Heng Ji, Jong C. Park, and Rui Xia, editors, <em>Proceedings of the 59th annual meeting of the association for computational linguistics and the 11th international joint conference on natural language processing: System demonstrations</em>, pages 249–257, Online. Association for Computational Linguistics.
</div>
<div id="ref-alves-etal-2024-tower" class="csl-entry" role="listitem">
Duarte Miguel Alves, José Pombal, Nuno M Guerreiro, Pedro Henrique Martins, João Alves, Amin Farajian, Ben Peters, Ricardo Rei, Patrick Fernandes, Sweta Agrawal, Pierre Colombo, José G. C. de Souza, and Andre Martins. 2024. <a href="https://openreview.net/forum?id=EHPns3hVkj">Tower: An open multilingual large language model for translation-related tasks</a>. In <em>First conference on language modeling</em>.
</div>
<div id="ref-amrhein-sennrich-2022-identifying" class="csl-entry" role="listitem">
Chantal Amrhein and Rico Sennrich. 2022. <a href="https://doi.org/10.18653/v1/2022.aacl-main.83">Identifying weaknesses in machine translation metrics through minimum <span>B</span>ayes risk decoding: A case study for <span>COMET</span></a>. In Yulan He, Heng Ji, Sujian Li, Yang Liu, and Chua-Hui Chang, editors, <em>Proceedings of the 2nd conference of the asia-pacific chapter of the association for computational linguistics and the 12th international joint conference on natural language processing (volume 1: Long papers)</em>, pages 1125–1141, Online only. Association for Computational Linguistics.
</div>
<div id="ref-arditi-etal-2024-refusal" class="csl-entry" role="listitem">
Andy Arditi, Oscar Obeso, Aaquib Syed, Daniel Paleka, Nina Panickssery, Wes Gurnee, and Neel Nanda. 2024. <a href="https://proceedings.neurips.cc/paper_files/paper/2024/file/f545448535dfde4f9786555403ab7c49-Paper-Conference.pdf">Refusal in language models is mediated by a single direction</a>. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, <em>Advances in neural information processing systems</em>, volume 37, pages 136037–136083, Red Hook, NY, USA. Curran Associates, Inc.
</div>
<div id="ref-arora-etal-2018-linear" class="csl-entry" role="listitem">
Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, and Andrej Risteski. 2018. <a href="https://doi.org/10.1162/tacl_a_00034">Linear algebraic structure of word senses, with applications to polysemy</a>. <em>Transactions of the Association for Computational Linguistics</em>, 6:483–495.
</div>
<div id="ref-atanasova-etal-2020-diagnostic" class="csl-entry" role="listitem">
Pepa Atanasova, Jakob Grue Simonsen, Christina Lioma, and Isabelle Augenstein. 2020. <a href="https://doi.org/10.18653/v1/2020.emnlp-main.263">A diagnostic study of explainability techniques for text classification</a>. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, <em>Proceedings of the 2020 conference on empirical methods in natural language processing (EMNLP)</em>, pages 3256–3274, Online. Association for Computational Linguistics.
</div>
<div id="ref-attanasio-etal-2023-ferret" class="csl-entry" role="listitem">
Giuseppe Attanasio, Eliana Pastor, Chiara Di Bonaventura, and Debora Nozza. 2023. <a href="https://doi.org/10.18653/v1/2023.eacl-demo.29">Ferret: A framework for benchmarking explainers on transformers</a>. In Danilo Croce and Luca Soldaini, editors, <em>Proceedings of the 17th conference of the european chapter of the association for computational linguistics: System demonstrations</em>, pages 256–266, Dubrovnik, Croatia. Association for Computational Linguistics.
</div>
<div id="ref-aziz-etal-2012-pet" class="csl-entry" role="listitem">
Wilker Aziz, Sheila Castilho, and Lucia Specia. 2012. <a href="https://aclanthology.org/L12-1587/"><span>PET</span>: A tool for post-editing and assessing machine translation</a>. In Nicoletta Calzolari, Khalid Choukri, Thierry Declerck, Mehmet Uğur Doğan, Bente Maegaard, Joseph Mariani, Asuncion Moreno, Jan Odijk, and Stelios Piperidis, editors, <em>Proceedings of the eighth international conference on language resources and evaluation (<span>LREC</span>‘12)</em>, pages 3982–3987, Istanbul, Turkey. European Language Resources Association (ELRA).
</div>
<div id="ref-ba-etal-2016-layer" class="csl-entry" role="listitem">
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. 2016. <a href="https://arxiv.org/abs/1607.06450">Layer normalization</a>. <em>Arxiv Preprint</em>.
</div>
<div id="ref-bach-etal-2015-pixel" class="csl-entry" role="listitem">
Alexander AND Montavon Bach Sebastian AND Binder. 2015. <a href="https://doi.org/10.1371/journal.pone.0130140">On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation</a>. <em>PLOS ONE</em>, 10(7):1–46.
</div>
<div id="ref-bahdanau-etal-2015-neural" class="csl-entry" role="listitem">
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. <a href="http://arxiv.org/abs/1409.0473">Neural machine translation by jointly learning to align and translate</a>. In Yoshua Bengio and Yann LeCun, editors, <em>Proceedings of the 3rd international conference on learning representations (<span>ICLR</span>)</em>, San Diego, CA, USA.
</div>
<div id="ref-banerjee-lavie-2005-meteor" class="csl-entry" role="listitem">
Satanjeev Banerjee and Alon Lavie. 2005. <a href="https://aclanthology.org/W05-0909/"><span>METEOR</span>: An automatic metric for <span>MT</span> evaluation with improved correlation with human judgments</a>. In Jade Goldstein, Alon Lavie, Chin-Yew Lin, and Clare Voss, editors, <em>Proceedings of the <span>ACL</span> workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization</em>, pages 65–72, Ann Arbor, Michigan. Association for Computational Linguistics.
</div>
<div id="ref-bastings-etal-2022-will" class="csl-entry" role="listitem">
Jasmijn Bastings, Sebastian Ebert, Polina Zablotskaia, Anders Sandholm, and Katja Filippova. 2022. <a href="https://doi.org/10.18653/v1/2022.emnlp-main.64"><span>“</span>Will you find these shortcuts?<span>”</span> A protocol for evaluating the faithfulness of input salience methods for text classification</a>. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, <em>Proceedings of the 2022 conference on empirical methods in natural language processing</em>, pages 976–991, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.
</div>
<div id="ref-bastings-filippova-2020-elephant" class="csl-entry" role="listitem">
Jasmijn Bastings and Katja Filippova. 2020. <a href="https://doi.org/10.18653/v1/2020.blackboxnlp-1.14">The elephant in the interpretability room: Why use attention as explanation when we have saliency methods?</a> In Afra Alishahi, Yonatan Belinkov, Grzegorz Chrupała, Dieuwke Hupkes, Yuval Pinter, and Hassan Sajjad, editors, <em>Proceedings of the third BlackboxNLP workshop on analyzing and interpreting neural networks for NLP</em>, pages 149–155, Online. Association for Computational Linguistics.
</div>
<div id="ref-bawden-etal-2018-evaluating" class="csl-entry" role="listitem">
Rachel Bawden, Rico Sennrich, Alexandra Birch, and Barry Haddow. 2018. <a href="https://doi.org/10.18653/v1/N18-1118">Evaluating discourse phenomena in neural machine translation</a>. In Marilyn Walker, Heng Ji, and Amanda Stent, editors, <em>Proceedings of the 2018 conference of the north <span>A</span>merican chapter of the association for computational linguistics: Human language technologies, volume 1 (long papers)</em>, pages 1304–1313, New Orleans, Louisiana. Association for Computational Linguistics.
</div>
<div id="ref-belinkov-2022-probing" class="csl-entry" role="listitem">
Yonatan Belinkov. 2022. <a href="https://doi.org/10.1162/coli_a_00422">Probing classifiers: Promises, shortcomings, and advances</a>. <em>Computational Linguistics</em>, 48(1):207–219.
</div>
<div id="ref-bentivogli-etal-2016-neural" class="csl-entry" role="listitem">
Luisa Bentivogli, Arianna Bisazza, Mauro Cettolo, and Marcello Federico. 2016. <a href="https://doi.org/10.18653/v1/D16-1025">Neural versus phrase-based machine translation quality: A case study</a>. In Jian Su, Kevin Duh, and Xavier Carreras, editors, <em>Proceedings of the 2016 conference on empirical methods in natural language processing</em>, pages 257–267, Austin, Texas. Association for Computational Linguistics.
</div>
<div id="ref-bilodeau-etal-2024-impossibility" class="csl-entry" role="listitem">
Blair Bilodeau, Natasha Jaques, Pang Wei Koh, and Been Kim. 2024. <a href="https://doi.org/10.1073/pnas.2304406120">Impossibility theorems for feature attribution</a>. <em>Proceedings of the National Academy of Sciences</em>, 121(2):e2304406120.
</div>
<div id="ref-blain-etal-2023-findings" class="csl-entry" role="listitem">
Frederic Blain, Chrysoula Zerva, Ricardo Rei, Nuno M. Guerreiro, Diptesh Kanojia, José G. C. de Souza, Beatriz Silva, Tânia Vaz, Yan Jingxuan, Fatemeh Azadi, Constantin Orasan, and André Martins. 2023. <a href="https://doi.org/10.18653/v1/2023.wmt-1.52">Findings of the <span>WMT</span> 2023 shared task on quality estimation</a>. In Philipp Koehn, Barry Haddow, Tom Kocmi, and Christof Monz, editors, <em>Proceedings of the eighth conference on machine translation</em>, pages 629–653, Singapore. Association for Computational Linguistics.
</div>
<div id="ref-blatz-etal-2004-confidence" class="csl-entry" role="listitem">
John Blatz, Erin Fitzgerald, George Foster, Simona Gandrabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis, and Nicola Ueffing. 2004. <a href="https://aclanthology.org/C04-1046/">Confidence estimation for machine translation</a>. In <em><span>COLING</span> 2004: Proceedings of the 20th international conference on computational linguistics</em>, pages 315–321, Geneva, Switzerland. COLING.
</div>
<div id="ref-bojar-etal-2017-findings" class="csl-entry" role="listitem">
Ondřej Bojar, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry Haddow, Shujian Huang, Matthias Huck, Philipp Koehn, Qun Liu, Varvara Logacheva, Christof Monz, Matteo Negri, Matt Post, Raphael Rubino, Lucia Specia, and Marco Turchi. 2017. <a href="https://doi.org/10.18653/v1/W17-4717">Findings of the 2017 conference on machine translation (<span>WMT</span>17)</a>. In Ondřej Bojar, Christian Buck, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes, Philipp Koehn, and Julia Kreutzer, editors, <em>Proceedings of the second conference on machine translation</em>, pages 169–214, Copenhagen, Denmark. Association for Computational Linguistics.
</div>
<div id="ref-bowker-2002-computer" class="csl-entry" role="listitem">
Lynne Bowker. 2002. <em><a href="http://www.jstor.org/stable/j.ctt1ch78kf">Computer-aided translation technology: A practical introduction</a></em>. University of Ottawa Press.
</div>
<div id="ref-briakou-etal-2024-translating" class="csl-entry" role="listitem">
Eleftheria Briakou, Jiaming Luo, Colin Cherry, and Markus Freitag. 2024. <a href="https://doi.org/10.18653/v1/2024.wmt-1.123">Translating step-by-step: Decomposing the translation process for improved translation quality of long-form texts</a>. In Barry Haddow, Tom Kocmi, Philipp Koehn, and Christof Monz, editors, <em>Proceedings of the ninth conference on machine translation</em>, pages 1301–1317, Miami, Florida, USA. Association for Computational Linguistics.
</div>
<div id="ref-bricken-etal-2023-monosemanticity" class="csl-entry" role="listitem">
Trenton Bricken, Adly Templeton, Joshua Batson, Brian Chen, Adam Jermyn, Tom Conerly, Nick Turner, Cem Anil, Carson Denison, Amanda Askell, Robert Lasenby, Yifan Wu, Shauna Kravec, Nicholas Schiefer, Tim Maxwell, Nicholas Joseph, Zac Hatfield-Dodds, Alex Tamkin, Karina Nguyen, et al. 2023. <a href="https://transformer-circuits.pub/2023/monosemantic-features/index.html">Towards monosemanticity: Decomposing language models with dictionary learning</a>. <em>Transformer Circuits Thread</em>.
</div>
<div id="ref-brown-etal-2020-language" class="csl-entry" role="listitem">
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, et al. 2020. Language models are few-shot learners. In <em>Proceedings of the 34th international conference on neural information processing systems</em>, Red Hook, NY, USA. Curran Associates Inc.
</div>
<div id="ref-bugliarello-etal-2020-easier" class="csl-entry" role="listitem">
Emanuele Bugliarello, Sabrina J. Mielke, Antonios Anastasopoulos, Ryan Cotterell, and Naoaki Okazaki. 2020. <a href="https://doi.org/10.18653/v1/2020.acl-main.149">It‘s easier to translate out of <span>E</span>nglish than into it: <span>M</span>easuring neural translation difficulty by cross-mutual information</a>. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault, editors, <em>Proceedings of the 58th annual meeting of the association for computational linguistics</em>, pages 1640–1649, Online. Association for Computational Linguistics.
</div>
<div id="ref-callison-burch-etal-2007-meta" class="csl-entry" role="listitem">
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn, Christof Monz, and Josh Schroeder. 2007. <a href="https://aclanthology.org/W07-0718/">(Meta-) evaluation of machine translation</a>. In Chris Callison-Burch, Philipp Koehn, Cameron Shaw Fordyce, and Christof Monz, editors, <em>Proceedings of the second workshop on statistical machine translation</em>, pages 136–158, Prague, Czech Republic. Association for Computational Linguistics.
</div>
<div id="ref-castilho-etal-2017-neural" class="csl-entry" role="listitem">
Sheila Castilho, Joss Moorkens, Federico Gaspari, Iacer Calixto, John Tinsley, and Andy Way. 2017. <a href="https://doi.org/10.1515/pralin-2017-0013">Is neural machine translation the new state of the art?</a> <em>The Prague Bulletin of Mathematical Linguistics</em>, 108(1):109–120.
</div>
<div id="ref-church-hovy-1993-good" class="csl-entry" role="listitem">
Kenneth W. Church and Eduard H. Hovy. 1993. <a href="https://doi.org/10.1007/BF00981759">Good applications for crummy machine translation</a>. <em>Machine Translation</em>, 8(4):239–258.
</div>
<div id="ref-clark-etal-2019-bert" class="csl-entry" role="listitem">
Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D. Manning. 2019. <a href="https://doi.org/10.18653/v1/W19-4828">What does <span>BERT</span> look at? An analysis of <span>BERT</span>‘s attention</a>. In Tal Linzen, Grzegorz Chrupała, Yonatan Belinkov, and Dieuwke Hupkes, editors, <em>Proceedings of the 2019 ACL workshop BlackboxNLP: Analyzing and interpreting neural networks for NLP</em>, pages 276–286, Florence, Italy. Association for Computational Linguistics.
</div>
<div id="ref-conneau-lample-2019-cross" class="csl-entry" role="listitem">
Alexis Conneau and Guillaume Lample. 2019. <a href="https://proceedings.neurips.cc/paper_files/paper/2019/file/c04c19c2c2474dbf5f7ac4372c5b9af1-Paper.pdf">Cross-lingual language model pretraining</a>. In H. Wallach, H. Larochelle, A. Beygelzimer, F. dAlché-Buc, E. Fox, and R. Garnett, editors, <em>Advances in neural information processing systems</em>, volume 32. Curran Associates, Inc.
</div>
<div id="ref-coppers-etal-2018-intellingo" class="csl-entry" role="listitem">
Sven Coppers, Jan Van den Bergh, Kris Luyten, Karin Coninx, Iulianna Van der Lek-Ciudin, Tom Vanallemeersch, and Vincent Vandeghinste. 2018. <a href="https://dl.acm.org/doi/abs/10.1145/3173574.3174098">Intellingo: <span>An</span> intelligible translation environment</a>. In <em>Proceedings of the 2018 CHI conference on human factors in computing systems</em>, pages 1–13.
</div>
<div id="ref-covert-etal-2021-explaining" class="csl-entry" role="listitem">
Ian Covert, Scott Lundberg, and Su-In Lee. 2021. <a href="http://jmlr.org/papers/v22/20-1316.html">Explaining by removing: A unified framework for model explanation</a>. <em>Journal of Machine Learning Research</em>, 22(209):1–90.
</div>
<div id="ref-crabbe-vanderschaar-2023-evaluating" class="csl-entry" role="listitem">
Jonathan Crabbé and Mihaela van der Schaar. 2023. <a href="https://openreview.net/forum?id=5UwnKSgY6u">Evaluating the robustness of interpretability methods through explanation invariance and equivariance</a>. In <em>Thirty-seventh conference on neural information processing systems</em>.
</div>
<div id="ref-cui-etal-2025-multilingual" class="csl-entry" role="listitem">
Menglong Cui, Pengzhi Gao, Wei Liu, Jian Luan, and Bin Wang. 2025. <a href="https://aclanthology.org/2025.naacl-long.280/">Multilingual machine translation with open large language models at practical scale: An empirical study</a>. In Luis Chiruzzo, Alan Ritter, and Lu Wang, editors, <em>Proceedings of the 2025 conference of the nations of the americas chapter of the association for computational linguistics: Human language technologies (volume 1: Long papers)</em>, pages 5420–5443, Albuquerque, New Mexico. Association for Computational Linguistics.
</div>
<div id="ref-daems-etal-2017-identifying" class="csl-entry" role="listitem">
Joke Daems, Sonia Vandepitte, Robert J. Hartsuiker, and Lieve Macken. 2017. <a href="https://doi.org/10.3389/fpsyg.2017.01282">Identifying the machine translation error types with the greatest impact on post-editing effort</a>. <em>Frontiers in Psychology</em>, 8.
</div>
<div id="ref-dale-etal-2023-detecting" class="csl-entry" role="listitem">
David Dale, Elena Voita, Loic Barrault, and Marta R. Costa-jussà. 2023. <a href="https://doi.org/10.18653/v1/2023.acl-long.3">Detecting and mitigating hallucinations in machine translation: Model internal workings alone do well, sentence similarity <span>E</span>ven better</a>. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, <em>Proceedings of the 61st annual meeting of the association for computational linguistics (volume 1: Long papers)</em>, pages 36–50, Toronto, Canada. Association for Computational Linguistics.
</div>
<div id="ref-devlin-etal-2019-bert" class="csl-entry" role="listitem">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. <a href="https://doi.org/10.18653/v1/N19-1423"><span>BERT</span>: Pre-training of deep bidirectional transformers for language understanding</a>. In Jill Burstein, Christy Doran, and Thamar Solorio, editors, <em>Proceedings of the 2019 conference of the north <span>A</span>merican chapter of the association for computational linguistics: Human language technologies, volume 1 (long and short papers)</em>, pages 4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics.
</div>
<div id="ref-dong-etal-2024-survey" class="csl-entry" role="listitem">
Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Jingyuan Ma, Rui Li, Heming Xia, Jingjing Xu, Zhiyong Wu, Baobao Chang, Xu Sun, Lei Li, and Zhifang Sui. 2024. <a href="https://doi.org/10.18653/v1/2024.emnlp-main.64">A survey on in-context learning</a>. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, <em>Proceedings of the 2024 conference on empirical methods in natural language processing</em>, pages 1107–1128, Miami, Florida, USA. Association for Computational Linguistics.
</div>
<div id="ref-donoho-elad-2003-optimally" class="csl-entry" role="listitem">
David L. Donoho and Michael Elad. 2003. <a href="https://doi.org/10.1073/pnas.0437847100">Optimally sparse representation in general (nonorthogonal) dictionaries via &amp;#x2113;&lt;sup&gt;1&lt;/sup&gt; minimization</a>. <em>Proceedings of the National Academy of Sciences</em>, 100(5):2197–2202.
</div>
<div id="ref-elhage-etal-2022-toy" class="csl-entry" role="listitem">
Nelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan, Shauna Kravec, Zac Hatfield-Dodds, Robert Lasenby, Dawn Drain, Carol Chen, Roger Grosse, Sam McCandlish, Jared Kaplan, Dario Amodei, Martin Wattenberg, and Christopher Olah. 2022. <a href="https://transformer-circuits.pub/2022/toy_model/index.html">Toy models of superposition</a>. <em>Transformer Circuits Thread</em>.
</div>
<div id="ref-elhage-etal-2021-mathematical" class="csl-entry" role="listitem">
Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, et al. 2021. A mathematical framework for transformer circuits. <em>Transformer Circuits Thread</em>. https://transformer-circuits.pub/2021/framework/index.html.
</div>
<div id="ref-fadeeva-etal-2024-fact" class="csl-entry" role="listitem">
Ekaterina Fadeeva, Aleksandr Rubashevskii, Artem Shelmanov, Sergey Petrakov, Haonan Li, Hamdy Mubarak, Evgenii Tsymbalov, Gleb Kuzmin, Alexander Panchenko, Timothy Baldwin, Preslav Nakov, and Maxim Panov. 2024. <a href="https://doi.org/10.18653/v1/2024.findings-acl.558">Fact-checking the output of large language models via token-level uncertainty quantification</a>. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, <em>Findings of the association for computational linguistics: ACL 2024</em>, pages 9367–9385, Bangkok, Thailand. Association for Computational Linguistics.
</div>
<div id="ref-fadeeva-etal-2023-lm" class="csl-entry" role="listitem">
Ekaterina Fadeeva, Roman Vashurin, Akim Tsvigun, Artem Vazhentsev, Sergey Petrakov, Kirill Fedyanin, Daniil Vasilev, Elizaveta Goncharova, Alexander Panchenko, Maxim Panov, Timothy Baldwin, and Artem Shelmanov. 2023. <a href="https://doi.org/10.18653/v1/2023.emnlp-demo.41"><span>LM</span>-polygraph: Uncertainty estimation for language models</a>. In Yansong Feng and Els Lefever, editors, <em>Proceedings of the 2023 conference on empirical methods in natural language processing: System demonstrations</em>, pages 446–461, Singapore. Association for Computational Linguistics.
</div>
<div id="ref-fan-etal-2021-beyond" class="csl-entry" role="listitem">
Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep Baines, Onur Çelebi, Guillaume Wenzek, Vishrav Chaudhary, Naman Goyal, Tom Birch, Vitaliy Liptchinsky, Sergey Edunov, Edouard Grave, Michael Auli, and Armand Joulin. 2021. <a href="http://jmlr.org/papers/v22/20-1307.html">Beyond english-centric multilingual machine translation</a>. <em>Journal of Machine Learning Research</em>, 22(107):1–48.
</div>
<div id="ref-federico-etal-2014-matecat-open" class="csl-entry" role="listitem">
Marcello Federico, Nicola Bertoldi, Marco Trombetti, and Alessandro Cattelan. 2014. <a href="https://aclanthology.org/2014.amta-tutorials.3/"><span>M</span>ate<span>C</span>at: An open source <span>CAT</span> tool for <span>MT</span> post-editing</a>. In <em>Proceedings of the 11th conference of the association for machine translation in the americas: tutorials</em>, Vancouver, Canada. Association for Machine Translation in the Americas.
</div>
<div id="ref-fel-2024-sparks" class="csl-entry" role="listitem">
Thomas Fel. 2024. <em><a href="https://arxiv.org/abs/2502.01048">Sparks of explainability: Recent advancements in explaining large vision models</a></em>. PhD thesis, University of Toulouse.
</div>
<div id="ref-fernandes-etal-2023-devil" class="csl-entry" role="listitem">
Patrick Fernandes, Daniel Deutsch, Mara Finkelstein, Parker Riley, André Martins, Graham Neubig, Ankush Garg, Jonathan Clark, Markus Freitag, and Orhan Firat. 2023a. <a href="https://doi.org/10.18653/v1/2023.wmt-1.100">The devil is in the errors: Leveraging large language models for fine-grained machine translation evaluation</a>. In Philipp Koehn, Barry Haddow, Tom Kocmi, and Christof Monz, editors, <em>Proceedings of the eighth conference on machine translation</em>, pages 1066–1083, Singapore. Association for Computational Linguistics.
</div>
<div id="ref-fernandes-etal-2023-translation" class="csl-entry" role="listitem">
Patrick Fernandes, Kayo Yin, Emmy Liu, André Martins, and Graham Neubig. 2023b. <a href="https://doi.org/10.18653/v1/2023.acl-long.36">When does translation require context? A data-driven, multilingual exploration</a>. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, <em>Proceedings of the 61st annual meeting of the association for computational linguistics (volume 1: Long papers)</em>, pages 606–626, Toronto, Canada. Association for Computational Linguistics.
</div>
<div id="ref-ferrando-etal-2022-measuring" class="csl-entry" role="listitem">
Javier Ferrando, Gerard I. Gállego, and Marta R. Costa-jussà. 2022. <a href="https://doi.org/10.18653/v1/2022.emnlp-main.595">Measuring the mixing of contextual information in the transformer</a>. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, <em>Proceedings of the 2022 conference on empirical methods in natural language processing</em>, pages 8698–8714, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.
</div>
<div id="ref-ferrando-etal-2024-primer" class="csl-entry" role="listitem">
Javier Ferrando, Gabriele Sarti, Arianna Bisazza, and Marta R. Costa-jussà. 2024. <a href="https://arxiv.org/abs/2405.00208">A primer on the inner workings of transformer-based language models</a>. <em>Arxiv Preprint</em>.
</div>
<div id="ref-fiottokaufman-etal-2024-nnsight" class="csl-entry" role="listitem">
Jaden Fried Fiotto-Kaufman, Alexander Russell Loftus, Eric Todd, Jannik Brinkmann, Koyena Pal, Dmitrii Troitskii, Michael Ripa, Adam Belfki, Can Rager, Caden Juang, Aaron Mueller, Samuel Marks, Arnab Sen Sharma, Francesca Lucchetti, Nikhil Prakash, Carla E. Brodley, Arjun Guha, Jonathan Bell, Byron C Wallace, et al. 2025. <a href="https://openreview.net/forum?id=MxbEiFRf39"><span>NN</span>sight and <span>NDIF</span>: Democratizing access to open-weight foundation model internals</a>. In <em>The thirteenth international conference on learning representations</em>.
</div>
<div id="ref-fomicheva-etal-2021-eval4nlp" class="csl-entry" role="listitem">
Marina Fomicheva, Piyawat Lertvittayakumjorn, Wei Zhao, Steffen Eger, and Yang Gao. 2021. <a href="https://doi.org/10.18653/v1/2021.eval4nlp-1.17">The <span>E</span>val4<span>NLP</span> shared task on explainable quality estimation: Overview and results</a>. In Yang Gao, Steffen Eger, Wei Zhao, Piyawat Lertvittayakumjorn, and Marina Fomicheva, editors, <em>Proceedings of the 2nd workshop on evaluation and comparison of NLP systems</em>, pages 165–178, Punta Cana, Dominican Republic. Association for Computational Linguistics.
</div>
<div id="ref-fomicheva-etal-2020-unsupervised" class="csl-entry" role="listitem">
Marina Fomicheva, Shuo Sun, Lisa Yankovskaya, Frédéric Blain, Francisco Guzmán, Mark Fishel, Nikolaos Aletras, Vishrav Chaudhary, and Lucia Specia. 2020. <a href="https://doi.org/10.1162/tacl_a_00330">Unsupervised quality estimation for neural machine translation</a>. <em>Transactions of the Association for Computational Linguistics</em>, 8:539–555.
</div>
<div id="ref-freitag-etal-2021-experts" class="csl-entry" role="listitem">
Markus Freitag, George Foster, David Grangier, Viresh Ratnakar, Qijun Tan, and Wolfgang Macherey. 2021. <a href="https://doi.org/10.1162/tacl_a_00437">Experts, errors, and context: A large-scale study of human evaluation for machine translation</a>. <em>Transactions of the Association for Computational Linguistics</em>, 9:1460–1474.
</div>
<div id="ref-freitag-etal-2023-results" class="csl-entry" role="listitem">
Markus Freitag, Nitika Mathur, Chi-kiu Lo, Eleftherios Avramidis, Ricardo Rei, Brian Thompson, Tom Kocmi, Frederic Blain, Daniel Deutsch, Craig Stewart, Chrysoula Zerva, Sheila Castilho, Alon Lavie, and George Foster. 2023. <a href="https://doi.org/10.18653/v1/2023.wmt-1.51">Results of <span>WMT</span>23 metrics shared task: Metrics might be guilty but references are not innocent</a>. In Philipp Koehn, Barry Haddow, Tom Kocmi, and Christof Monz, editors, <em>Proceedings of the eighth conference on machine translation</em>, pages 578–628, Singapore. Association for Computational Linguistics.
</div>
<div id="ref-freitag-etal-2022-results" class="csl-entry" role="listitem">
Markus Freitag, Ricardo Rei, Nitika Mathur, Chi-kiu Lo, Craig Stewart, Eleftherios Avramidis, Tom Kocmi, George Foster, Alon Lavie, and André F. T. Martins. 2022. <a href="https://aclanthology.org/2022.wmt-1.2/">Results of <span>WMT</span>22 metrics shared task: Stop using <span>BLEU</span> <span>–</span> neural metrics are better and more robust</a>. In Philipp Koehn, Loïc Barrault, Ondřej Bojar, Fethi Bougares, Rajen Chatterjee, Marta R. Costa-jussà, Christian Federmann, Mark Fishel, Alexander Fraser, Markus Freitag, Yvette Graham, Roman Grundkiewicz, Paco Guzman, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes, Tom Kocmi, André Martins, Makoto Morishita, et al., editors, <em>Proceedings of the seventh conference on machine translation (WMT)</em>, pages 46–68, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics.
</div>
<div id="ref-gal-ghahramani-2016-dropout" class="csl-entry" role="listitem">
Yarin Gal and Zoubin Ghahramani. 2016. <a href="https://proceedings.mlr.press/v48/gal16.html">Dropout as a bayesian approximation: Representing model uncertainty in deep learning</a>. In Maria Florina Balcan and Kilian Q. Weinberger, editors, <em>Proceedings of the 33rd international conference on machine learning</em>, volume 48, pages 1050–1059, New York, NY, USA. Proceedings of Machine Learning Research (PLMR).
</div>
<div id="ref-garcia-2009-beyond" class="csl-entry" role="listitem">
Ignacio Garcia. 2009. <a href="https://doi.org/10.26034/cm.jostrans.2009.624">Beyond translation memory: Computers and the professional translator</a>. <em>The Journal of Specialised Translation</em>.
</div>
<div id="ref-geva-etal-2021-transformer" class="csl-entry" role="listitem">
Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. 2021. <a href="https://doi.org/10.18653/v1/2021.emnlp-main.446">Transformer feed-forward layers are key-value memories</a>. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, <em>Proceedings of the 2021 conference on empirical methods in natural language processing</em>, pages 5484–5495, Online; Punta Cana, Dominican Republic. Association for Computational Linguistics.
</div>
<div id="ref-goodfellow-etal-2016-deep" class="csl-entry" role="listitem">
Ian Goodfellow, Yoshua Bengio, and Aaron Courville. 2016. <em><a href="http://www.deeplearningbook.org">Deep learning</a></em>. MIT Press.
</div>
<div id="ref-goyal-etal-2021-larger" class="csl-entry" role="listitem">
Naman Goyal, Jingfei Du, Myle Ott, Giri Anantharaman, and Alexis Conneau. 2021. <a href="https://doi.org/10.18653/v1/2021.repl4nlp-1.4">Larger-scale transformers for multilingual masked language modeling</a>. In Anna Rogers, Iacer Calixto, Ivan Vulić, Naomi Saphra, Nora Kassner, Oana-Maria Camburu, Trapit Bansal, and Vered Shwartz, editors, <em>Proceedings of the 6th workshop on representation learning for NLP (RepL4NLP-2021)</em>, pages 29–33, Online. Association for Computational Linguistics.
</div>
<div id="ref-graham-etal-2013-continuous" class="csl-entry" role="listitem">
Yvette Graham, Timothy Baldwin, Alistair Moffat, and Justin Zobel. 2013. <a href="https://aclanthology.org/W13-2305/">Continuous measurement scales in human evaluation of machine translation</a>. In Antonio Pareja-Lora, Maria Liakata, and Stefanie Dipper, editors, <em>Proceedings of the 7th linguistic annotation workshop and interoperability with discourse</em>, pages 33–41, Sofia, Bulgaria. Association for Computational Linguistics.
</div>
<div id="ref-green-etal-2013-efficacy" class="csl-entry" role="listitem">
Spence Green, Jeffrey Heer, and Christopher D. Manning. 2013. <a href="https://doi.org/10.1145/2470654.2470718">The efficacy of human post-editing for language translation</a>. In <em>Proceedings of the <span>SIGCHI</span> <span>Conference</span> on <span>Human</span> <span>Factors</span> in <span>Computing</span> <span>Systems</span></em>, pages 439–448, New York, NY, USA. Association for Computing Machinery.
</div>
<div id="ref-guerberof-2009-productivity" class="csl-entry" role="listitem">
Ana Guerberof. 2009. <a href="https://aclanthology.org/2009.mtsummit-btm.7/">Productivity and quality in <span>MT</span> post-editing</a>. In <em>Beyond translation memories: New tools for translators workshop</em>, Ottawa, Canada.
</div>
<div id="ref-guerberof-toral-2022-creativity" class="csl-entry" role="listitem">
Ana Guerberof-Arenas and Antonio Toral. 2022. <a href="https://doi.org/10.1075/ts.21025.gue">Creativity in translation: Machine translation as a constraint for literary texts</a>. <em>Translation Spaces</em>, 11(2):184–212.
</div>
<div id="ref-guerreiro-etal-2024-xcomet" class="csl-entry" role="listitem">
Nuno M. Guerreiro, Ricardo Rei, Daan van Stigt, Luisa Coheur, Pierre Colombo, and André F. T. Martins. 2024. <a href="https://doi.org/10.1162/tacl_a_00683">Xcomet: Transparent machine translation evaluation through fine-grained error detection</a>. <em>Transactions of the Association for Computational Linguistics</em>, 12:979–995.
</div>
<div id="ref-gupta-etal-2015-distributional" class="csl-entry" role="listitem">
Abhijeet Gupta, Gemma Boleda, Marco Baroni, and Sebastian Padó. 2015. <a href="https://doi.org/10.18653/v1/D15-1002">Distributional vectors encode referential attributes</a>. In Lluís Màrquez, Chris Callison-Burch, and Jian Su, editors, <em>Proceedings of the 2015 conference on empirical methods in natural language processing</em>, pages 12–21, Lisbon, Portugal. Association for Computational Linguistics.
</div>
<div id="ref-hadiwinoto-2017-book" class="csl-entry" role="listitem">
Christian Hadiwinoto. 2017. <a href="https://doi.org/10.1162/COLI_r_00303">Book review: Syntax-based statistical machine translation by philip <span>W</span>illiams, rico <span>S</span>ennrich, matt post and philipp <span>K</span>oehn</a>. <em>Computational Linguistics</em>, 43(4):893–896.
</div>
<div id="ref-harris-1954-distributional" class="csl-entry" role="listitem">
Zellig S. Harris. 1954. <a href="https://doi.org/10.1080/00437956.1954.11659520">Distributional structure</a>. <em>Word</em>, 10(2-3):146–162.
</div>
<div id="ref-he-etal-2016-deep-residual" class="csl-entry" role="listitem">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. <a href="https://doi.org/10.1109/CVPR.2016.90">Deep residual learning for image recognition</a>. In <em>2016 IEEE conference on computer vision and pattern recognition (CVPR)</em>, pages 770–778, Los Alamitos, CA, USA. IEEE Computer Society.
</div>
<div id="ref-herbig-etal-2020-mmpe" class="csl-entry" role="listitem">
Nico Herbig, Tim Düwel, Santanu Pal, Kalliopi Meladaki, Mahsa Monshizadeh, Antonio Krüger, and Josef van Genabith. 2020. <a href="https://doi.org/10.18653/v1/2020.acl-main.155"><span>MMPE</span>: <span>A</span> <span>M</span>ulti-<span>M</span>odal <span>I</span>nterface for <span>P</span>ost-<span>E</span>diting <span>M</span>achine <span>T</span>ranslation</a>. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault, editors, <em>Proceedings of the 58th annual meeting of the association for computational linguistics</em>, pages 1691–1702, Online. Association for Computational Linguistics.
</div>
<div id="ref-himmi-etal-2024-enhanced" class="csl-entry" role="listitem">
Anas Himmi, Guillaume Staerman, Marine Picot, Pierre Colombo, and Nuno M Guerreiro. 2024. <a href="https://doi.org/10.18653/v1/2024.emnlp-main.1033">Enhanced hallucination detection in neural machine translation through simple detector aggregation</a>. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, <em>Proceedings of the 2024 conference on empirical methods in natural language processing</em>, pages 18573–18583, Miami, Florida, USA. Association for Computational Linguistics.
</div>
<div id="ref-hochreiter-1998-vanishing" class="csl-entry" role="listitem">
Sepp Hochreiter. 1998. <a href="https://doi.org/10.1142/S0218488598000094">The vanishing gradient problem during learning recurrent neural nets and problem solutions</a>. <em>Int. J. Uncertain. Fuzziness Knowl.-Based Syst.</em>, 6(2):107–116.
</div>
<div id="ref-hochreiter-schmidhuber-1997-long" class="csl-entry" role="listitem">
Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory. <em>Neural computation</em>, 9(8):1735–1780.
</div>
<div id="ref-holtzman-etal-2021-surface" class="csl-entry" role="listitem">
Ari Holtzman, Peter West, Vered Shwartz, Yejin Choi, and Luke Zettlemoyer. 2021. <a href="https://doi.org/10.18653/v1/2021.emnlp-main.564">Surface form competition: Why the highest probability answer isn‘t always right</a>. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, <em>Proceedings of the 2021 conference on empirical methods in natural language processing</em>, pages 7038–7051, Online; Punta Cana, Dominican Republic. Association for Computational Linguistics.
</div>
<div id="ref-howard-ruder-2018-universal" class="csl-entry" role="listitem">
Jeremy Howard and Sebastian Ruder. 2018. <a href="https://doi.org/10.18653/v1/P18-1031">Universal language model fine-tuning for text classification</a>. In Iryna Gurevych and Yusuke Miyao, editors, <em>Proceedings of the 56th annual meeting of the association for computational linguistics (volume 1: Long papers)</em>, pages 328–339, Melbourne, Australia. Association for Computational Linguistics.
</div>
<div id="ref-huben-etal-2024-sparse" class="csl-entry" role="listitem">
Robert Huben, Hoagy Cunningham, Logan Riggs Smith, Aidan Ewart, and Lee Sharkey. 2024. <a href="https://openreview.net/forum?id=F76bwRSLeK">Sparse autoencoders find highly interpretable features in language models</a>. In <em>The twelfth international conference on learning representations</em>.
</div>
<div id="ref-hutchins-2001-machine" class="csl-entry" role="listitem">
William J. Hutchins. 2001. <a href="https://doi.org/10.3406/hel.2001.2815">Machine translation over fifty years</a>. <em>Histoire Épistémologie Langage</em>, 23:7–31.
</div>
<div id="ref-jacovi-goldberg-2020-towards" class="csl-entry" role="listitem">
Alon Jacovi and Yoav Goldberg. 2020. <a href="https://doi.org/10.18653/v1/2020.acl-main.386">Towards faithfully interpretable <span>NLP</span> systems: How should we define and evaluate faithfulness?</a> In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault, editors, <em>Proceedings of the 58th annual meeting of the association for computational linguistics</em>, pages 4198–4205, Online. Association for Computational Linguistics.
</div>
<div id="ref-jain-wallace-2019-attention" class="csl-entry" role="listitem">
Sarthak Jain and Byron C. Wallace. 2019. <a href="https://doi.org/10.18653/v1/N19-1357"><span>A</span>ttention is not <span>E</span>xplanation</a>. In Jill Burstein, Christy Doran, and Thamar Solorio, editors, <em>Proceedings of the 2019 conference of the north <span>A</span>merican chapter of the association for computational linguistics: Human language technologies, volume 1 (long and short papers)</em>, pages 3543–3556, Minneapolis, Minnesota. Association for Computational Linguistics.
</div>
<div id="ref-jastrzebski-etal-2018-residual" class="csl-entry" role="listitem">
Stanisław Jastrzebski, Devansh Arpit, Nicolas Ballas, Vikas Verma, Tong Che, and Yoshua Bengio. 2018. <a href="https://openreview.net/forum?id=SJa9iHgAZ">Residual connections encourage iterative inference</a>. In <em>International conference on learning representations</em>.
</div>
<div id="ref-kalchbrenner-blunsom-2013-recurrent" class="csl-entry" role="listitem">
Nal Kalchbrenner and Phil Blunsom. 2013. <a href="https://aclanthology.org/D13-1176/">Recurrent continuous translation models</a>. In David Yarowsky, Timothy Baldwin, Anna Korhonen, Karen Livescu, and Steven Bethard, editors, <em>Proceedings of the 2013 conference on empirical methods in natural language processing</em>, pages 1700–1709, Seattle, Washington, USA. Association for Computational Linguistics.
</div>
<div id="ref-karimova-etal-2018-user" class="csl-entry" role="listitem">
Sariya Karimova, Patrick Simianer, and Stefan Riezler. 2018. <a href="https://doi.org/10.1007/s10590-018-9224-8">A user-study on online adaptation of neural machine translation to human post-edits</a>. <em>Machine Translation</em>, 32(4):309–324.
</div>
<div id="ref-kepler-etal-2019-openkiwi" class="csl-entry" role="listitem">
Fabio Kepler, Jonay Trénous, Marcos Treviso, Miguel Vera, and André F. T. Martins. 2019. <a href="https://doi.org/10.18653/v1/P19-3020"><span>O</span>pen<span>K</span>iwi: An open source framework for quality estimation</a>. In Marta R. Costa-jussà and Enrique Alfonseca, editors, <em>Proceedings of the 57th annual meeting of the association for computational linguistics: System demonstrations</em>, pages 117–122, Florence, Italy. Association for Computational Linguistics.
</div>
<div id="ref-kim-etal-2019-pivot" class="csl-entry" role="listitem">
Yunsu Kim, Petre Petrov, Pavel Petrushkov, Shahram Khadivi, and Hermann Ney. 2019. <a href="https://doi.org/10.18653/v1/D19-1080">Pivot-based transfer learning for neural machine translation between non-<span>E</span>nglish languages</a>. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors, <em>Proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing (EMNLP-IJCNLP)</em>, pages 866–876, Hong Kong, China. Association for Computational Linguistics.
</div>
<div id="ref-kobayashi-etal-2020-attention" class="csl-entry" role="listitem">
Goro Kobayashi, Tatsuki Kuribayashi, Sho Yokoi, and Kentaro Inui. 2020. <a href="https://doi.org/10.18653/v1/2020.emnlp-main.574">Attention is not only a weight: Analyzing transformers with vector norms</a>. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, <em>Proceedings of the 2020 conference on empirical methods in natural language processing (EMNLP)</em>, pages 7057–7075, Online. Association for Computational Linguistics.
</div>
<div id="ref-kobayashi-etal-2021-incorporating" class="csl-entry" role="listitem">
Goro Kobayashi, Tatsuki Kuribayashi, Sho Yokoi, and Kentaro Inui. 2021. <a href="https://doi.org/10.18653/v1/2021.emnlp-main.373"><span>I</span>ncorporating <span>R</span>esidual and <span>N</span>ormalization <span>L</span>ayers into <span>A</span>nalysis of <span>M</span>asked <span>L</span>anguage <span>M</span>odels</a>. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, <em>Proceedings of the 2021 conference on empirical methods in natural language processing</em>, pages 4547–4568, Online; Punta Cana, Dominican Republic. Association for Computational Linguistics.
</div>
<div id="ref-kocmi-etal-2024-findings" class="csl-entry" role="listitem">
Tom Kocmi, Eleftherios Avramidis, Rachel Bawden, Ondřej Bojar, Anton Dvorkovich, Christian Federmann, Mark Fishel, Markus Freitag, Thamme Gowda, Roman Grundkiewicz, Barry Haddow, Marzena Karpinska, Philipp Koehn, Benjamin Marie, Christof Monz, Kenton Murray, Masaaki Nagata, Martin Popel, Maja Popović, et al. 2024a. <a href="https://doi.org/10.18653/v1/2024.wmt-1.1">Findings of the <span>WMT</span>24 general machine translation shared task: The <span>LLM</span> era is here but <span>MT</span> is not solved yet</a>. In Barry Haddow, Tom Kocmi, Philipp Koehn, and Christof Monz, editors, <em>Proceedings of the ninth conference on machine translation</em>, pages 1–46, Miami, Florida, USA. Association for Computational Linguistics.
</div>
<div id="ref-kocmi-federmann-2023-gemba" class="csl-entry" role="listitem">
Tom Kocmi and Christian Federmann. 2023. <a href="https://doi.org/10.18653/v1/2023.wmt-1.64"><span>GEMBA</span>-<span>MQM</span>: Detecting translation quality error spans with <span>GPT</span>-4</a>. In Philipp Koehn, Barry Haddow, Tom Kocmi, and Christof Monz, editors, <em>Proceedings of the eighth conference on machine translation</em>, pages 768–775, Singapore. Association for Computational Linguistics.
</div>
<div id="ref-kocmi-etal-2024-error" class="csl-entry" role="listitem">
Tom Kocmi, Vilém Zouhar, Eleftherios Avramidis, Roman Grundkiewicz, Marzena Karpinska, Maja Popović, Mrinmaya Sachan, and Mariya Shmatova. 2024b. <a href="https://doi.org/10.18653/v1/2024.wmt-1.131">Error span annotation: A balanced approach for human evaluation of machine translation</a>. In Barry Haddow, Tom Kocmi, Philipp Koehn, and Christof Monz, editors, <em>Proceedings of the ninth conference on machine translation</em>, pages 1440–1453, Miami, Florida, USA. Association for Computational Linguistics.
</div>
<div id="ref-koehn-etal-2003-statistical" class="csl-entry" role="listitem">
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003. <a href="https://aclanthology.org/N03-1017/">Statistical phrase-based translation</a>. In <em>Proceedings of the 2003 human language technology conference of the north <span>A</span>merican chapter of the association for computational linguistics</em>, pages 127–133.
</div>
<div id="ref-kohn-2015-whats" class="csl-entry" role="listitem">
Arne Köhn. 2015. <a href="https://doi.org/10.18653/v1/D15-1246">What‘s in an embedding? Analyzing word embeddings through multilingual evaluation</a>. In Lluís Màrquez, Chris Callison-Burch, and Jian Su, editors, <em>Proceedings of the 2015 conference on empirical methods in natural language processing</em>, pages 2067–2073, Lisbon, Portugal. Association for Computational Linguistics.
</div>
<div id="ref-kokhlikyan-etal-2020-captum" class="csl-entry" role="listitem">
Narine Kokhlikyan, Vivek Miglani, Miguel Martin, Edward Wang, Bilal Alsallakh, Jonathan Reynolds, Alexander Melnikov, Natalia Kliushkina, Carlos Araya, Siqi Yan, and Orion Reblitz-Richardson. 2020. <a href="https://arxiv.org/abs/2009.07896">Captum: A unified and generic model interpretability library for PyTorch</a>. <em>ArXiv</em>.
</div>
<div id="ref-krings-2001-repairing" class="csl-entry" role="listitem">
Hans P. Krings. 2001. <em>Repairing texts: Empirical investigations of machine translation post-editing processes</em>. Kent State University Press.
</div>
<div id="ref-krishna-etal-2024-disagreement" class="csl-entry" role="listitem">
Satyapriya Krishna, Tessa Han, Alex Gu, Steven Wu, Shahin Jabbari, and Himabindu Lakkaraju. 2024. <a href="https://openreview.net/forum?id=jESY2WTZCe">The disagreement problem in explainable machine learning: A practitioner<span>’</span>s perspective</a>. <em>Transactions on Machine Learning Research</em>.
</div>
<div id="ref-lage-etal-2019-evaluation" class="csl-entry" role="listitem">
Isaac Lage, Emily Chen, Jeffrey He, Menaka Narayanan, Been Kim, Sam Gershman, and Finale Doshi-Velez. 2019. <a href="http://arxiv.org/abs/1902.00006">An evaluation of the human-interpretability of explanation</a>. <em>ArXiv</em>, abs/1902.00006.
</div>
<div id="ref-laubli-etal-2019-post" class="csl-entry" role="listitem">
Samuel Läubli, Chantal Amrhein, Patrick Düggelin, Beatriz Gonzalez, Alena Zwahlen, and Martin Volk. 2019. <a href="https://aclanthology.org/W19-6626/">Post-editing productivity with neural machine translation: An empirical assessment of speed and quality in the banking and finance domain</a>. In Mikel Forcada, Andy Way, Barry Haddow, and Rico Sennrich, editors, <em>Proceedings of machine translation summit XVII: Research track</em>, pages 267–272, Dublin, Ireland. European Association for Machine Translation.
</div>
<div id="ref-laubli-etal-2013-assessing" class="csl-entry" role="listitem">
Samuel Läubli, Mark Fishel, Gary Massey, Maureen Ehrensberger-Dow, and Martin Volk. 2013. <a href="https://aclanthology.org/2013.mtsummit-wptp.10/">Assessing post-editing efficiency in a realistic translation environment</a>. In Sharon O’Brien, Michel Simard, and Lucia Specia, editors, <em>Proceedings of the 2nd workshop on post-editing technology and practice</em>, Nice, France.
</div>
<div id="ref-laubli-etal-2018-machine" class="csl-entry" role="listitem">
Samuel Läubli, Rico Sennrich, and Martin Volk. 2018. <a href="https://doi.org/10.18653/v1/D18-1512">Has machine translation achieved human parity? A case for document-level evaluation</a>. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun’ichi Tsujii, editors, <em>Proceedings of the 2018 conference on empirical methods in natural language processing</em>, pages 4791–4796, Brussels, Belgium. Association for Computational Linguistics.
</div>
<div id="ref-lee-etal-2023-survey" class="csl-entry" role="listitem">
Seungjun Lee, Jungseob Lee, Hyeonseok Moon, Chanjun Park, Jaehyung Seo, Sugyeong Eo, Seonmin Koo, and Heuiseok Lim. 2023. <a href="https://doi.org/10.3390/math11041006">A survey on evaluation metrics for machine translation</a>. <em>Mathematics</em>, 11(4).
</div>
<div id="ref-leiter-etal-2024-towards" class="csl-entry" role="listitem">
Christoph Leiter, Piyawat Lertvittayakumjorn, Marina Fomicheva, Wei Zhao, Yang Gao, and Steffen Eger. 2024. <a href="http://jmlr.org/papers/v25/22-0416.html">Towards explainable evaluation metrics for machine translation</a>. <em>Journal of Machine Learning Research</em>, 25(75):1–49.
</div>
<div id="ref-lewis-etal-2020-rag" class="csl-entry" role="listitem">
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-augmented generation for knowledge-intensive NLP tasks. In <em>Proceedings of the 34th international conference on neural information processing systems</em>, Red Hook, NY, USA. Curran Associates Inc.
</div>
<div id="ref-li-etal-2016-visualizing" class="csl-entry" role="listitem">
Jiwei Li, Xinlei Chen, Eduard Hovy, and Dan Jurafsky. 2016. <a href="https://doi.org/10.18653/v1/N16-1082">Visualizing and understanding neural models in <span>NLP</span></a>. In Kevin Knight, Ani Nenkova, and Owen Rambow, editors, <em>Proceedings of the 2016 conference of the north <span>A</span>merican chapter of the association for computational linguistics: Human language technologies</em>, pages 681–691, San Diego, California. Association for Computational Linguistics.
</div>
<div id="ref-liu-etal-2024-beyond-human" class="csl-entry" role="listitem">
Zhongtao Liu, Parker Riley, Daniel Deutsch, Alison Lui, Mengmeng Niu, Apurva Shah, and Markus Freitag. 2024. <a href="https://doi.org/10.18653/v1/2024.wmt-1.110">Beyond human-only: Evaluating human-machine collaboration for collecting high-quality translation data</a>. In Barry Haddow, Tom Kocmi, Philipp Koehn, and Christof Monz, editors, <em>Proceedings of the ninth conference on machine translation</em>, pages 1095–1106, Miami, Florida, USA. Association for Computational Linguistics.
</div>
<div id="ref-burchardt-2013-multidimensional" class="csl-entry" role="listitem">
Arle Richard Lommel, Aljoscha Burchardt, and Hans Uszkoreit. 2013. <a href="https://aclanthology.org/2013.tc-1.6/">Multidimensional quality metrics: A flexible system for assessing translation quality</a>. In <em>Proceedings of translating and the computer 35</em>, London, UK. Aslib.
</div>
<div id="ref-lommel-etal-2024-multi" class="csl-entry" role="listitem">
Arle Lommel, Serge Gladkoff, Alan Melby, Sue Ellen Wright, Ingemar Strandvik, Katerina Gasova, Angelika Vaasa, Andy Benzo, Romina Marazzato Sparano, Monica Foresi, Johani Innis, Lifeng Han, and Goran Nenadic. 2024. <a href="https://aclanthology.org/2024.amta-presentations.6/">The multi-range theory of translation quality measurement: <span>MQM</span> scoring models and statistical quality control</a>. In Marianna Martindale, Janice Campbell, Konstantin Savenkov, and Shivali Goel, editors, <em>Proceedings of the 16th conference of the association for machine translation in the americas (volume 2: presentations)</em>, pages 75–94, Chicago, USA. Association for Machine Translation in the Americas.
</div>
<div id="ref-lu-etal-2022-fantastically" class="csl-entry" role="listitem">
Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2022. <a href="https://doi.org/10.18653/v1/2022.acl-long.556">Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity</a>. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, <em>Proceedings of the 60th annual meeting of the association for computational linguistics (volume 1: Long papers)</em>, pages 8086–8098, Dublin, Ireland. Association for Computational Linguistics.
</div>
<div id="ref-lundberg-lee-2017-shap" class="csl-entry" role="listitem">
Scott M. Lundberg and Su-In Lee. 2017. <a href="https://dl.acm.org/doi/10.5555/3295222.3295230">A unified approach to interpreting model predictions</a>. In <em>Proceedings of the 31st international conference on neural information processing systems</em>, volume 30, pages 4768–4777, Long Beach, California, USA. Curran Associates Inc.
</div>
<div id="ref-madsen-etal-2022-posthoc" class="csl-entry" role="listitem">
Andreas Madsen, Siva Reddy, and Sarath Chandar. 2022. <a href="https://doi.org/10.1145/3546577">Post-hoc interpretability for neural NLP: A survey</a>. <em>ACM Comput. Surv.</em>, 55(8).
</div>
<div id="ref-marks-tegmark-2024-geometry" class="csl-entry" role="listitem">
Samuel Marks and Max Tegmark. 2024. <a href="https://arxiv.org/abs/2310.06824">The geometry of truth: Emergent linear structure in large language model representations of true/false datasets</a>. In <em>Proceedings of the 1st conference on language modeling (COLM)</em>.
</div>
<div id="ref-martindale-carpuat-2018-fluency" class="csl-entry" role="listitem">
Marianna Martindale and Marine Carpuat. 2018. <a href="https://aclanthology.org/W18-1803/">Fluency over adequacy: A pilot study in measuring user trust in imperfect <span>MT</span></a>. In Colin Cherry and Graham Neubig, editors, <em>Proceedings of the 13th conference of the association for machine translation in the <span>A</span>mericas (volume 1: Research track)</em>, pages 13–25, Boston, MA. Association for Machine Translation in the Americas.
</div>
<div id="ref-maruf-haffari-2018-document" class="csl-entry" role="listitem">
Sameen Maruf and Gholamreza Haffari. 2018. <a href="https://doi.org/10.18653/v1/P18-1118">Document context neural machine translation with memory networks</a>. In Iryna Gurevych and Yusuke Miyao, editors, <em>Proceedings of the 56th annual meeting of the association for computational linguistics (volume 1: Long papers)</em>, pages 1275–1284, Melbourne, Australia. Association for Computational Linguistics.
</div>
<div id="ref-mccoy-etal-2019-right" class="csl-entry" role="listitem">
R. Thomas McCoy, Ellie Pavlick, and Tal Linzen. 2019. <a href="https://doi.org/10.18653/v1/P19-1334">Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference</a>. In Anna Korhonen, David Traum, and Lluís Màrquez, editors, <em>Proceedings of the 57th annual meeting of the association for computational linguistics</em>, pages 3428–3448, Florence, Italy. Association for Computational Linguistics.
</div>
<div id="ref-mehandru-etal-2023-physician" class="csl-entry" role="listitem">
Nikita Mehandru, Sweta Agrawal, Yimin Xiao, Ge Gao, Elaine Khoong, Marine Carpuat, and Niloufar Salehi. 2023. <a href="https://doi.org/10.18653/v1/2023.emnlp-main.712">Physician detection of clinical harm in machine translation: Quality estimation aids in reliance and backtranslation identifies critical errors</a>. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, <em>Proceedings of the 2023 conference on empirical methods in natural language processing</em>, pages 11633–11647, Singapore. Association for Computational Linguistics.
</div>
<div id="ref-miculicich-etal-2018-document" class="csl-entry" role="listitem">
Lesly Miculicich, Dhananjay Ram, Nikolaos Pappas, and James Henderson. 2018. <a href="https://doi.org/10.18653/v1/D18-1325">Document-level neural machine translation with hierarchical attention networks</a>. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun’ichi Tsujii, editors, <em>Proceedings of the 2018 conference on empirical methods in natural language processing</em>, pages 2947–2954, Brussels, Belgium. Association for Computational Linguistics.
</div>
<div id="ref-miglani-etal-2023-using" class="csl-entry" role="listitem">
Vivek Miglani, Aobo Yang, Aram Markosyan, Diego Garcia-Olano, and Narine Kokhlikyan. 2023. <a href="https://doi.org/10.18653/v1/2023.nlposs-1.19">Using captum to explain generative language models</a>. In Liling Tan, Dmitrijs Milajevs, Geeticka Chauhan, Jeremy Gwinnup, and Elijah Rippeth, editors, <em>Proceedings of the 3rd workshop for natural language processing open source software (NLP-OSS 2023)</em>, pages 165–173, Singapore. Association for Computational Linguistics.
</div>
<div id="ref-mikolov-etal-2013-linguistic" class="csl-entry" role="listitem">
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013. <a href="https://aclanthology.org/N13-1090/">Linguistic regularities in continuous space word representations</a>. In Lucy Vanderwende, Hal Daumé III, and Katrin Kirchhoff, editors, <em>Proceedings of the 2013 conference of the north <span>A</span>merican chapter of the association for computational linguistics: Human language technologies</em>, pages 746–751, Atlanta, Georgia. Association for Computational Linguistics.
</div>
<div id="ref-mohebbi-etal-2023-quantifying" class="csl-entry" role="listitem">
Hosein Mohebbi, Willem Zuidema, Grzegorz Chrupała, and Afra Alishahi. 2023. <a href="https://doi.org/10.18653/v1/2023.eacl-main.245">Quantifying context mixing in transformers</a>. In Andreas Vlachos and Isabelle Augenstein, editors, <em>Proceedings of the 17th conference of the european chapter of the association for computational linguistics</em>, pages 3378–3400, Dubrovnik, Croatia. Association for Computational Linguistics.
</div>
<div id="ref-moran-etal-2014-towards" class="csl-entry" role="listitem">
John Moran, Christian Saam, and Dave Lewis. 2014. <a href="https://aclanthology.org/2014.amta-wptp.8/">Towards desktop-based <span>CAT</span> tool instrumentation</a>. In Sharon O’Brien, Michel Simard, and Lucia Specia, editors, <em>Proceedings of the 11th conference of the association for machine translation in the americas</em>, pages 99–112, Vancouver, Canada. Association for Machine Translation in the Americas.
</div>
<div id="ref-muller-etal-2018-large" class="csl-entry" role="listitem">
Mathias Müller, Annette Rios, Elena Voita, and Rico Sennrich. 2018. <a href="https://doi.org/10.18653/v1/W18-6307">A large-scale test set for the evaluation of context-aware pronoun translation in neural machine translation</a>. In Ondřej Bojar, Rajen Chatterjee, Christian Federmann, Mark Fishel, Yvette Graham, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes, Philipp Koehn, Christof Monz, Matteo Negri, Aurélie Névéol, Mariana Neves, Matt Post, Lucia Specia, Marco Turchi, and Karin Verspoor, editors, <em>Proceedings of the third conference on machine translation: Research papers</em>, pages 61–72, Brussels, Belgium. Association for Computational Linguistics.
</div>
<div id="ref-nllb-2024-scaling" class="csl-entry" role="listitem">
NLLB Team, Marta R. Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula, Loic Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti, et al. 2024. <a href="https://doi.org/10.1038/s41586-024-07335-x">Scaling neural machine translation to 200 languages</a>. <em>Nature</em>, 630(8018):841–846.
</div>
<div id="ref-logitlens" class="csl-entry" role="listitem">
nostalgebraist. 2020. <a href="https://www.alignmentforum.org/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens">Interpreting <span>GPT</span>: The logit lens</a>. <em>AI Alignment Forum</em>.
</div>
<div id="ref-och-etal-1999-improved" class="csl-entry" role="listitem">
Franz Josef Och, Christoph Tillmann, and Hermann Ney. 1999. <a href="https://aclanthology.org/W99-0604/">Improved alignment models for statistical machine translation</a>. In <em>1999 joint <span>SIGDAT</span> conference on empirical methods in natural language processing and very large corpora</em>.
</div>
<div id="ref-olah-2023-distributed" class="csl-entry" role="listitem">
Chris Olah. 2023. <a href="https://transformer-circuits.pub/2023/superposition-composition/index.html">Distributed representations: Composition &amp; superposition</a>. <em>Transformer Circuits Thread</em>.
</div>
<div id="ref-olshausen-field-1997-sparse" class="csl-entry" role="listitem">
Bruno A. Olshausen and David J. Field. 1997. <a href="https://doi.org/10.1016/S0042-6989(97)00169-7">Sparse coding with an overcomplete basis set: A strategy employed by V1?</a> <em>Vision Research</em>, 37(23):3311–3325.
</div>
<div id="ref-papineni-etal-2002-bleu" class="csl-entry" role="listitem">
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. <a href="https://doi.org/10.3115/1073083.1073135"><span>B</span>leu: A method for automatic evaluation of machine translation</a>. In Pierre Isabelle, Eugene Charniak, and Dekang Lin, editors, <em>Proceedings of the 40th annual meeting of the association for computational linguistics</em>, pages 311–318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.
</div>
<div id="ref-park-etal-2023-linear" class="csl-entry" role="listitem">
Kiho Park, Yo Joong Choe, and Victor Veitch. 2023. <a href="https://openreview.net/forum?id=T0PoOJg8cK">The linear representation hypothesis and the geometry of large language models</a>. In <em>Causal representation learning workshop at NeurIPS 2023</em>.
</div>
<div id="ref-parra-escartin-arcedillo-2015-machine" class="csl-entry" role="listitem">
Carla Parra Escartín and Manuel Arcedillo. 2015. <a href="https://aclanthology.org/2015.mtsummit-papers.11/">Machine translation evaluation made fuzzier: A study on post-editing productivity and evaluation metrics in commercial settings</a>. In <em>Proceedings of machine translation summit XV: papers</em>, Miami, USA.
</div>
<div id="ref-pennington-etal-2014-glove" class="csl-entry" role="listitem">
Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. <a href="https://doi.org/10.3115/v1/D14-1162"><span>G</span>lo<span>V</span>e: Global vectors for word representation</a>. In Alessandro Moschitti, Bo Pang, and Walter Daelemans, editors, <em>Proceedings of the 2014 conference on empirical methods in natural language processing (<span>EMNLP</span>)</em>, pages 1532–1543, Doha, Qatar. Association for Computational Linguistics.
</div>
<div id="ref-petroni-etal-2020-how" class="csl-entry" role="listitem">
Fabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim Rocktäschel, Yuxiang Wu, Alexander H. Miller, and Sebastian Riedel. 2020. <a href="https://openreview.net/forum?id=025X0zPfn">How context affects language models’ factual predictions</a>. In <em>Automated knowledge base construction</em>.
</div>
<div id="ref-pierse-2021-transformers" class="csl-entry" role="listitem">
Charles Pierse. 2021. <a href="https://github.com/cdpierse/transformers-interpret">Transformers interpret</a>.
</div>
<div id="ref-plitt-masselot-2010-productivity" class="csl-entry" role="listitem">
Mirko Plitt and François Masselot. 2010. <a href="https://doi.org/10.2478/v10108-010-0010-x">A <span>Productivity</span> <span>Test</span> of <span>Statistical</span> <span>Machine</span> <span>Translation</span> <span>Post</span>-<span>Editing</span> in a <span>Typical</span> <span>Localisation</span> <span>Context</span></a>. <em>The Prague Bulletin of Mathematical Linguistics</em>, 93(1).
</div>
<div id="ref-popovic-2015-chrf" class="csl-entry" role="listitem">
Maja Popović. 2015. <a href="https://doi.org/10.18653/v1/W15-3049">Chr<span>F</span>: Character n-gram <span>F</span>-score for automatic <span>MT</span> evaluation</a>. In Ondřej Bojar, Rajan Chatterjee, Christian Federmann, Barry Haddow, Chris Hokamp, Matthias Huck, Varvara Logacheva, and Pavel Pecina, editors, <em>Proceedings of the tenth workshop on statistical machine translation</em>, pages 392–395, Lisbon, Portugal. Association for Computational Linguistics.
</div>
<div id="ref-popovic-2020-informative" class="csl-entry" role="listitem">
Maja Popović. 2020. <a href="https://doi.org/10.18653/v1/2020.coling-main.444">Informative manual evaluation of machine translation output</a>. In Donia Scott, Nuria Bel, and Chengqing Zong, editors, <em>Proceedings of the 28th international conference on computational linguistics</em>, pages 5059–5069, Barcelona, Spain (Online). International Committee on Computational Linguistics.
</div>
<div id="ref-rei-etal-2022-comet" class="csl-entry" role="listitem">
Ricardo Rei, José G. C. de Souza, Duarte Alves, Chrysoula Zerva, Ana C Farinha, Taisiya Glushkova, Alon Lavie, Luisa Coheur, and André F. T. Martins. 2022a. <a href="https://aclanthology.org/2022.wmt-1.52/"><span>COMET</span>-22: Unbabel-<span>IST</span> 2022 submission for the metrics shared task</a>. In Philipp Koehn, Loïc Barrault, Ondřej Bojar, Fethi Bougares, Rajen Chatterjee, Marta R. Costa-jussà, Christian Federmann, Mark Fishel, Alexander Fraser, Markus Freitag, Yvette Graham, Roman Grundkiewicz, Paco Guzman, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes, Tom Kocmi, André Martins, Makoto Morishita, et al., editors, <em>Proceedings of the seventh conference on machine translation (WMT)</em>, pages 578–585, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics.
</div>
<div id="ref-rei-etal-2022-searching" class="csl-entry" role="listitem">
Ricardo Rei, Ana C Farinha, José G. C. de Souza, Pedro G. Ramos, André F. T. Martins, Luisa Coheur, and Alon Lavie. 2022b. <a href="https://aclanthology.org/2022.eamt-1.9/">Searching for <span>COMETINHO</span>: The little metric that could</a>. In Helena Moniz, Lieve Macken, Andrew Rufener, Loïc Barrault, Marta R. Costa-jussà, Christophe Declercq, Maarit Koponen, Ellie Kemp, Spyridon Pilos, Mikel L. Forcada, Carolina Scarton, Joachim Van den Bogaert, Joke Daems, Arda Tezcan, Bram Vanroy, and Margot Fonteyne, editors, <em>Proceedings of the 23rd annual conference of the european association for machine translation</em>, pages 61–70, Ghent, Belgium. European Association for Machine Translation.
</div>
<div id="ref-rei-etal-2021-references" class="csl-entry" role="listitem">
Ricardo Rei, Ana C Farinha, Chrysoula Zerva, Daan van Stigt, Craig Stewart, Pedro Ramos, Taisiya Glushkova, André F. T. Martins, and Alon Lavie. 2021. <a href="https://aclanthology.org/2021.wmt-1.111/">Are references really needed? Unbabel-<span>IST</span> 2021 submission for the metrics shared task</a>. In Loic Barrault, Ondrej Bojar, Fethi Bougares, Rajen Chatterjee, Marta R. Costa-jussa, Christian Federmann, Mark Fishel, Alexander Fraser, Markus Freitag, Yvette Graham, Roman Grundkiewicz, Paco Guzman, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes, Philipp Koehn, Tom Kocmi, Andre Martins, Makoto Morishita, et al., editors, <em>Proceedings of the sixth conference on machine translation</em>, pages 1030–1040, Online. Association for Computational Linguistics.
</div>
<div id="ref-rei-etal-2023-inside" class="csl-entry" role="listitem">
Ricardo Rei, Nuno M. Guerreiro, Marcos Treviso, Luisa Coheur, Alon Lavie, and André Martins. 2023. <a href="https://doi.org/10.18653/v1/2023.acl-short.94">The inside story: Towards better understanding of machine translation neural evaluation metrics</a>. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, <em>Proceedings of the 61st annual meeting of the association for computational linguistics (volume 2: Short papers)</em>, pages 1089–1105, Toronto, Canada. Association for Computational Linguistics.
</div>
<div id="ref-rei-etal-2024-tower" class="csl-entry" role="listitem">
Ricardo Rei, Jose Pombal, Nuno M. Guerreiro, João Alves, Pedro Henrique Martins, Patrick Fernandes, Helena Wu, Tania Vaz, Duarte Alves, Amin Farajian, Sweta Agrawal, Antonio Farinhas, José G. C. De Souza, and André Martins. 2024. <a href="https://doi.org/10.18653/v1/2024.wmt-1.12">Tower v2: Unbabel-<span>IST</span> 2024 submission for the general <span>MT</span> shared task</a>. In Barry Haddow, Tom Kocmi, Philipp Koehn, and Christof Monz, editors, <em>Proceedings of the ninth conference on machine translation</em>, pages 185–204, Miami, Florida, USA. Association for Computational Linguistics.
</div>
<div id="ref-rei-etal-2020-comet" class="csl-entry" role="listitem">
Ricardo Rei, Craig Stewart, Ana C Farinha, and Alon Lavie. 2020. <a href="https://doi.org/10.18653/v1/2020.emnlp-main.213"><span>COMET</span>: A neural framework for <span>MT</span> evaluation</a>. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, <em>Proceedings of the 2020 conference on empirical methods in natural language processing (EMNLP)</em>, pages 2685–2702, Online. Association for Computational Linguistics.
</div>
<div id="ref-ribeiro-etal-2016-lime" class="csl-entry" role="listitem">
Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. <a href="https://doi.org/10.1145/2939672.2939778">"Why should i trust you?": Explaining the predictions of any classifier</a>. In <em>Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining</em>, pages 1135–1144, New York, NY, USA. Association for Computing Machinery.
</div>
<div id="ref-rogers-etal-2020-primer" class="csl-entry" role="listitem">
Anna Rogers, Olga Kovaleva, and Anna Rumshisky. 2020. <a href="https://doi.org/10.1162/tacl_a_00349">A primer in <span>BERT</span>ology: What we know about how <span>BERT</span> works</a>. <em>Transactions of the Association for Computational Linguistics</em>, 8:842–866.
</div>
<div id="ref-rubino-etal-2021-error" class="csl-entry" role="listitem">
Raphael Rubino, Atsushi Fujita, and Benjamin Marie. 2021. <a href="https://doi.org/10.18653/v1/2021.eval4nlp-1.15">Error identification for machine translation with metric embedding and attention</a>. In Yang Gao, Steffen Eger, Wei Zhao, Piyawat Lertvittayakumjorn, and Marina Fomicheva, editors, <em>Proceedings of the 2nd workshop on evaluation and comparison of NLP systems</em>, pages 146–156, Punta Cana, Dominican Republic. Association for Computational Linguistics.
</div>
<div id="ref-rumelhart-mcclelland-1987-learning" class="csl-entry" role="listitem">
David E. Rumelhart and James L. McClelland. 1987. Learning internal representations by error propagation. In <em>Parallel distributed processing: Explorations in the microstructure of cognition: foundations</em>, pages 318–362. MIT Press.
</div>
<div id="ref-sanh2022multitask" class="csl-entry" role="listitem">
Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, et al. 2022. <a href="https://openreview.net/forum?id=9Vrb9D0WI4">Multitask prompted training enables zero-shot task generalization</a>. In <em>Proceedings of the tenth international conference on learning representations (ICLR)</em>.
</div>
<div id="ref-sarti-etal-2024-quantifying" class="csl-entry" role="listitem">
Gabriele Sarti, Grzegorz Chrupała, Malvina Nissim, and Arianna Bisazza. 2024. <a href="https://openreview.net/forum?id=XTHfNGI3zT">Quantifying the plausibility of context reliance in neural machine translation</a>. In <em>The twelfth international conference on learning representations (ICLR 2024)</em>, Vienna, Austria. OpenReview.
</div>
<div id="ref-savoldi-etal-2025-translation" class="csl-entry" role="listitem">
Beatrice Savoldi, Alan Ramponi, Matteo Negri, and Luisa Bentivogli. 2025. <a href="https://arxiv.org/abs/2502.13780">Translation in the hands of many: Centering lay users in machine translation interactions</a>.
</div>
<div id="ref-scalena-etal-2024-multi" class="csl-entry" role="listitem">
Daniel Scalena, Gabriele Sarti, and Malvina Nissim. 2024. <a href="https://doi.org/10.18653/v1/2024.blackboxnlp-1.34">Multi-property steering of large language models with dynamic activation composition</a>. In Yonatan Belinkov, Najoung Kim, Jaap Jumelet, Hosein Mohebbi, Aaron Mueller, and Hanjie Chen, editors, <em>Proceedings of the 7th BlackboxNLP workshop: Analyzing and interpreting neural networks for NLP</em>, pages 577–603, Miami, Florida, US. Association for Computational Linguistics.
</div>
<div id="ref-schulman-etal-2017-proximal" class="csl-entry" role="listitem">
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. <a href="https://arxiv.org/abs/1707.06347">Proximal policy optimization algorithms</a>.
</div>
<div id="ref-schwenk-etal-2021-ccmatrix" class="csl-entry" role="listitem">
Holger Schwenk, Guillaume Wenzek, Sergey Edunov, Edouard Grave, Armand Joulin, and Angela Fan. 2021. <a href="https://doi.org/10.18653/v1/2021.acl-long.507"><span>CCM</span>atrix: Mining billions of high-quality parallel sentences on the web</a>. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, <em>Proceedings of the 59th annual meeting of the association for computational linguistics and the 11th international joint conference on natural language processing (volume 1: Long papers)</em>, pages 6490–6500, Online. Association for Computational Linguistics.
</div>
<div id="ref-sellam-etal-2020-bleurt" class="csl-entry" role="listitem">
Thibault Sellam, Dipanjan Das, and Ankur Parikh. 2020. <a href="https://doi.org/10.18653/v1/2020.acl-main.704"><span>BLEURT</span>: Learning robust metrics for text generation</a>. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault, editors, <em>Proceedings of the 58th annual meeting of the association for computational linguistics</em>, pages 7881–7892, Online. Association for Computational Linguistics.
</div>
<div id="ref-sennrich-etal-2016-neural" class="csl-entry" role="listitem">
Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. <a href="https://doi.org/10.18653/v1/P16-1162">Neural machine translation of rare words with subword units</a>. In Katrin Erk and Noah A. Smith, editors, <em>Proceedings of the 54th annual meeting of the association for computational linguistics (volume 1: Long papers)</em>, pages 1715–1725, Berlin, Germany. Association for Computational Linguistics.
</div>
<div id="ref-shenoy-etal-2021-investigating" class="csl-entry" role="listitem">
Raksha Shenoy, Nico Herbig, Antonio Krüger, and Josef van Genabith. 2021. <a href="https://doi.org/10.18653/v1/2021.emnlp-main.799">Investigating the helpfulness of word-level quality estimation for post-editing machine translation output</a>. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, <em>Proceedings of the 2021 conference on empirical methods in natural language processing</em>, pages 10173–10185, Online; Punta Cana, Dominican Republic. Association for Computational Linguistics.
</div>
<div id="ref-simonyan-etal-2014-saliency" class="csl-entry" role="listitem">
Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. 2014. <a href="http://arxiv.org/abs/1312.6034">Deep inside convolutional networks: Visualising image classification models and saliency maps</a>. In Yoshua Bengio and Yann LeCun, editors, <em>2nd international conference on learning representations, (<span>ICLR</span>)</em>, Banff, AB, Canada.
</div>
<div id="ref-sixt-etal-2020-explanations" class="csl-entry" role="listitem">
Leon Sixt, Maximilian Granz, and Tim Landgraf. 2020. <a href="https://proceedings.mlr.press/v119/sixt20a.html">When explanations lie: Why many modified <span>BP</span> attributions fail</a>. In Hal Daumé III and Aarti Singh, editors, <em>Proceedings of the 37th international conference on machine learning</em>, volume 119, pages 9046–9057. PMLR.
</div>
<div id="ref-smilkov-etal-2017-smoothgrad" class="csl-entry" role="listitem">
Daniel Smilkov, Nikhil Thorat, Been Kim, Fernanda Viégas, and Martin Wattenberg. 2017. <a href="https://arxiv.org/abs/1706.03825">SmoothGrad: Removing noise by adding noise</a>.
</div>
<div id="ref-smolensky-1986-neural" class="csl-entry" role="listitem">
Paul Smolensky. 1986. Neural and conceptual interpretation of PDP models.
</div>
<div id="ref-snover-etal-2006-study" class="csl-entry" role="listitem">
Matthew Snover, Bonnie Dorr, Rich Schwartz, Linnea Micciulla, and John Makhoul. 2006. <a href="https://aclanthology.org/2006.amta-papers.25/">A study of translation edit rate with targeted human annotation</a>. In <em>Proceedings of the 7th conference of the association for machine translation in the americas: Technical papers</em>, pages 223–231, Cambridge, Massachusetts, USA. Association for Machine Translation in the Americas.
</div>
<div id="ref-specia-etal-2018-quality" class="csl-entry" role="listitem">
Lucia Specia, Carolina Scarton, Gustavo Henrique Paetzold, and Graeme Hirst. 2018. <em>Quality estimation for machine translation</em>. Morgan &amp; Claypool Publishers.
</div>
<div id="ref-specia-etal-2009-estimating" class="csl-entry" role="listitem">
Lucia Specia, Marco Turchi, Nicola Cancedda, Nello Cristianini, and Marc Dymetman. 2009. <a href="https://aclanthology.org/2009.eamt-1.5/">Estimating the sentence-level quality of machine translation systems</a>. In Lluís Màrquez and Harold Somers, editors, <em>Proceedings of the 13th annual conference of the european association for machine translation</em>, Barcelona, Spain. European Association for Machine Translation.
</div>
<div id="ref-dropout" class="csl-entry" role="listitem">
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. <a href="http://jmlr.org/papers/v15/srivastava14a.html">Dropout: A simple way to prevent neural networks from overfitting</a>. <em>Journal of Machine Learning Research</em>, 15(56):1929–1958.
</div>
<div id="ref-su-etal-2024-roformer" class="csl-entry" role="listitem">
Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. 2024. <a href="https://doi.org/10.1016/j.neucom.2023.127063">RoFormer: Enhanced transformer with rotary position embedding</a>. <em>Neurocomputing</em>, 568:127063.
</div>
<div id="ref-sun-etal-2022-investigating" class="csl-entry" role="listitem">
Jiao Sun, Swabha Swayamdipta, Jonathan May, and Xuezhe Ma. 2022. <a href="https://doi.org/10.18653/v1/2022.findings-emnlp.432">Investigating the benefits of free-form rationales</a>. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, <em>Findings of the association for computational linguistics: EMNLP 2022</em>, pages 5867–5882, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.
</div>
<div id="ref-sundararajan-etal-2017-ig" class="csl-entry" role="listitem">
Mukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017. <a href="https://dl.acm.org/doi/10.5555/3305890.3306024">Axiomatic attribution for deep networks</a>. In <em>Proceedings of the 34th international conference on machine learning (ICML)</em>, volume 70, pages 3319–3328, Sydney, Australia. Journal of Machine Learning Research (JMLR).
</div>
<div id="ref-tamchyna-2021-deploying" class="csl-entry" role="listitem">
Aleš Tamchyna. 2021. <a href="https://aclanthology.org/2021.mtsummit-up.21/">Deploying <span>MT</span> quality estimation on a large scale: Lessons learned and open questions</a>. In Janice Campbell, Ben Huyck, Stephen Larocca, Jay Marciano, Konstantin Savenkov, and Alex Yanishevsky, editors, <em>Proceedings of machine translation summit XVIII: Users and providers track</em>, pages 291–305, Virtual. Association for Machine Translation in the Americas.
</div>
<div id="ref-tang-etal-2021-multilingual" class="csl-entry" role="listitem">
Yuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Naman Goyal, Vishrav Chaudhary, Jiatao Gu, and Angela Fan. 2021. <a href="https://doi.org/10.18653/v1/2021.findings-acl.304">Multilingual translation from denoising pre-training</a>. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, <em>Findings of the association for computational linguistics: ACL-IJCNLP 2021</em>, pages 3450–3466, Online. Association for Computational Linguistics.
</div>
<div id="ref-tenney-etal-2019-bert" class="csl-entry" role="listitem">
Ian Tenney, Dipanjan Das, and Ellie Pavlick. 2019. <a href="https://doi.org/10.18653/v1/P19-1452"><span>BERT</span> rediscovers the classical <span>NLP</span> pipeline</a>. In Anna Korhonen, David Traum, and Lluís Màrquez, editors, <em>Proceedings of the 57th annual meeting of the association for computational linguistics</em>, pages 4593–4601, Florence, Italy. Association for Computational Linguistics.
</div>
<div id="ref-tenney-etal-2024-interactive" class="csl-entry" role="listitem">
Ian Tenney, Ryan Mullins, Bin Du, Shree Pandya, Minsuk Kahng, and Lucas Dixon. 2024. <a href="https://arxiv.org/abs/2404.07498">Interactive prompt debugging with sequence salience</a>. <em>Arxiv</em>.
</div>
<div id="ref-tenney-etal-2020-language" class="csl-entry" role="listitem">
Ian Tenney, James Wexler, Jasmijn Bastings, Tolga Bolukbasi, Andy Coenen, Sebastian Gehrmann, Ellen Jiang, Mahima Pushkarna, Carey Radebaugh, Emily Reif, and Ann Yuan. 2020. <a href="https://doi.org/10.18653/v1/2020.emnlp-demos.15">The language interpretability tool: Extensible, interactive visualizations and analysis for <span>NLP</span> models</a>. In Qun Liu and David Schlangen, editors, <em>Proceedings of the 2020 conference on empirical methods in natural language processing: System demonstrations</em>, pages 107–118, Online. Association for Computational Linguistics.
</div>
<div id="ref-thompson-post-2020-automatic" class="csl-entry" role="listitem">
Brian Thompson and Matt Post. 2020. <a href="https://doi.org/10.18653/v1/2020.emnlp-main.8">Automatic machine translation evaluation in many languages via zero-shot paraphrasing</a>. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, <em>Proceedings of the 2020 conference on empirical methods in natural language processing (EMNLP)</em>, pages 90–121, Online. Association for Computational Linguistics.
</div>
<div id="ref-hollinsworth-etal-2024-language" class="csl-entry" role="listitem">
Curt Tigges, Oskar J. Hollinsworth, Atticus Geiger, and Neel Nanda. 2024. <a href="https://doi.org/10.18653/v1/2024.blackboxnlp-1.5">Language models linearly represent sentiment</a>. In Yonatan Belinkov, Najoung Kim, Jaap Jumelet, Hosein Mohebbi, Aaron Mueller, and Hanjie Chen, editors, <em>Proceedings of the 7th BlackboxNLP workshop: Analyzing and interpreting neural networks for NLP</em>, pages 58–87, Miami, Florida, US. Association for Computational Linguistics.
</div>
<div id="ref-toral-etal-2018-attaining" class="csl-entry" role="listitem">
Antonio Toral, Sheila Castilho, Ke Hu, and Andy Way. 2018a. <a href="https://doi.org/10.18653/v1/W18-6312">Attaining the unattainable? Reassessing claims of human parity in neural machine translation</a>. In Ondřej Bojar, Rajen Chatterjee, Christian Federmann, Mark Fishel, Yvette Graham, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes, Philipp Koehn, Christof Monz, Matteo Negri, Aurélie Névéol, Mariana Neves, Matt Post, Lucia Specia, Marco Turchi, and Karin Verspoor, editors, <em>Proceedings of the third conference on machine translation: Research papers</em>, pages 113–123, Brussels, Belgium. Association for Computational Linguistics.
</div>
<div id="ref-toral-etal-2018-postediting" class="csl-entry" role="listitem">
Antonio Toral, Martijn Wieling, and Andy Way. 2018b. <a href="https://doi.org/10.3389/fdigh.2018.00009">Post-editing effort of a novel with statistical and neural machine translation</a>. <em>Frontiers in Digital Humanities</em>, 5:1–11.
</div>
<div id="ref-touvron-etal-2023-llama2" class="csl-entry" role="listitem">
Hugo Touvron, Louis Martin, Kevin R. Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Daniel M. Bikel, Lukas Blecher, Cristian Cantòn Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, et al. 2023. <a href="https://arxiv.org/abs/2307.09288">Llama 2: Open foundation and fine-tuned chat models</a>. <em>ArXiv</em>.
</div>
<div id="ref-turchi-etal-2014-adaptive" class="csl-entry" role="listitem">
Marco Turchi, Antonios Anastasopoulos, José G. C. de Souza, and Matteo Negri. 2014. <a href="https://doi.org/10.3115/v1/P14-1067">Adaptive quality estimation for machine translation</a>. In Kristina Toutanova and Hua Wu, editors, <em>Proceedings of the 52nd annual meeting of the association for computational linguistics (volume 1: Long papers)</em>, pages 710–720, Baltimore, Maryland. Association for Computational Linguistics.
</div>
<div id="ref-turchi-etal-2017-continuous" class="csl-entry" role="listitem">
Marco Turchi, Matteo Negri, M. Amin Farajian, and Marcello Federico. 2017. <a href="https://doi.org/10.1515/pralin-2017-0023">Continuous learning from human post-edits for neural machine translation</a>. <em>The Prague Bulletin of Mathematical Linguistics</em>, 108:233–244.
</div>
<div id="ref-turchi-etal-2013-coping" class="csl-entry" role="listitem">
Marco Turchi, Matteo Negri, and Marcello Federico. 2013. <a href="https://aclanthology.org/W13-2231/">Coping with the subjectivity of human judgements in <span>MT</span> quality estimation</a>. In Ondrej Bojar, Christian Buck, Chris Callison-Burch, Barry Haddow, Philipp Koehn, Christof Monz, Matt Post, Herve Saint-Amand, Radu Soricut, and Lucia Specia, editors, <em>Proceedings of the eighth workshop on statistical machine translation</em>, pages 240–251, Sofia, Bulgaria. Association for Computational Linguistics.
</div>
<div id="ref-vapnik-1995-nature" class="csl-entry" role="listitem">
Vladimir N. Vapnik. 1995. <em>The nature of statistical learning theory</em>. Springer-Verlag New York, Inc.
</div>
<div id="ref-vasconcelos-etal-2025-generation" class="csl-entry" role="listitem">
Helena Vasconcelos, Gagan Bansal, Adam Fourney, Q. Vera Liao, and Jennifer Wortman Vaughan. 2025. <a href="https://doi.org/10.1145/3702320">Generation probabilities are not enough: Uncertainty highlighting in AI code completions</a>. <em>ACM Trans. Comput.-Hum. Interact.</em>, 32(1).
</div>
<div id="ref-vaswani-etal-2017-attention" class="csl-entry" role="listitem">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. <a href="https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf">Attention is all you need</a>. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, <em>Advances in neural information processing systems</em>, volume 30. Curran Associates, Inc.
</div>
<div id="ref-voita-etal-2019-good" class="csl-entry" role="listitem">
Elena Voita, Rico Sennrich, and Ivan Titov. 2019. <a href="https://doi.org/10.18653/v1/P19-1116">When a good translation is wrong in context: Context-aware machine translation improves on deixis, ellipsis, and lexical cohesion</a>. In Anna Korhonen, David Traum, and Lluís Màrquez, editors, <em>Proceedings of the 57th annual meeting of the association for computational linguistics</em>, pages 1198–1212, Florence, Italy. Association for Computational Linguistics.
</div>
<div id="ref-voita-etal-2018-context" class="csl-entry" role="listitem">
Elena Voita, Pavel Serdyukov, Rico Sennrich, and Ivan Titov. 2018. <a href="https://doi.org/10.18653/v1/P18-1117">Context-aware neural machine translation learns anaphora resolution</a>. In Iryna Gurevych and Yusuke Miyao, editors, <em>Proceedings of the 56th annual meeting of the association for computational linguistics (volume 1: Long papers)</em>, pages 1264–1274, Melbourne, Australia. Association for Computational Linguistics.
</div>
<div id="ref-wagner-1983-rapid" class="csl-entry" role="listitem">
Elizabeth Wagner. 1983. <a href="https://aclanthology.org/1983.tc-1.23/">Rapid post-editing of systran</a>. In Veronica Lawson, editor, <em>Proceedings of translating and the computer 5: Tools for the trade</em>, London, UK. Aslib.
</div>
<div id="ref-wang-etal-2023-document-level" class="csl-entry" role="listitem">
Longyue Wang, Chenyang Lyu, Tianbo Ji, Zhirui Zhang, Dian Yu, Shuming Shi, and Zhaopeng Tu. 2023. <a href="https://doi.org/10.18653/v1/2023.emnlp-main.1036">Document-level machine translation with large language models</a>. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, <em>Proceedings of the 2023 conference on empirical methods in natural language processing</em>, pages 16646–16661, Singapore. Association for Computational Linguistics.
</div>
<div id="ref-warstadt-etal-2020-blimp-benchmark" class="csl-entry" role="listitem">
Alex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mohananey, Wei Peng, Sheng-Fu Wang, and Samuel R. Bowman. 2020. <a href="https://doi.org/10.1162/tacl_a_00321"><span>BL</span>i<span>MP</span>: The benchmark of linguistic minimal pairs for <span>E</span>nglish</a>. <em>Transactions of the Association for Computational Linguistics</em>, 8:377–392.
</div>
<div id="ref-white-etal-1994-arpa" class="csl-entry" role="listitem">
John S. White, Theresa A. O’Connell, and Francis E. O’Mara. 1994. <a href="https://aclanthology.org/1994.amta-1.25/">The <span>ARPA</span> <span>MT</span> evaluation methodologies: Evolution, lessons, and future approaches</a>. In <em>Proceedings of the first conference of the association for machine translation in the americas</em>, Columbia, Maryland, USA.
</div>
<div id="ref-wiegreffe-pinter-2019-attention" class="csl-entry" role="listitem">
Sarah Wiegreffe and Yuval Pinter. 2019. <a href="https://doi.org/10.18653/v1/D19-1002">Attention is not not explanation</a>. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors, <em>Proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing (EMNLP-IJCNLP)</em>, pages 11–20, Hong Kong, China. Association for Computational Linguistics.
</div>
<div id="ref-wu-etal-2024-reft" class="csl-entry" role="listitem">
Zhengxuan Wu, Aryaman Arora, Zheng Wang, Atticus Geiger, Dan Jurafsky, Christopher D. Manning, and Christopher Potts. 2024. <a href="https://proceedings.neurips.cc/paper_files/paper/2024/file/75008a0fba53bf13b0bb3b7bff986e0e-Paper-Conference.pdf">ReFT: Representation finetuning for language models</a>. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, <em>Advances in neural information processing systems</em>, volume 37, pages 63908–63962. Curran Associates, Inc.
</div>
<div id="ref-xu-etal-2024-paradigm" class="csl-entry" role="listitem">
Haoran Xu, Young Jin Kim, Amr Sharaf, and Hany Hassan Awadalla. 2024. <a href="https://openreview.net/forum?id=farT6XXntP">A paradigm shift in machine translation: Boosting translation performance of large language models</a>. In <em>The twelfth international conference on learning representations</em>.
</div>
<div id="ref-xu-etal-2023-understanding" class="csl-entry" role="listitem">
Weijia Xu, Sweta Agrawal, Eleftheria Briakou, Marianna J. Martindale, and Marine Carpuat. 2023. <a href="https://doi.org/10.1162/tacl_a_00563">Understanding and detecting hallucinations in neural machine translation via model introspection</a>. <em>Transactions of the Association for Computational Linguistics</em>, 11:546–564.
</div>
<div id="ref-yin-neubig-2022-interpreting" class="csl-entry" role="listitem">
Kayo Yin and Graham Neubig. 2022. <a href="https://doi.org/10.18653/v1/2022.emnlp-main.14">Interpreting language models with contrastive explanations</a>. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, <em>Proceedings of the 2022 conference on empirical methods in natural language processing</em>, pages 184–198, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.
</div>
<div id="ref-zeiler-etal-2011-adaptive" class="csl-entry" role="listitem">
Matthew D. Zeiler, Graham W. Taylor, and Rob Fergus. 2011. <a href="https://doi.org/10.1109/ICCV.2011.6126474">Adaptive deconvolutional networks for mid and high level feature learning</a>. In <em>2011 international conference on computer vision (ICCV)</em>, pages 2018–2025.
</div>
<div id="ref-zerva-etal-2024-findings" class="csl-entry" role="listitem">
Chrysoula Zerva, Frederic Blain, José G. C. De Souza, Diptesh Kanojia, Sourabh Deoghare, Nuno M. Guerreiro, Giuseppe Attanasio, Ricardo Rei, Constantin Orasan, Matteo Negri, Marco Turchi, Rajen Chatterjee, Pushpak Bhattacharyya, Markus Freitag, and André Martins. 2024. <a href="https://doi.org/10.18653/v1/2024.wmt-1.3">Findings of the quality estimation shared task at <span>WMT</span> 2024: Are <span>LLM</span>s closing the gap in <span>QE</span>?</a> In Barry Haddow, Tom Kocmi, Philipp Koehn, and Christof Monz, editors, <em>Proceedings of the ninth conference on machine translation</em>, pages 82–109, Miami, Florida, USA. Association for Computational Linguistics.
</div>
<div id="ref-zerva-etal-2022-findings" class="csl-entry" role="listitem">
Chrysoula Zerva, Frédéric Blain, Ricardo Rei, Piyawat Lertvittayakumjorn, José G. C. de Souza, Steffen Eger, Diptesh Kanojia, Duarte Alves, Constantin Orăsan, Marina Fomicheva, André F. T. Martins, and Lucia Specia. 2022. <a href="https://aclanthology.org/2022.wmt-1.3/">Findings of the <span>WMT</span> 2022 shared task on quality estimation</a>. In Philipp Koehn, Loïc Barrault, Ondřej Bojar, Fethi Bougares, Rajen Chatterjee, Marta R. Costa-jussà, Christian Federmann, Mark Fishel, Alexander Fraser, Markus Freitag, Yvette Graham, Roman Grundkiewicz, Paco Guzman, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes, Tom Kocmi, André Martins, Makoto Morishita, et al., editors, <em>Proceedings of the seventh conference on machine translation (WMT)</em>, pages 69–99, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics.
</div>
<div id="ref-zhang-sennrich-2019-root" class="csl-entry" role="listitem">
Biao Zhang and Rico Sennrich. 2019. Root mean square layer normalization. In <em>Proceedings of the 33rd international conference on neural information processing systems</em>, Red Hook, NY, USA. Curran Associates Inc.
</div>
<div id="ref-zhang-etal-2018-improving" class="csl-entry" role="listitem">
Jiacheng Zhang, Huanbo Luan, Maosong Sun, Feifei Zhai, Jingfang Xu, Min Zhang, and Yang Liu. 2018. <a href="https://doi.org/10.18653/v1/D18-1049">Improving the transformer translation model with document-level context</a>. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun’ichi Tsujii, editors, <em>Proceedings of the 2018 conference on empirical methods in natural language processing</em>, pages 533–542, Brussels, Belgium. Association for Computational Linguistics.
</div>
<div id="ref-zou-etal-2024-enhancing" class="csl-entry" role="listitem">
Andy Zou, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard Ren, Alexander Pan, Xuwang Yin, Mantas Mazeika, Ann-Kathrin Dombrowski, Shashwat Goel, Nathaniel Li, Michael J. Byun, Zifan Wang, Alex Mallen, Steven Basart, Sanmi Koyejo, Dawn Song, Matt Fredrikson, et al. 2024. <a href="https://openreview.net/forum?id=aCgybhcZFi">Enhancing neural network transparency through representation analysis</a>. <em>OpenReview</em>.
</div>
<div id="ref-zouhar-etal-2025-ai" class="csl-entry" role="listitem">
Vilém Zouhar, Tom Kocmi, and Mrinmaya Sachan. 2025. <a href="https://aclanthology.org/2025.naacl-long.255/"><span>AI</span>-assisted human evaluation of machine translation</a>. In Luis Chiruzzo, Alan Ritter, and Lu Wang, editors, <em>Proceedings of the 2025 conference of the nations of the americas chapter of the association for computational linguistics: Human language technologies (volume 1: Long papers)</em>, pages 4936–4950, Albuquerque, New Mexico. Association for Computational Linguistics.
</div>
<div id="ref-zouhar-etal-2021-backtranslation" class="csl-entry" role="listitem">
Vilém Zouhar, Michal Novák, Matúš Žilinec, Ondřej Bojar, Mateo Obregón, Robin L. Hill, Frédéric Blain, Marina Fomicheva, Lucia Specia, and Lisa Yankovskaya. 2021. <a href="https://doi.org/10.18653/v1/2021.naacl-main.14">Backtranslation feedback improves user confidence in <span>MT</span>, not quality</a>. In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou, editors, <em>Proceedings of the 2021 conference of the north american chapter of the association for computational linguistics: Human language technologies</em>, pages 151–161, Online. Association for Computational Linguistics.
</div>
</div>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>More details on neural networks can be found in <span class="citation" data-cites="goodfellow-etal-2016-deep">Goodfellow et al. (<a href="references.html#ref-goodfellow-etal-2016-deep" role="doc-biblioref">2016</a>)</span>.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Bias terms can be omitted, following the practice of recent models such as Llama <span class="citation" data-cites="touvron-etal-2023-llama2">(<a href="references.html#ref-touvron-etal-2023-llama2" role="doc-biblioref">Touvron et al., 2023</a>)</span><a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p><a href="https://openai.com/index/chatgpt" class="uri">https://openai.com/index/chatgpt</a><a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>Probability scores are commonly used as differentiation targets, see discussion in <span class="citation" data-cites="bastings-etal-2022-will">Bastings et al. (<a href="references.html#ref-bastings-etal-2022-will" role="doc-biblioref">2022</a>)</span>.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p><a href="https://github.com/PAIR-code/saliency" class="uri">https://github.com/PAIR-code/saliency</a><a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>Since the push towards proprietary model serving, details about the distribution of training data across languages in tech reports are often scarce.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p><a href="https://go.proz.com/blog/cat-tool-use-by-translators-who-is-using" class="uri">https://go.proz.com/blog/cat-tool-use-by-translators-who-is-using</a><a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>A comprehensive overview of MT metrics was released by <span class="citation" data-cites="lee-etal-2023-survey">Lee et al. (<a href="references.html#ref-lee-etal-2023-survey" role="doc-biblioref">2023</a>)</span>.<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../index.html" class="pagination-link" aria-label="Introduction">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>© 2025 Gabriele Sarti. All rights reserved.</p>
</div>   
    <div class="nav-footer-center">
<p><a href="https://www.rug.nl/?lang=en"><img src="../figures/logos/rug_eng_red.png" class="img-fluid" style="width: auto; height: 35px; padding-left: 10px; padding-right: 10px; margin-bottom:5px;"></a> <a href="https://www.rug.nl/research/clcg/?lang=en"><img src="../figures/logos/clcg.png" class="img-fluid" style="width: auto; height: 35px; padding-left: 10px; padding-right: 10px; margin-bottom:5px;"></a> <a href="https://projects.illc.uva.nl/indeep/"><img src="../figures/logos/indeep_logo_horizontal.png" class="img-fluid" style="width: auto; height: 35px; padding-left: 10px; padding-right: 10px; margin-bottom:5px;"></a> <a href="https://www.rug.nl/research/clcg/research/cl/?lang=en"><img src="../figures/logos/gronlp.png" class="img-fluid" style="width: auto; height: 35px; padding-left: 10px; padding-right: 10px; margin-bottom:5px;"></a></p>
</div>
    <div class="nav-footer-right">
<p>Written with <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>
    <script type="text/javascript">
    (function(d) {
      d.querySelectorAll(".pseudocode-container").forEach(function(el) {
        let pseudocodeOptions = {
          indentSize: el.dataset.indentSize || "1.2em",
          commentDelimiter: el.dataset.commentDelimiter || "//",
          lineNumber: el.dataset.lineNumber === "true" ? true : false,
          lineNumberPunc: el.dataset.lineNumberPunc || ":",
          noEnd: el.dataset.noEnd === "true" ? true : false,
          titlePrefix: el.dataset.captionPrefix || "Algorithm"
        };
        pseudocode.renderElement(el.querySelector(".pseudocode"), pseudocodeOptions);
      });
    })(document);
    (function(d) {
      d.querySelectorAll(".pseudocode-container").forEach(function(el) {
        let captionSpan = el.querySelector(".ps-root > .ps-algorithm > .ps-line > .ps-keyword")
        if (captionSpan !== null) {
          let captionPrefix = el.dataset.captionPrefix + " ";
          let captionNumber = "";
          if (el.dataset.pseudocodeNumber) {
            captionNumber = el.dataset.pseudocodeNumber + " ";
            if (el.dataset.chapterLevel) {
              captionNumber = el.dataset.chapterLevel + "." + captionNumber;
            }
          }
          captionSpan.innerHTML = captionPrefix + captionNumber;
        }
      });
    })(document);
    </script>
  




</body></html>