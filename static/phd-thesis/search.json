[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "From Insights to Impact",
    "section": "",
    "text": "1 Introduction\nIn recent years, language models have undergone a significant transformation, going from simple research prototypes producing barely coherent text to becoming a cornerstone of modern technological infrastructure. This success stems in large part from the remarkable ability of large neural networks such as the transformer (Vaswani et al., 2017) to learn rich representations of language—and by extension, our world and society—from staggering amounts of text. Yet, the complex and deeply intertwined structure that renders these systems so powerful is also the main culprit behind their opacity. The inner workings of neural networks remain notoriously difficult to interpret, and the lack of transparency in their decision-making processes has raised serious concerns about their reliability and fairness in high-stakes applications (Rudin, 2019).\nThese circumstances have led to a growing interest in interpretability— a field closely aligned with the broader area of explainable artificial intelligence (XAI), which seeks to develop methods and tools to understand how neural networks work and provide insights into their decision-making processes (Doshi-Velez and Kim, 2017; Li et al., 2022). In natural language processing (NLP), interpretability research has made significant strides by uncovering how language models encode and process factual knowledge and linguistic information (Tenney et al., 2019; Belinkov, 2022; Meng et al., 2022), revealing their use of context during generation (Clark et al., 2019; Ferrando et al., 2022) and identifying the learned mechanisms underlying their capabilities (Elhage et al., 2021; Saphra and Wiegreffe, 2024).\nWhile interpretability insights have earned broad recognition and influence within the NLP research community (Mosbach et al., 2024), critics have often pointed out that these findings rarely translate into actionable improvements for real-world systems (Räuker et al., 2023; Rai et al., 2024; Hendrycks and Hiscott, 2025). Most interpretability work today focuses on identifying subnetworks and mechanisms responsible for specific tasks inside language models (Ferrando et al., 2024; Sharkey et al., 2025), yet few studies have put interpretability insights in relation to end-users’ needs and desires (Ehsan et al., 2021), despite their crucial role in determining the practical usefulness of interpretability findings (Ehsan et al., 2024). This disconnect stems from a fundamental divide between research communities: most AI interpretability researchers pursue theoretical understanding of complex systems, while human-computer interaction (HCI) researchers prioritize actionable insights and practical applications.\nA prime example of this disconnect can be found in the field of machine translation (MT), a long-standing area of research within NLP. MT researchers pioneered the use of neural language models for sequence generation tasks (Sutskever et al., 2014; Bahdanau et al., 2015), and were among the first to analyze their inner workings (Belinkov et al., 2017; Voita et al., 2019; Rogers et al., 2020). Yet, despite the significant progress in the performance of MT systems across hundreds of languages over the past decade, the field has been remarkably slow to bring interpretability insights to the users of these systems, especially in the case of professional translators who work with these systems on a daily basis. Users of “classic” translation tools such as Google Translate are, to this day, simply presented with translations, without the possibility to personalize their tone or properties, quantify the model uncertainty in its response, or identify potential errors or alternative formulations. At the other extreme, when large language models like GPT-4 (OpenAI, 2023) eagerly offer eloquent justifications alongside their translations, these explanations may sound plausible but often fail to reflect the model’s actual processing and context usage, resulting in plausible yet unfaithful rationalizations (Turpin et al., 2023).\nThis dissertation aims to bridge the gap between method-centric interpretability research and outcome-centric real-world machine translation applications. We develop novel methods to understand and control language model generation, then study how to integrate these advances effectively into human translation workflows. Our research spans three interconnected macro-themes: (1) understanding how language models exploit contextual information during generation, (2) controlling model generation for personalized translation outputs, and (3) integrating interpretability insights into human translation workflows. Our methodological contributions, empirical evaluations, and user studies demonstrate how insights from interpretability research can lead to meaningful impact in the way machine translation systems are used in real-world translation workflows.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#outline-and-contributions",
    "href": "index.html#outline-and-contributions",
    "title": "From Insights to Impact",
    "section": "1.1 Outline and Contributions",
    "text": "1.1 Outline and Contributions\nThe experimental chapters of this dissertation are organized into three parts, each addressing one of the research directions outlined above. Each part is composed of multiple chapters, each presenting a self-contained contribution or study related to the overarching theme. Figure 1.1 provides a visual overview of parts and chapters, highlighting for each chapter the topics introduced in detail in Chapter 2. Below, we summarize the contents, research questions and contributions of each part.\n\n\n\n\n\n\nFigure 1.1: Chapter guide for the three parts of this dissertation.\n\n\n\nPart I: Attributing Context Usage in Multilingual NLP\nPart I establishes the foundational infrastructure and methodological frameworks for understanding how neural language models and machine translation systems process contextual information during generation. We begin with Inseq (Chapter 3), a toolkit that democratizes access to interpretability analyses of generative language models, providing the foundation for our investigations into context usage. Then, Chapter 4 introduces PECoRe, a data-driven framework for quantifying the plausibility of context usage in language models through the contrastive identification of context-sensitive tokens and contextual cues that influence their prediction. PECoRe is used to study context usage in context-aware machine translation systems, identifying failure cases stemming from an incorrect usage of context. Chapter 5 extends this analysis to modern large language models and retrieval-augmented generation settings with Mirage, adapting the PECoRe framework to demonstrate how model internals enable faithful answer attribution in question answering. This part addresses two fundamental research questions:\n\n\n\n\n\n\n❓ Research Question 1 (RQ1)\n\n\n\nWhat are the conceptual and technical requirements for interpretability software tools enabling scalable and reproducible analyses into the inner workings of generative language models?\n\n\n\n\n\n\n\n\n❓ Research Question 2 (RQ2)\n\n\n\nHow do language models and machine translation systems exploit contextual information during generation, and how can we quantify this usage in a faithful manner?\n\n\nPart I’s primary contributions include: (1) two open-source releases of the Inseq interpretability library; (2) the contrastive attribution tracing (CAT) method, a gradient-based alternative to causal intervention for efficiently identifying salient model components; (3) the PECoRe framework for context reliance attribution in language models, enabling data-driven exploration of context usage patterns in context-aware MT systems; and (4) an extended evaluation of context attribution for retrieval-augmented generation using Mirage, producing high quality citations of retrieved documents while ensuring greater faithfulness to the model’s reasoning process.\nPart II: Conditioning Generation for Personalized Machine Translation\nPart II moves from understanding context usage to actively controlling model generation for customized translation outputs. Across two chapters, we explore two paradigms to condition machine translation outputs—prompting-based methods and direct interventions in model processing—addressing the question:\n\n\n\n\n\n\n❓ Research Question 3 (RQ3)\n\n\n\nAre interpretability-based steering methods viable approaches for controllable machine translation? How do they compare with prompting-based methods in terms of their performance and their impact on models’ internal mechanisms?\n\n\nChapter 6 pioneers the usage of prompting-based strategies for attribute-controlled translation, while Chapter 7 connects generation conditioning to interpretability techniques, expanding the scope of our analysis from simple attributes in common domains to sophisticated personal styles in the challenging literary translation domain.\nThe core contributions of Part II include: (1) Ramp, a novel prompting methodology achieving strong performance in attribute-controlled translation across multiple languages and attributes without model fine-tuning; (2) the first comprehensive comparison of prompting versus interpretability-based steering for machine translation personalization; (3) a novel contrastive steering method using sparse autoencoder latents to achieve personalization accuracy comparable to prompting while preserving quality in literary translation; and (4) evidence that prompting and steering methods converge to similar mechanistic solutions, revealing fundamental principles of generation conditioning.\nPart III: Interpretability in Human Translation Workflows\nPart III evaluates how interpretability insights can benefit human professionals who edit machine-translated content in a practical sense. We begin with DivEMT (Chapter 8), a study investigating the effectiveness of professional MT post-editing across a diverse set of mid-resourced languages, going beyond the one-size-fits-all analysis of high-resourced translation directions. This allows us to establish our human evaluation setup, providing valuable insights into the question:\n\n\n\n\n\n\n❓ Research Question 4 (RQ4)\n\n\n\nDoes MT contribute positively to the productivity of professional translators across different languages? Which factors influence its effectiveness?\n\n\nBuilding upon these insights, our second large-scale study QE4PE (Chapter 9) investigates how word-level error span highlights—including those derived from MT systems’ uncertainty during generation—impact the productivity of professional translators and the quality of post-edited contents:\n\n\n\n\n\n\n❓ Research Question 5 (RQ5)\n\n\n\nHow do word-level error highlights impact the productivity and editing choices of professional translators and the quality of resulting translations?\n\n\nChapter 10 concludes our human-centered investigation with a deeper analysis of multiple uncertainty and interpretability-based word-level quality estimation methods. Such analysis allows us to assess how the performance of such techniques varies across different models, languages and human annotators:\n\n\n\n\n\n\n❓ Research Question 6 (RQ6)\n\n\n\nCan unsupervised error span detection methods reliably identify problems in machine-translated outputs? How does human label variation affect their performance, compared to traditional supervised approaches?\n\n\nPart III contributions include (1) DivEMT, a cross-lingual post-editing dataset enabling controlled comparison of translator productivity across editing modalities and typologically diverse languages; (2) evidence that MT quality metrics fail to correlate with human post-editing productivity across languages, with productivity being heavily influenced by source-target language relatedness; (3) QE4PE, a comprehensive post-editing dataset containing error spans, behavioral editing metrics, and quality annotations from 42 professional post-editors for two translation directions; (4) evidence that error span highlights may reduce productivity but improve critical error detection; and (5) evidence that unsupervised quality estimation methods based on model internals can match state-of-the-art supervised approaches in both accuracy and downstream usability, revealing how subjective editing choice impact the evaluation of error span detection methods.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#scientific-output",
    "href": "index.html#scientific-output",
    "title": "From Insights to Impact",
    "section": "1.2 Scientific Output",
    "text": "1.2 Scientific Output\nThis dissertation is the product of several research articles and open-source projects, which are categorized in the following sections.\n\n1.2.1 Main Publications\nThe following articles represent the main contributions reflected in this thesis’ experimental chapters, organized in their respective parts:1\nIntroduction and Background\n\nFerrando, J., Sarti, G., Bisazza, A. and Costa-jussà, M. R. (2024). A Primer on the Inner Workings of Transformer-based Language Models. Arxiv Preprint (Chapter 2)\n\nPart I: Attributing Context Usage in Multilingual NLP\n\nSarti, G., Feldhus, N., Sickert, L., van der Wal, O., Nissim, M. and Bisazza, A. (2023a). Inseq: An Interpretability Toolkit for Sequence Generation Models. In Proc. of the 61st Annual Meeting of the Association for Computational Linguistics (ACL Demo) (Chapter 3)\nSarti, G., Feldhus, N., Qi, J., Nissim, M. and Bisazza, A. (2024d). Democratizing Advanced Attribution Analyses of Generative Language Models with the Inseq Toolkit. In Proc. of the 2nd World Conference on eXplainable Artificial Intelligence: Late-breaking works and demos (xAI Demo) (Chapter 3 and Chapter 4)\nSarti, G., Chrupała G., Nissim, M. and Bisazza, A. (2024c). Quantifying the Plausibility of Context Reliance in Neural Machine Translation. In Proc. of the 12th International Conference on Learning Representations (ICLR) (Chapter 4)\nQi, J.\\(^\\dagger\\), Sarti, G.\\(^\\dagger\\), Fernández, R. and Bisazza, A. (2024). Model Internals-based Answer Attribution for Trustworthy Retrieval-Augmented Generation. In Proc. of the 2024 Conference on Empirical Methods in Natural Language Processing (EMNLP) (Chapter 5)\n\nPart II: Conditioning Generation for Personalized Machine Translation\n\nSarti, G., Htut, P. M., Niu, X., Hsu, B., Currey, A., Dinu, G. and Nadejde, M. (2023b). RAMP: Retrieval and Attribute-Marking Enhanced Prompting for Attribute-Controlled Translation. In Proc. of the 61st Annual Meeting of the Association for Computational Linguistics (ACL) (Chapter 6)\nScalena, D.\\(^\\dagger\\), Sarti, G.\\(^\\dagger\\), Bisazza, A., Fersini, E. and Nissim, M. (2025). Steering Large Language Models for Machine Translation Personalization. Arxiv Preprint (Chapter 7)\n\nPart III: Interpretability in Human Translation Workflows\n\nSarti, G., Bisazza, A., Guerberof-Arenas, A. and Toral, A. (2022). DivEMT: Neural Machine Translation Post-Editing Effort Across Typologically Diverse Languages. In Proc. of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP) (Chapter 8)\nSarti, G., Zouhar, V., Chrupała, G., Guerberof-Arenas, A., Nissim, M. and Bisazza, A. (2025a). QE4PE: Word-level Quality Estimation for Human Post-Editing. Transactions of the Association for Computational Linguistics (TACL) (Chapter 9)\nSarti, G., Zouhar, V., Nissim, M. and Bisazza, A. (2025b). Unsupervised Word-level Quality Estimation for Machine Translation Through the Lens of Annotators (Dis)agreement. In Proc. of the 2025 Conference on Empirical Methods in Natural Language Processing (EMNLP) (Chapter 10)\n\nI led the conceptualization, implementation, experimental evaluation, and manuscript writing for each article for which I am the sole first author. For articles with shared first authorship, I co-led the conceptualization, experimental design, and manuscript writing. In Qi^* et al. (2024), I implemented the API for experimental evaluation. The background in Chapter 2 adapts parts of our primer on transformer interpretability (Ferrando et al., 2024), for which I contributed by surveying the literature and writing content regarding transformer architecture, input attribution methods, steering approaches, and interpretability tools.\n\n\n1.2.2 Open-source Contributions\nOpen-source software proved fundamental to this thesis, providing a solid foundation for conducting reproducible experimental work. Notably, all investigations we conducted employed solely open-source tools, models and datasets, despite the current popularity of proprietary language models. Each chapter provides links to all datasets, models, code, and demos to encourage scrutiny and foster further research.\nMy most notable contribution to the open-source research ecosystem is the Inseq toolkit, presented in Chapter 3, for which I serve as development lead. The library now counts 430+ Github stars and 80+ citations across international venues.\nI also contributed to the development of the following open-source projects:\n\nThe Groningen Translation Environment (GroTE), a Gradio-based UI for machine translation post-editing supporting the live recording of behavioral logs using the Hugging Face datasets hub and spaces ecosystem, developed with the help of Vilém Zouhar for the QE4PE study (Chapter 9). Available at https://github.com/gsarti/grote or via pip install grote.\ngradio-highlightedtextbox, a Svelte component for Gradio supporting text editing with highlighted spans, developed for collecting behavioral edit data in GroTE. Available at https://huggingface.co/spaces/gsarti/gradio_highlightedtextbox or via pip install gradio-highlightedtextbox.\nlabl, a toolkit to facilitate token-level analyses of annotated texts with multiple edits and tokenization schemes, developed with the help of Vilém Zouhar for Chapter 10 analyses. Available at https://github.com/gsarti/labl or via pip install labl.\nInterpreto, a Python toolbox for concept-based interpretability analyses of language models maintained by the FOR/DEEL teams, which I helped design and develop as part of my visit to the IRT Saint Exupéry research institute in Toulouse, France. Interpreto is available at https://github.com/FOR-sight-ai/interpreto or via pip install interpreto.\n\nThe full set of open-source contributions, including demos, models, and datasets, are available on GitHub and 🤗 Hugging Face.\n\n\n1.2.3 Other Research Contributions\nBeyond this dissertation’s scope, my research output included projects organized around two main themes:\nAdvancing Italian natural language processing:\n\nMiaschi, A., Sarti, G., Brunato, D., Dell’Orletta, F. and Venturi, G. (2022). Probing Linguistic Knowledge in Italian Neural Language Models across Language Varieties. Italian Journal of Computational Linguistics (IJCoL)\nBianchi, F., Attanasio, G., Pisoni, R., Terragni, S., Sarti, G. and Balestri, D. (2023). Contrastive Language-Image Pre-training for the Italian Language. In Proc. of the 9th Italian Conference on Computational Linguistics (CLiC-it)\nSarti, G. and Nissim, M. (2024). IT5: Text-to-text Pretraining for Italian Language Understanding and Generation. In Proc. of the Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING)\nSarti, G., Caselli, T., Nissim, M. and Bisazza, A. (2024b). Non Verbis, Sed Rebus: Large Language Models Are Weak Solvers of Italian Rebuses. In Proc. of the 10th Italian Conference on Computational Linguistics (CLiC-it)\nSarti, G., Caselli, T., Bisazza, A. and Nissim, M. (2024a). EurekaRebus - Verbalized Rebus Solving with LLMs: A CALAMITA Challenge. In Proc. of the 10th Italian Conference on Computational Linguistics (CLiC-it)\nCiaccio, C., Sarti, G., Miaschi, A. and Dell’Orletta, F. (2025). Crossword Space: Latent Manifold Learning for Italian Crosswords and Beyond. In Proc. of the 11th Italian Conference on Computational Linguistics (CLiC-it)\n\n Interpreting the inner workings of generative language models:\n\nLangedijk, A., Mohebbi, H., Sarti, G., Zuidema, W. and Jumelet, J. (2024). DecoderLens: Layerwise Interpretation of Encoder-Decoder Transformers. In Findings of the North American Chapter of the Association for Computational Linguistics (NAACL Findings)\nEdman, L., Sarti, G., Toral, A., van Noord, G. and Bisazza, A. (2024). Are Character-level Translations Worth the Wait? Comparing ByT5 and mT5 for Machine Translation. Trans. of the Association for Computational Linguistics (TACL)\nScalena, D., Sarti, G. and Nissim, M. (2024). Multi-property Steering of Large Language Models with Dynamic Activation Composition. In Proc. of the 7th Workshop on Analyzing and Interpreting Neural Networks for NLP (BlackboxNLP)\nGhasemi Madani, M. R., Gema, A. P., Sarti, G., Zhao, Y., Minervini, P. and Passerini, A. (2025). Noiser: Bounded Input Perturbations for Attributing Large Language Models. In Proc. of the Second Conference on Language Modeling (CoLM)\nCandussio, S., Saveri, G., Sarti, G. and Bortolussi, L. (2025). Bridging Logic and Learning: Decoding Temporal Logic Embeddings via Transformers. In Proc. of the European Conference on Machine Learning and Principles of Knowledge Discovery in Databases (ECML-PKDD)\nIslam, K. I. and Sarti, G. (2025). Reveal-Bangla: A Dataset for Cross-Lingual Multi-Step Reasoning Evaluation. Arxiv Preprint.\n\nI also had the privilege of organizing the BlackboxNLP 2025 workshop2—the leading venue for NLP interpretability work—and its shared task on benchmarking mechanistic interpretability methods for circuit localization and causal variable identification in large language models.\n\n\n\n\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to align and translate. In Yoshua Bengio and Yann LeCun, editors, Proceedings of the 3rd international conference on learning representations (ICLR), San Diego, CA, USA.\n\n\nYonatan Belinkov. 2022. Probing classifiers: Promises, shortcomings, and advances. Computational Linguistics, 48(1):207–219.\n\n\nYonatan Belinkov, Nadir Durrani, Fahim Dalvi, Hassan Sajjad, and James Glass. 2017. What do neural machine translation models learn about morphology? In Regina Barzilay and Min-Yen Kan, editors, Proceedings of the 55th annual meeting of the association for computational linguistics (volume 1: Long papers), pages 861–872, Vancouver, Canada. Association for Computational Linguistics.\n\n\nFederico Bianchi, Giuseppe Attanasio, Raphael Pisoni, Silvia Terragni, Gabriele Sarti, and Dario Balestri. 2023. Contrastive language–image pre-training for the Italian language. In Federico Boschetti, Gianluca E. Lebani, Bernardo Magnini, and Nicole Novielli, editors, Proceedings of the 9th italian conference on computational linguistics (CLiC-it 2023), pages 78–85, Venice, Italy. CEUR Workshop Proceedings.\n\n\nSara Candussio, Gaia Saveri, Gabriele Sarti, and Luca Bortolussi. 2025. Bridging logic and learning: Decoding temporal logic embeddings via transformers. In Machine learning and knowledge discovery in databases. Research track. Springer Nature Switzerland.\n\n\nCristiano Ciaccio, Gabriele Sarti, Alessio Miaschi, and Felice Dell’Orletta. 2025. Crossword space: Latent manifold learning for italian crosswords and beyond. In Cristina Bosco, Elisabetta Jezek, Marco Polignano, and Manuela Sanguinetti, editors, Proceedings of the 11th italian conference on computational linguistics (CLiC-it 2023), Cagliari, Italy. CEUR Workshop Proceedings.\n\n\nKevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D. Manning. 2019. What does BERT look at? An analysis of BERT‘s attention. In Tal Linzen, Grzegorz Chrupała, Yonatan Belinkov, and Dieuwke Hupkes, editors, Proceedings of the 2019 ACL workshop BlackboxNLP: Analyzing and interpreting neural networks for NLP, pages 276–286, Florence, Italy. Association for Computational Linguistics.\n\n\nFinale Doshi-Velez and Been Kim. 2017. Towards a rigorous science of interpretable machine learning.\n\n\nLukas Edman, Gabriele Sarti, Antonio Toral, Gertjan van Noord, and Arianna Bisazza. 2024. Are character-level translations worth the wait? Comparing ByT5 and mT5 for machine translation. Transactions of the Association for Computational Linguistics, 12:392–410.\n\n\nUpol Ehsan, Q. Vera Liao, Michael Muller, Mark O. Riedl, and Justin D. Weisz. 2021. Expanding explainability: Towards social transparency in AI systems. In Proceedings of the 2021 CHI conference on human factors in computing systems, New York, NY, USA. Association for Computing Machinery.\n\n\nUpol Ehsan, Samir Passi, Q. Vera Liao, Larry Chan, I-Hsiang Lee, Michael Muller, and Mark O Riedl. 2024. The who in XAI: How AI background shapes perceptions of AI explanations. In Proceedings of the 2024 CHI conference on human factors in computing systems, New York, NY, USA. Association for Computing Machinery.\n\n\nNelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, et al. 2021. A mathematical framework for transformer circuits. Transformer Circuits Thread. https://transformer-circuits.pub/2021/framework/index.html.\n\n\nJavier Ferrando, Gerard I. Gállego, and Marta R. Costa-jussà. 2022. Measuring the mixing of contextual information in the transformer. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Proceedings of the 2022 conference on empirical methods in natural language processing, pages 8698–8714, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\n\n\nJavier Ferrando, Gabriele Sarti, Arianna Bisazza, and Marta R. Costa-jussà. 2024. A primer on the inner workings of transformer-based language models. Arxiv Preprint.\n\n\nDan Hendrycks and Laura Hiscott. 2025. The misguided quest for mechanistic AI interpretability. Accessed August 4, 2025.\n\n\nKhondoker Ittehadul Islam and Gabriele Sarti. 2025. Reveal-bangla: A dataset for cross-lingual multi-step reasoning evaluation. Arxiv Preprint.\n\n\nAnna Langedijk, Hosein Mohebbi, Gabriele Sarti, Willem Zuidema, and Jaap Jumelet. 2024. DecoderLens: Layerwise interpretation of encoder-decoder transformers. In Kevin Duh, Helena Gomez, and Steven Bethard, editors, Findings of the association for computational linguistics: NAACL 2024, pages 4764–4780, Mexico City, Mexico. Association for Computational Linguistics.\n\n\nXuhong Li, Haoyi Xiong, Xingjian Li, Xuanyu Wu, Xiao Zhang, Ji Liu, Jiang Bian, and Dejing Dou. 2022. Interpretable deep learning: Interpretation, interpretability, trustworthiness, and beyond. Knowledge and Information Systems, 64(12):3197–3234.\n\n\nMohammad Reza Ghasemi Madani, Aryo Pradipta Gema, Gabriele Sarti, Yu Zhao, Pasquale Minervini, and Andrea Passerini. 2025. Noiser: Bounded input perturbations for attributing large language models. In Second conference on language modeling.\n\n\nKevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. 2022. Locating and editing factual associations in GPT. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in neural information processing systems, volume 35, pages 17359–17372. Curran Associates, Inc.\n\n\nAlessio Miaschi, Gabriele Sarti, Dominique Brunato, Felice Dell’Orletta, and Giulia Venturi. 2022. Probing linguistic knowledge in italian neural language models across language varieties. Italian Journal of Computational Linguistics (IJCoL), 8(1):25–44.\n\n\nMarius Mosbach, Vagrant Gautam, Tomás Vergara Browne, Dietrich Klakow, and Mor Geva. 2024. From insights to actions: The impact of interpretability and analysis research on NLP. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Proceedings of the 2024 conference on empirical methods in natural language processing, pages 3078–3105, Miami, Florida, USA. Association for Computational Linguistics.\n\n\nOpenAI. 2023. Gpt-4 technical report. Arxiv.\n\n\nJirui Qi^*, Gabriele Sarti^*, Raquel Fernández, and Arianna Bisazza. 2024. Model internals-based answer attribution for trustworthy retrieval-augmented generation. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Proceedings of the 2024 conference on empirical methods in natural language processing, pages 6037–6053, Miami, Florida, USA. Association for Computational Linguistics.\n\n\nDaking Rai, Yilun Zhou, Shi Feng, Abulhair Saparov, and Ziyu Yao. 2024. A practical review of mechanistic interpretability for transformer-based language models. Arxiv Preprint.\n\n\nTilman Räuker, Anson Ho, Stephen Casper, and Dylan Hadfield-Menell. 2023. Toward transparent AI: A survey on interpreting the inner structures of deep neural networks. In 2023 IEEE conference on secure and trustworthy machine learning (SaTML), pages 464–483.\n\n\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky. 2020. A primer in BERTology: What we know about how BERT works. Transactions of the Association for Computational Linguistics, 8:842–866.\n\n\nCynthia Rudin. 2019. Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. Nature Machine Intelligence, 1:206–215.\n\n\nNaomi Saphra and Sarah Wiegreffe. 2024. Mechanistic? In Yonatan Belinkov, Najoung Kim, Jaap Jumelet, Hosein Mohebbi, Aaron Mueller, and Hanjie Chen, editors, Proceedings of the 7th BlackboxNLP workshop: Analyzing and interpreting neural networks for NLP, pages 480–498, Miami, Florida, US. Association for Computational Linguistics.\n\n\nGabriele Sarti, Arianna Bisazza, Ana Guerberof-Arenas, and Antonio Toral. 2022. DivEMT: Neural machine translation post-editing effort across typologically diverse languages. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Proceedings of the 2022 conference on empirical methods in natural language processing, pages 7795–7816, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\n\n\nGabriele Sarti, Tommaso Caselli, Arianna Bisazza, and Malvina Nissim. 2024a. EurekaRebus - verbalized rebus solving with LLMs: A CALAMITA challenge. In Felice Dell’Orletta, Alessandro Lenci, Simonetta Montemagni, and Rachele Sprugnoli, editors, Proceedings of the 10th italian conference on computational linguistics (CLiC-it 2024), pages 1202–1208, Pisa, Italy. CEUR Workshop Proceedings.\n\n\nGabriele Sarti, Tommaso Caselli, Malvina Nissim, and Arianna Bisazza. 2024b. Non verbis, sed rebus: Large language models are weak solvers of Italian rebuses. In Felice Dell’Orletta, Alessandro Lenci, Simonetta Montemagni, and Rachele Sprugnoli, editors, Proceedings of the 10th italian conference on computational linguistics (CLiC-it 2024), pages 888–897, Pisa, Italy. CEUR Workshop Proceedings.\n\n\nGabriele Sarti, Grzegorz Chrupała, Malvina Nissim, and Arianna Bisazza. 2024c. Quantifying the plausibility of context reliance in neural machine translation. In The twelfth international conference on learning representations (ICLR 2024), Vienna, Austria. OpenReview.\n\n\nGabriele Sarti, Nils Feldhus, Jirui Qi, Malvina Nissim, and Arianna Bisazza. 2024d. Democratizing advanced attribution analyses of generative language models with the inseq toolkit. In xAI-2024 late-breaking work, demos and doctoral consortium joint proceedings, pages 289–296, Valletta, Malta. CEUR.org.\n\n\nGabriele Sarti, Nils Feldhus, Ludwig Sickert, Oskar van der Wal, Malvina Nissim, and Arianna Bisazza. 2023a. Inseq: An interpretability toolkit for sequence generation models. In Danushka Bollegala, Ruihong Huang, and Alan Ritter, editors, Proceedings of the 61st annual meeting of the association for computational linguistics (volume 3: System demonstrations), pages 421–435, Toronto, Canada. Association for Computational Linguistics.\n\n\nGabriele Sarti, Phu Mon Htut, Xing Niu, Benjamin Hsu, Anna Currey, Georgiana Dinu, and Maria Nadejde. 2023b. RAMP: Retrieval and attribute-marking enhanced prompting for attribute-controlled translation. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st annual meeting of the association for computational linguistics (volume 2: Short papers), pages 1476–1490, Toronto, Canada. Association for Computational Linguistics.\n\n\nGabriele Sarti and Malvina Nissim. 2024. IT5: Text-to-text pretraining for Italian language understanding and generation. In Nicoletta Calzolari, Min-Yen Kan, Veronique Hoste, Alessandro Lenci, Sakriani Sakti, and Nianwen Xue, editors, Proceedings of the 2024 joint international conference on computational linguistics, language resources and evaluation (LREC-COLING 2024), pages 9422–9433, Torino, Italia. ELRA; ICCL.\n\n\nGabriele Sarti, Vilém Zouhar, Grzegorz Chrupała, Ana Guerberof-Arenas, Malvina Nissim, and Arianna Bisazza. 2025a. QE4PE: Word-level quality estimation for human post-editing. Arxiv Preprint.\n\n\nGabriele Sarti, Vilém Zouhar, Malvina Nissim, and Arianna Bisazza. 2025b. Unsupervised word-level quality estimation for machine translation through the lens of annotators (dis)agreement. Arxiv Preprint.\n\n\nDaniel Scalena, Gabriele Sarti, and Malvina Nissim. 2024. Multi-property steering of large language models with dynamic activation composition. In Yonatan Belinkov, Najoung Kim, Jaap Jumelet, Hosein Mohebbi, Aaron Mueller, and Hanjie Chen, editors, Proceedings of the 7th BlackboxNLP workshop: Analyzing and interpreting neural networks for NLP, pages 577–603, Miami, Florida, US. Association for Computational Linguistics.\n\n\nDaniel Scalena^*, Gabriele Sarti^*, Arianna Bisazza, Elisabetta Fersini, and Malvina Nissim. 2025. Steering large language models for machine translation personalization. Arxiv Preprint.\n\n\nLee Sharkey, Bilal Chughtai, Joshua Batson, Jack Lindsey, Jeff Wu, Lucius Bushnaq, Nicholas Goldowsky-Dill, Stefan Heimersheim, Alejandro Ortega, Joseph Bloom, Stella Biderman, Adria Garriga-Alonso, Arthur Conmy, Neel Nanda, Jessica Rumbelow, Martin Wattenberg, Nandi Schoots, Joseph Miller, Eric J. Michaud, et al. 2025. Open problems in mechanistic interpretability.\n\n\nIlya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014. Sequence to sequence learning with neural networks. In Proceedings of the 28th international conference on neural information processing systems - volume 2, pages 3104–3112, Cambridge, MA, USA. MIT Press.\n\n\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019. BERT rediscovers the classical NLP pipeline. In Anna Korhonen, David Traum, and Lluís Màrquez, editors, Proceedings of the 57th annual meeting of the association for computational linguistics, pages 4593–4601, Florence, Italy. Association for Computational Linguistics.\n\n\nMiles Turpin, Julian Michael, Ethan Perez, and Samuel R. Bowman. 2023. Language models don’t always say what they think: Unfaithful explanations in chain-of-thought prompting. In Proceedings of the 37th international conference on neural information processing systems, Red Hook, NY, USA. Curran Associates Inc.\n\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in neural information processing systems, volume 30. Curran Associates, Inc.\n\n\nElena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov. 2019. Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned. In Anna Korhonen, David Traum, and Lluís Màrquez, editors, Proceedings of the 57th annual meeting of the association for computational linguistics, pages 5797–5808, Florence, Italy. Association for Computational Linguistics.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "From Insights to Impact",
    "section": "",
    "text": "Shared first co-authorship is indicated by \\(^\\dagger\\).↩︎\nhttps://blackboxnlp.github.io/2025↩︎",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/chap-2-background.html",
    "href": "chapters/chap-2-background.html",
    "title": "2  Background",
    "section": "",
    "text": "2.1 From Neural Networks to Neural Language Models\nThis chapter provides a succinct introduction to various topics discussed in the experimental chapters of this dissertation. Rather than a comprehensive review of relevant literature, it aims to provide key background about the research presented in this manuscript.\nIn particular, Section 2.1 and Section 2.4 discuss the basic functioning of neural networks-based language models and machine translation (MT) systems, and introduce the machine translation task representing the core focus of this work. Section 2.2 and Section 2.3 provide an introduction to methods for attributing inputs and conditioning generation in language models, corresponding to the topics discussed in Part I and Part II. Finally, Section 2.5 and Section 2.6 dive deeper in the translation domain, providing an overview of how MT models are employed in the translation industry by human post-editors, and discussing techniques for automatically evaluating machine translation quality. These notions provide a valuable background to Part III, which focuses on the impact of interpretability insights on human translation workflows.\nBeyond this background section, each experimental chapter briefly summarizes relevant literature to contextualize the research questions and findings.\nNeural networks are computational models which integrate principles from statistical learning theory (Vapnik, 1995), consisting of interconnected nodes (neurons) organized in layers, where each connection has an associated weight. Formally, a neural network is a function \\(\\mathbf{f}: \\mathcal{X} \\to \\mathcal{Y}\\) that maps inputs \\(\\mathbf{x} \\in \\mathcal{X}\\) to outputs \\(\\mathbf{y} \\in \\mathcal{Y}\\), where \\(\\mathcal{X}\\) and \\(\\mathcal{Y}\\) are the input and output feature spaces, respectively. The function \\(\\mathbf{f}\\) is parameterized by weights \\(\\mathbf{\\theta} \\in \\Theta\\), which are typically learned from data through the training process described in Section 2.1.1. Individual neurons are functions parametrized by weights \\(\\mathbf{w} \\in \\mathbb{R}^d\\) and biases \\(b \\in \\mathbb{R}\\), which are combined to produce an output \\(\\sigma(\\mathbf{w}^T\\mathbf{x} + b)\\), where \\(\\sigma\\) is a non-linear activation function. Thanks to non-linearities, sequences of neurons can learn to represent complex relations from input vector \\(\\mathbf{x}\\).1",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "chapters/chap-2-background.html#sec-chap2-nlm",
    "href": "chapters/chap-2-background.html#sec-chap2-nlm",
    "title": "2  Background",
    "section": "",
    "text": "2.1.1 Supervised Learning for Neural Networks\nIn the supervised learning paradigm, given a training dataset \\(\\mathcal{D}\\) containing paired instances:\n\\[\\mathcal{D} = \\{(\\mathbf{x}_1, y_1), \\ldots, (\\mathbf{x}_N, y_N)\\} \\in (\\mathcal{X} \\times \\mathcal{Y})^n \\tag{2.1}\\]\nwhere \\(\\mathbf{x}_i\\) is a vector of input features and \\(\\mathbf{y}_i\\) is the expected output, a neural network is trained to learn a functional mapping \\(\\mathbf{f}\\) from inputs \\(\\mathbf{x}\\) to labels \\(\\mathbf{y}\\) by minimizing the average value of a loss function \\(\\ell: \\mathcal{Y} \\times \\mathcal{Y} \\to \\mathcal{R}\\), such that \\(\\ell(\\mathbf{f}(\\mathbf{x}), \\mathbf{y})\\) quantifies the gap between predicted outcomes \\(\\tilde{y}\\) and ground truth \\(y\\) over examples in \\(\\mathcal{D}\\). The function \\(\\mathbf{f}\\) is parameterized by weights \\(\\mathbf{\\theta} \\in \\Theta\\), which are optimized during training so as to minimize the loss function. Such optimization is typically performed using some variant of stochastic gradient descent (SGD), in which iterative steps \\(1, \\dots, t \\dots, T\\) are taken to update \\(\\mathbf{\\theta}\\) in the direction of the negative gradient of the loss function with respect to the weights:\n\\[\\mathbf{\\theta}_{t+1} \\leftarrow \\mathbf{\\theta}_t - \\eta \\;\\nabla_{\\mathbf{\\theta}} \\,\\ell(\\mathbf{f}(\\mathbf{x}_j; \\mathbf{\\theta}_t), \\mathbf{y}_j) \\tag{2.2}\\]\nwhere \\(\\eta\\) is a chosen learning rate, and \\(\\mathbf{x}_j\\) and \\(\\mathbf{y}_j\\) are a subset of randomly sampled input-output pairs from the training set \\(\\mathcal{D}\\), typically referred to as mini-batch. This iterative refinement of model parameters is repeated until convergence, i.e. until the model performance on a left-out validation set does not improve significantly, allowing for a robust convergence to a local minimum of the loss function, even for non-convex problems and high-dimensional parameter spaces.\nWe commonly refer to the inference process going from input \\(\\mathbf{x}\\) to output \\(\\mathbf{y}\\) as forward pass, and to the process of computing gradients and updating model parameters as backward pass.\n\n\n2.1.2 Transformer Neural Networks\nTransformers (Vaswani et al., 2017) are a class of neural networks that have become the de-facto standard for most natural language processing tasks, constituting the core neural network architecture employed throughout this thesis’ experiments. In essence, a transformer consists of a sequence of identical macro-layers, dubbed blocks, progressively contextualizing a sequence of input features \\(\\mathbf{Z} \\in \\mathbb{R}^{S \\times d}\\), where \\(S\\) is the sequence length and \\(d\\) is the size of each feature vector. Figure 2.1 illustrates the structure of a single transformer module, constituting the core of decoder-only language models such as GPT-3 (Brown et al., 2020) presented later in Section 2.1.3. Notably, the transformer architecture is characterized by its ability to process input sequences in parallel, as opposed to recurrent models (Rumelhart and McClelland, 1987; Hochreiter and Schmidhuber, 1997), making it highly efficient for training on large datasets.\n\n\n\n\n\n\nFigure 2.1: An example transformer module with \\(N\\) blocks. We adopt the residual stream view of Elhage et al. (2021), with residual connections linearized to emphasize the read-write operations performed by attention and feed-forward network modules.\n\n\n\nWe now describe the main components of a transformer block in order of execution during the forward pass, using \\(\\mathbf{z}_i \\in \\mathbb{R}^d\\) to denote the input representations at each component for sequence element \\(i\\). This will be useful for explaining steering intervention and vocabulary projection methods used in Chapter 7 and Chapter 10, respectively.\nLayer normalization (LN). The layer normalization operation, also known as LayerNorm (Ba et al., 2016), is a common approach for stabilizing the training process of deep neural networks. In practice, layer normalization applies the transformation:\n\\[\\text{LN}(\\mathbf{z}_i) = \\frac{\\mathbf{z}_i - \\mu(\\mathbf{z}_i)}{\\sigma(\\mathbf{z}_i)} \\odot \\gamma + \\beta \\tag{2.3}\\]\nwhere \\(\\mu, \\sigma\\) are the mean and the standard deviation of \\(\\mathbf{z}\\), and \\(\\gamma, \\beta\\) are learnable scale and bias parameters for the normalization. This operation helps to mitigate issues related to internal covariate shift, improving convergence during training. Recently, LayerNorm has been substituted with RMSNorm (Zhang and Sennrich, 2019), which removes the mean centering step and scales the input using the root mean square (RMS) statistic.\nMulti-head self-attention (MHSA). The self-attention mechanism is the core component of the transformer architecture, allowing the model to contextualize its representations at each layer by combining information across the input sequence. While the original formulation of multi-head self-attention by Vaswani et al. (2017) involves a concatenation of attention head outputs before the final output projection, we follow here the more recent formulation by Kobayashi et al. (2021) and Elhage et al. (2021), which reformulates the attention output computation using the sum of individual attention heads, emphasizing the linear reading and writing operations within the attention computation.\nConcretely, the self-attention module is composed by a series of \\(H\\) attention heads \\(\\text{Attn}_1, \\ldots, \\text{Attn}_H\\), each computing the following weighted sum:\n\\[\\text{Attn}_h(\\mathbf{z}_i) = \\sum_{j} \\alpha^h_{ij} \\mathbf{z}_j \\mathbf{W}_V \\mathbf{W}_O \\tag{2.4}\\]\nIntuitively, the sharding of the attention mechanism into separate computations can be beneficial when processing the complex relations within different elements of the input sequence, for example, the lexical, syntactic and semantic dimensions of words in a text. The learnable weight matrices \\(\\mathbf{W}_V \\in \\mathbb{R}^{d \\times d_h}\\) and \\(\\mathbf{W}_O \\in \\mathbb{R}^{d_h \\times d}\\), where \\(d_h\\) represents the dimension of each head, can be combined into the so-called output-value (OV) circuit as \\({\\mathbf{W}_V \\mathbf{W}_O = \\mathbf{W}_{OV} \\in \\mathbb{R}^{d \\times d}}\\). For every key \\(j\\) given the current query position \\(i &lt; S\\), the corresponding attention weight \\(\\alpha^h_{i}\\) is computed as:\n\\[\\alpha^h_{i} = \\text{softmax}(\\frac{\\mathbf{z}_i \\mathbf{W}_Q (\\mathbf{W}_K \\mathbf{Z})^T}{\\sqrt{d_h}}) \\tag{2.5}\\]\nOnce again, the learnable weight matrices \\(\\mathbf{W}_Q \\in \\mathbb{R}^{d \\times d_h}\\) and \\(\\mathbf{W}_K \\in \\mathbb{R}^{d \\times d_h}\\) can be combined as the query-key (QK) circuit \\({\\mathbf{W}_Q \\mathbf{W}_K^T = \\mathbf{W}_{QK} \\in \\mathbb{R}^{d \\times d}}\\). This decomposition enables a view of QK and OV circuits as the units responsible for reading from (QK) and writing to (OV) the residual stream. Finally, the attention block output is the sum of individual attention heads: \\[\\text{Attn}(\\mathbf{z}_i) = \\sum_{h=1}^{H} \\text{Attn}_h(\\mathbf{z}_i) \\tag{2.6}\\]\nResidual connection. The introduction of residual connections (He et al., 2016) in the transformer architecture allows the model to learn identity mappings more easily, facilitating the training of deeper networks and avoiding the vanishing gradient problem (Hochreiter, 1998). A residual connection is commonly applied to the output of the self-attention module, resulting in:\n\\[\\text{ResAttn}(\\mathbf{z}_i) = \\text{Attn}\\big(\\text{LN}(\\mathbf{z}_i)\\big) + \\mathbf{z}_i \\tag{2.7}\\]\nFeedforward network (FFN). The feedforward network (FFN) in the transformer block is composed of two learnable weight matrices2: \\(\\mathbf{W}_{\\text{in}} \\in \\mathbb{R}^{d \\times d_{\\text{ffn}}}\\) and \\(\\mathbf{W}_{\\text{out}} \\in \\mathbb{R}^{d_{\\text{ffn}} \\times d}\\). \\(\\mathbf{W}_{\\text{in}}\\) reads from the residual stream state \\(\\mathbf{z}\\), and its result is passed through an element-wise non-linear activation function \\(\\sigma\\), producing a set of neuron activations. These get transformed by \\(\\mathbf{W}_{\\text{out}}\\) to produce the output \\(\\text{FFN}(\\mathbf{z})\\):\n\\[\\text{FFN}(\\mathbf{z}_i) = \\sigma(\\mathbf{z}_i \\mathbf{W}_{\\text{in}}) \\mathbf{W}_{\\text{out}} \\tag{2.8}\\]\nThe FFN operation was compared to a retrieval step from a key-value memory (Geva et al., 2021), with keys stored in columns of \\(\\mathbf{W}_{\\text{in}}\\) acting as pattern detectors over the input sequence, and values in rows of \\(\\mathbf{W}_{\\text{out}}\\) being upweighted by respective neuron activation. The overall block structure from Figure 2.1 can then be summarized as:\n\\[\\text{Block}(\\mathbf{z}_i) = \\text{FFN}\\Big(\\text{LN}\\big(\\text{ResAttn}(\\mathbf{z}_i)\\big)\\Big) + \\text{ResAttn}(\\mathbf{z}_i) \\tag{2.9}\\]\nWe will henceforth use \\(\\mathbf{z}_i^l\\) to denote the output of the \\(l\\)-th block for the \\(i\\)-th element of the input sequence for transformer models.\n\n\n2.1.3 Transformer Language Models\nA language model is a probabilistic model that can assign probabilities to sequences of tokens. Formally, given an input sequence \\(\\mathbf{X} = \\langle t_1, \\dots, t_S \\rangle\\) of \\(S\\) tokens, which in the case of natural language are typically words or subword units (Sennrich et al., 2016) from a vocabulary \\(\\mathcal{V}\\), a language model \\(f\\) computes the probability of the sequence as the product of token-level conditional probabilities:\n\\[P(\\mathbf{X}) = P(t_1, \\dots, t_S) = \\prod_{i=1}^{S} P(t_i|t_1, \\dots, t_{i-1}) \\tag{2.10}\\]\nLanguage models operating under such formulation are typically referred to as auto-regressive or causal language models (CLMs, or simply LMs), to differentiate them from masked language models (MLMs) trained to fill the blanks in a sequence (Devlin et al., 2019). While MLMs were the main object of analysis of early interpretability research on transformer models (Tenney et al., 2019; Clark et al., 2019; Rogers et al., 2020), this dissertation focuses solely on CLMs, which after the advent of ChatGPT3 in 2022 became the dominant paradigm in the NLP and interpretability community. CLMs are typically decoder-only models, following the structure introduced in Section 2.1.2, or encoder-decoder models, such as the MT systems later discussed in Section 2.4.\nImportantly, LMs can be used for generating text by iteratively sampling from the probability distribution over the next token \\(t_{i}\\) given the previous tokens \\(t_1, \\dots, t_{i-1}\\), e.g. using the greedy decoding sampling method:\n\\[t_i^* = \\underset{t \\in \\mathcal{V}}{\\arg\\,\\max}\\;P(\\,\\cdot\\,|t_1, \\ldots, t_{i-1}) \\tag{2.11}\\]\nThis sampling process can be repeated autoregressively, i.e. by adding the selected token \\(t_i^*\\) to the input sequence, until a special end-of-sequence token is generated, or until a maximum sequence length is reached.\nWe now turn to the additional components required to convert the generic transformer model presented in the previous section into a language model able to process and generate sequences of tokens. Figure 2.2 shows a stylized view of a transformer LM.\n\n\n\n\n\n\nFigure 2.2: A transformer language model predicting the next word given a prefix.\n\n\n\nEmbedding layer. The first component of a transformer language model is the embedding layer, which maps input tokens to continuous vector representations, known as embeddings. Word embeddings such as Word2Vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014) revolutionized the field of natural language processing by exploiting distributional semantics, i.e. the fact that words which frequently appear in similar contexts should have similar meaning (Harris, 1954), to learn word representations end-to-end using gradient descent. In transformers, the token embedding matrix \\(\\mathbf{E} \\in \\mathbb{R}^{|\\mathcal{V}| \\times d}\\), where \\(d\\) is the size of the embedding vectors, and \\(|\\mathcal{V}|\\) is the vocabulary size, is learned jointly with the rest of model parameters during training. The embedding layer maps each token \\(t_i\\) in the input sequence to its corresponding vector \\(\\mathbf{z}_i = \\mathbf{E}[t_i]\\). The resulting sequence of embeddings \\(\\mathbf{Z} \\in \\mathbb{R}^{S \\times d}\\) corresponds to the input to the first transformer block. It is important to note that representations produced by \\(\\mathbf{E}\\) are not contextualized, i.e. the same token \\(t_i\\) will always be mapped to the same vector \\(\\mathbf{z}_i\\), regardless of its meaning in the given sequence. For example, the word ring will always be mapped to the same vector, regardless of whether it is used as a noun or a verb. The transformer blocks are used to contextualize these representations, i.e. produce different vectors for the same token depending on the remainder of the sequence.\nPositional encodings. While the sequential nature of language is an important factor in how we produce and process linguistic information, transformer models do not explicitly account for ordering across elements of the input sequence. For this reason, positional encodings injecting information about the position of each token in the sequence are commonly used in transformer-based language models. The most basic positional encoding is a fixed sinusoidal encoding (Vaswani et al., 2017), which is added directly to the input embeddings. Recent models, however, employ rotary position embeddings, allowing for the encoding of both absolute and relative positions between tokens, and allowing the model to generalize to longer contexts beyond those seen during training (Su et al., 2024).\nCausal self-attention. The self-attention mechanism in transformer language models is causal, meaning that the attention weights for each token \\(t_i\\) are computed only over the tokens preceding it in the sequence, i.e. \\(t_1, \\ldots, t_{i-1}\\). This ensures that the model can only attend to past tokens when predicting the next token, preserving the auto-regressive nature of the model. The causal self-attention mechanism is implemented by masking out future tokens in the attention computation, ensuring that \\(\\alpha^h\\) is computed only for \\(j \\leq i\\) in Equation 2.4, and that only representations \\(Z_{\\leq i}\\) are used to compute the key vector in Equation 2.5.\nPrediction head. The prediction head of a transformer language models consists of a so-called unembedding matrix \\(\\mathbf{W}_{U} \\in \\mathbb{R}^{d \\times |\\mathcal{V}|}\\) mirroring the initial embedding operation, sometimes accompanied by a bias. The last residual stream state \\(\\mathbf{z}_S^L\\), where \\(L\\) is the number of transformer blocks and \\(S\\) is the sequence length, gets transformed by this linear map converting the representation into a next-token distribution of logits, which is turned into a probability distribution via the softmax function:\n\\[P(\\,\\cdot\\,|t_1, \\ldots, t_{i-1}) = \\text{softmax}(\\mathbf{z}_i^L \\mathbf{W}_{U}) \\tag{2.12}\\]\nIn light of the residual stream view presented in Section 2.1.2, showing that different model components read from and write to the residual stream, it is natural to believe that the predictions derived by applying the unembedding matrix to the final residual stream state \\(\\mathbf{z}_S^L\\) are the product of an iterative refinement across model components (Jastrzebski et al., 2018). The logit lens method (nostalgebraist, 2020), which we study for error detection in Chapter 10, exploits this intuition to analyze how the model refines the prediction throughout the forward pass, by projecting intermediate residual stream states \\(\\mathbf{z}_S^l\\), with \\(l &lt; L\\), to the vocabulary space using \\(\\mathbf{W}_{U}\\).\nLanguage model pre-training. Modern language models such as those employed in this thesis are typically pre-trained on large web corpora spanning billions or trillions of tokens using the next-token prediction objective, i.e. minimizing the cross-entropy loss between the next-token distribution predicted by the model and the next observed token. This frames the language model training problem as an instance of supervised learning, which we presented in Section 2.1.1. Formally, given a minibatch \\(D_t\\) of corpus \\(\\mathcal{D}\\) composed by sequences of tokens \\(\\mathbf{X_k} = \\langle t_1, \\ldots, t_{S_k} \\rangle\\), the loss for a single training step is computed as:\n\\[\\mathcal{L}_{\\text{step}} = -\\frac{1}{|D_t|} \\sum_{\\mathbf{X_k} \\in D_t} \\sum_{i=1}^{S_k} \\log P(t_i|t_1, \\ldots, t_{i-1}) \\tag{2.13}\\]\nConcretely, this corresponds to maximizing the likelihood of the observed tokens given the context provided by the preceding tokens, while minimizing the likelihood of all other incorrect tokens.\nLanguage model post-training. After pre-training, language models can be used for generating text given some context, but mostly lack the ability to perform specific tasks without being provided explicit examples, or respond to queries as conversational agents. For this reasons, all language models used for our experiments underwent additional supervised fine-tuning (SFT, also known as instruction tuning), allowing them to learn input-output mappings for realistic user queries beyond natural text occurrences in the pre-training corpus (Howard and Ruder, 2018; Sanh et al., 2022). The fine-tuning process still involves the same \\(\\mathcal{L}_{\\text{step}}\\) loss function over a smaller, curated set of demonstrations. Some of the models we study—such as the Gemma 2 models from Chapter 7 or the Zephyr model from Chapter 5 —underwent an additional reinforcement learning from human feedback (RLHF) step, in which the model is fine-tuned to maximize the likelihood of human preferences over pairs of model generations, using a reward model trained on human preferences. This process is typically performed using Proximal Policy Optimization [PPO;Schulman et al. (2017)] or similar reinforcement learning algorithms. Unless otherwise specified, we use the term language model to refer to transformer language models that were first pre-trained and then fine-tuned, representing the main focus of this thesis.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "chapters/chap-2-background.html#sec-chap2-attrib",
    "href": "chapters/chap-2-background.html#sec-chap2-attrib",
    "title": "2  Background",
    "section": "2.2 Explaining Predictions with Input Attribution",
    "text": "2.2 Explaining Predictions with Input Attribution\nContrary to linear models, where learned coefficients directly correspond to the influence of their respective features towards predictions, neural networks’ outcomes cannot be directly interpreted due to the presence of multiple nonlinearities across layers, rendering the attribution of model prediction to individual input features non-trivial. Input attribution methods, also known as feature attribution, were introduced to address this issue by providing a principled way to assign importance scores to input features, clarifying the rationales behind model decisions (Zeiler et al., 2011).\nFormally, for a model \\(\\mathbf{f} \\in \\mathcal{F}: \\mathcal{X} \\to \\mathcal{Y}\\), given an input \\(\\mathbf{x} \\in \\mathcal{X}\\), we can define the attribution method \\(\\gamma\\) as a functional:\n\\[\\gamma: \\mathcal{X} \\times \\mathcal{F} \\to \\mathbb{R}^{|\\mathcal{X}|}\\]\nso that \\(\\mathbf{a}_{\\mathbf{f}(\\mathbf{x})} = \\gamma(\\mathbf{x}, \\mathbf{f})\\) is a vector of attribution scores quantifying the influence of each element of \\(\\mathbf{x}\\) on the model predictive distribution \\(\\mathbf{f}(\\mathbf{x})\\), with higher scores representing greater importance (Fel, 2024). It is worth noting that attribution methods can rely on one or more specific outcomes \\(\\mathbf{y} \\in \\mathcal{Y}\\) from the predictive distribution \\(\\mathbf{f}(\\mathbf{x})\\), such as perturbation-based approaches (Covert et al., 2021), or simply rely on the flow of information within the model to identify important input elements (Abnar and Zuidema, 2020). We call the former methods target-dependent, and we discuss them further in Chapter 4.\n\n2.2.1 Attribution Method Categories\nWe now briefly summarize common families of input attribution methods, which are employed throughout the first part of this thesis. An in-depth overview of input attribution techniques for natural language processing can be found in Madsen et al. (2022).\nGradient-based attribution For neural network models like transformer LMs, gradients are a natural source of input saliency which can be exploited for attribution purposes (Simonyan et al., 2014; Li et al., 2016). A simple gradient-based attribution corresponds to a first-order Taylor expansion of the model at a point \\(\\mathbf{x}\\), expressed as \\(\\nabla \\mathbf{f}(\\mathbf{x}) \\cdot \\mathbf{x} + \\mathbf{b}\\). The resulting gradient \\(\\nabla_\\mathbf{x}^c {\\mathbf{f}}\\) captures intuitively the sensitivity of the model prediction \\(c\\) to each element in the input. In the case of transformer LMs, \\(\\nabla_\\mathbf{x}^{t^*} {\\mathbf{f}} \\in \\mathbb{R}^{S \\times d}\\), i.e. every dimension of the input embedding is associated with a attribution score, and the logit of the top predicted token \\(t^*\\) is used as differentiation target for gradient computation.4 These scores are generally aggregated at a token level to obtain a more intuitive overview of the influence of individual tokens. This is commonly done by taking the \\(L^p\\) norm of the gradient vector:\n\\[\\text{Grad}_{\\,\\mathbf{f}(\\mathbf{x}) \\leftarrow t^*} = \\|\\nabla_{\\mathbf{x}}^{t^*} \\mathbf{f}\\|_p \\in \\mathbb{R}^{S} \\tag{2.14}\\]\nFigure 2.3 shows an example of gradient attribution on a language model. By taking the dot product between the gradient vector and the input embedding \\({\\nabla_{\\mathbf{x}}^{t^*} \\mathbf{f}\\cdot \\mathbf{x}}\\), known as the gradient \\(\\times\\) input method, this sensitivity information can be converted to an importance estimate. More elaborate gradient-based attribution methods employ perturbations of the input embedding (Sundararajan et al., 2017; Smilkov et al., 2017) or ad-hoc gradient propagation rules (Bach, 2015; Achtibat et al., 2024) to filter noisy gradient information.\n\n\n\n\n\n\nFigure 2.3: Gradient-based attribution in a neural language model.\n\n\n\nGradient-based attribution methods are heavily used in the investigations of Chapter 3, Chapter 4 and Chapter 5, representing the majority of methods supported by the Inseq toolkit and the most effective approaches for contextual cues imputation in the PECoRe framework. Notably, gradient attribution can be exploited in a similar way to identify the importance of intermediate states \\(\\mathbf{z}\\) in the model, as opposed to input representations \\(\\mathbf{x}\\), i.e. using \\(\\nabla_{\\mathbf{z}}^{t^*} \\mathbf{f}\\). The CAT method proposed in Chapter 3 case study adopts this attribution-based approach to locate factual knowledge across LM layers.\nPerturbation-based attribution Another popular family of approaches estimates input importance by adding noise or ablating input elements and measuring the resulting impact on model predictions. For instance, the input token \\(w_j\\) at position \\(j\\) can be removed, and the resulting probability difference \\(p(t^*|t_{&lt;i}) - p(t_{\\setminus w_j}^*|t_{&lt;i})\\), where \\(t^*\\) is the predicted token for current sequence position \\(i\\) and \\(j &lt; i\\), can be used as an estimate for its importance. If the logit or probability given to \\(w\\) does not change, we conclude that the \\(i\\)-th token has no influence. A multitude of perturbation-based attribution methods exist in the literature, such as those based on local surrogate models such as LIME (Ribeiro et al., 2016), or those derived from game theory like SHAP (Lundberg and Lee, 2017). Notably, some architecture-specific methods such as Value Zeroing (Mohebbi et al., 2023) have been proposed to mitigate the disruptive impact of perturbations on model behaviors. A comprehensive framework unifying various perturbation-based approaches is presented by Covert et al. (2021).\nContext mixing for attribution Model internals such as the attention weights \\(\\alpha\\) presented in Section 2.1.2 were initially proposed as possible explanations for model behavior (Bahdanau et al., 2015), but were found unfaithful in reflecting the actual predictive behavior of language models (Jain and Wallace, 2019; Bastings and Filippova, 2020). This is because, contrary to other approaches, they only accounted for the importance of specific model components, rather than a more general notion of saliency across the full model. However, recent methods have proposed more refined estimates of token contributions exploiting internals to quantify the information flow within LMs. Some of these alternatives include the use of the norm of value-weighted vectors and output-value-weighted vectors (Kobayashi et al., 2020; Kobayashi et al., 2021), or the use of vectors’ distances to estimate token contributions (Ferrando et al., 2022). These methods result in a set of attribution scores \\(\\mathbf{a}_{\\mathbf{f}(\\mathbf{x})} \\in \\mathbb{R}^{S \\times L}\\), marking the contribution of position-specific representation across all layers \\(1, \\ldots, L\\) of the model. These per-layer attributions reflecting context mixing patterns are often aggregated using techniques such as rollout (Abnar and Zuidema, 2020), resulting in one score per input token participating in the attention operation. Such context mixing approaches have shown competitive faithfulness compared to best gradient and perturbation-based methods, despite employing only a single forward pass to estimate contributions.\nContrastive input attribution An important limitation of input attribution methods for interpreting language models is that attributed output tokens belong to a large vocabulary space, often having semantically equivalent tokens competing for probability mass in next-word prediction (Holtzman et al., 2021). In this context, attribution scores are likely to misrepresent several overlapping factors such as grammatical correctness and semantic appropriateness driving the model prediction. Recent work addresses this issue by proposing a contrastive formulation of such methods, producing counterfactual explanations for why the model predicts token \\(t^*\\) instead of an alternative token \\(t^\\sim\\). Yin and Neubig (2022) extend the vanilla gradient method of Equation 2.14 to the contrastive setting as:\n\\[\\text{ContGrad}_{\\,\\mathbf{f}(\\mathbf{x}) \\leftarrow t^*, t^\\sim} = \\nabla_{\\mathbf{x}}^{t^* - t^\\sim} \\mathbf{f} \\tag{2.15}\\]\nWe employ this formulation in the PECoRe framework in Chapter 4 and its extension of Chapter 5 to identify salient context cues for generated tokens that were highly influenced by context.\n\n\n2.2.2 Evaluating and Using Attribution Methods\nPlausibility and Faithfulness The evaluation of input attribution methods can be operationalized in terms of various desiderata. Plausibility, also referred to as “human-interpretability” (Lage et al., 2019), is a measure of “how convincing the interpretation is to humans” (Jacovi and Goldberg, 2020), i.e. how well the salient tokens identified by an attribution method are in agreement with those selected by human annotators. It is important to note that plausibility does not imply faithfulness, i.e. how accurately the rationale reflects the true reasoning process of the model (Wiegreffe and Pinter, 2019), since a good explanation of model behavior might not align with human intuition. Consider the following sentence from the BLiMP corpus (Warstadt et al., 2020).\n\n\\(\\mathbf{x}\\) = A report about the Impressionists has/\\(*\\)have won the competition.\n\nFor the sentence to be grammatically correct, the verb to have must be correctly inflected as has to agree with the preceding noun report. Hence, to evaluate the plausibility of a language model for this example, the model is provided with the prefix \\(\\mathbf{x}'\\) =“A report about the Impressionists”. Then, attribution scores are computed for every input token towards the prediction of has as the next token. Finally, we verify whether these scores identify the token report as the most important to predict has. We note that the selection of the pair report-has in the canonical procedure described above is entirely based on grammatical correctness, and other potential pairs not matching these constraints are not considered (e.g. the usage of report to predict writing instead of has as a likely continuation). This common procedure might also cause reasonable behaviors to be labeled as implausible. For example, the indefinite article A might be identified as the most important token to predict has since it is forcibly followed by a singular noun and can co-occur with has more frequently than report in the model’s training data. These limitations in the standard hypothesis-driven approach to plausibility evaluation motivate our proposal for PECoRe as a data-driven alternative in Chapter 4.\nLimitations of input attribution methods While input attribution methods are commonly used to debug failure cases and identify biases in models’ predictions (McCoy et al., 2019), popular approaches were shown to be insensitive to variations in the model and data generating process (Adebayo et al., 2018; Sixt et al., 2020), to disagree with each others’ predictions (Atanasova et al., 2020; Crabbé and Schaar, 2023; Krishna et al., 2024) and to show limited capacity in detecting unseen spurious correlations (Adebayo et al., 2020; Adebayo et al., 2022). Importantly, popular methods were found provably unreliable at predicting counterfactual model behavior in realistic settings (Bilodeau et al., 2024). Apart from theoretical limitations, perturbation-based approaches also suffer from out-of-distribution predictions induced by unrealistic noised or ablated inputs, and from high computational cost of targeted ablations for granular input elements.\nTools for input attribution The captum library (Kokhlikyan et al., 2020) is part of the Pytorch ecosystem providing access to several gradient and perturbation-based input attribution methods for any Pytorch-based model, with the recent addition of utilities for simplifying attribution analyses of generative LMs (Miglani et al., 2023). Several captum-based tools provide convenient APIs for input attribution of transformer-based models, notably Transformers Interpret (Pierse, 2021), ferret (Attanasio et al., 2023) and Ecco (Alammar, 2021), which are mainly centered around language classification tasks. SHAP (Lundberg and Lee, 2017) is a popular toolkit mainly centered on perturbation-based input attribution methods and model-agnostic explanations for various data modalities. The saliency library5 provides framework-agnostic implementations for mainly gradient-based input attribution methods, while LIT (Tenney et al., 2020) is a framework-agnostic tool providing a convenient set of utilities and an intuitive interface for interpretability studies spanning input attribution, concept-based explanations and counterfactual behavior evaluation. It notably includes a visual tool for debugging complex LLM prompts (Tenney et al., 2024). More recent low-level interpretability tools such as nnsight (Fiotto-Kaufman et al., 2025) also support attribution, without explicitly providing abstractions to facilitate its usage. inseq, which we introduce in Chapter 3 as part of this thesis’ contributions, is one of the most popular tools for input attribution of generative LMs, supporting advanced approaches for contrastive context attribution (Sarti et al., 2024) and context mixing evaluation.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "chapters/chap-2-background.html#sec-chap2-steer",
    "href": "chapters/chap-2-background.html#sec-chap2-steer",
    "title": "2  Background",
    "section": "2.3 Conditioning Language Model Generations",
    "text": "2.3 Conditioning Language Model Generations\nThis section describes the two main families of approaches for conditioning the behavior of language models during text generation. First, we present methods for modifying the input context by providing relevant information retrieved from external sources, or demonstrations of desired behavior, which we use in Chapter 5, Chapter 6, and 7. Then, we discuss approaches for modifying the model’s internal representations to achieve targeted interventions in the generation process, which we compare to prompting methods in Chapter 7.\n\n2.3.1 Controlling Input Context\nLarge language models have become widely popular due to their ability to adjust their predictions in light of few examples or relevant information provided in an input context (prompt), without requiring additional training (Brown et al., 2020). Prompting LLMs to exploit their in-context learning skills has become pervasive in the NLP community, with much effort devoted to designing effective prompts for various tasks (Dong et al., 2024).\nFew-shot prompting is an effective approach to adapt LLMs to new tasks by providing a few demonstrations of the desired behavior in the input context. For example, to perform a translation, a few source language examples can be provided in the prompt with their respective target language translations, and the model is expected to translate new source entries used as queries (Figure 2.4, left). Zero-shot prompting is a more challenging task, where the model is expected to perform well on a new task without any demonstrations, relying solely on its pre-trained knowledge. While effective, several studies highlighted the brittleness of prompting to unexpected factors such as the order of provided examples (Lu et al., 2022). In this thesis, we use few-shot prompting in our attribute-controlled translation experiments of Chapter 6 and our literary translation experiments of Chapter 7.\n\n\n\n\n\n\nFigure 2.4: Left: Few-shot prompting for English\\(\\rightarrow\\)Italian translation. Right: Retrieval-augmented generation for factual question answering. Relevant paragraphs are dynamically retrieved and infilled in the prompt using their similarity to the query to improve answer quality.\n\n\n\nRetrieval-augmented generation (RAG) is a different approach for conditioning generation where the model is provided with relevant context paragraphs retrieved on-the-fly from an external dataset, such as Wikipedia or a domain-specific corpus. This context is then used to inform the model’s predictions, allowing it to generate more accurate and relevant responses without relying solely on its potentially faulty pre-training knowledge (Figure 2.4, right). RAG has been shown to be effective in improving the factual accuracy of model outputs and reducing hallucinations (Lewis et al., 2020; Petroni et al., 2020). However, it is not directly obvious which retrieved paragraphs are motivating the model’s predictions, a challenge we address via input attribution in Chapter 5. Chapter 6 also employs a similarity retrieval component to control the examples selected for few-shot prompting, showing that example selection leads to better performances in machine translation with LLMs.\n\n\n2.3.2 Controlling Model Representations\nTechniques for conditioning model behavior by modifying the model’s internal representations are commonly referred to as steering methods, and often exploit the linear structure of model activations to achieve simple targeted interventions. Indeed, the linear representation hypothesis states that latent properties of interest—for example, the tone of a response—are encoded as linear subspaces of the representation space in language model activation (Park et al., 2023). Such property was already observed in early work on word embeddings (Mikolov et al., 2013), where the direction of the vector between two words was shown to encode their semantic relationship, e.g. \\(\\mathbf{z}_{\\text{king}} - \\mathbf{z}_{\\text{man}} + \\mathbf{z}_{\\text{woman}} \\approx \\mathbf{z}_{\\text{queen}}\\).\nRecent work highlighted the effectiveness of linear interventions on language models representations using directions identified by a probing classifier, i.e. a model \\(\\mathbf{p}: \\mathbb{R}^{d} \\to \\mathcal{C}\\) trained to predict a specific property of interest \\(c \\in \\mathcal{C}\\) from the intermediate representation of a trained transformer LM (Köhn, 2015; Gupta et al., 2015; see Belinkov, 2022 for a review). For instance, adding negative multiples of the sentiment direction (\\(\\mathbf{c}_\\text{sent}\\)) to the residual stream, i.e. modifying the activation \\(\\mathbf{z}^l\\) as \\({\\tilde{\\mathbf{z}}^l \\leftarrow \\mathbf{z}^l - \\alpha \\mathbf{c}_\\text{sent}}\\), where here \\(\\alpha\\) is a pre-selected steering coefficient controlling the intensity of the intervention, is sufficient to generate a text exhibiting the opposite sentiment label (Tigges et al., 2024). This simple procedure, known as activation addition, has become popular for conditioning desired attributes in model generations, including multiple properties at once (Scalena et al., 2024). Some of its variants omit probing classifiers and employ other unsupervised methods for computing feature directions, such as K-Means clustering of representations for examples showing a desired property (Zou et al., 2024), or mean difference between representations for positive and negative sets of demonstrations (Marks and Tegmark, 2024; Arditi et al., 2024).\nWu et al. (2024) describe a broader framework for representation steering, proposing the use of learnable interventions for conditioning generation at specific steps with variable intensity. Formally, an intervention \\(I\\) can be defined as a tuple composed by an intervention function \\(\\xi: \\mathbb{R}^d \\to \\mathbb{R}^d\\) with learnable parameters, a set of input positions \\(P \\subseteq \\{1, \\dots, S\\}\\) that the intervention is applied to and the layer \\(l\\) at which the intervention is applied. This framework, dubbed representation fine-tuning (ReFT), allows to learn interventions overriding \\(\\mathbf{z}^l\\) as:\n\\[z^l_i = \\begin{cases}\n\\xi(\\mathbf{z}^l_i), & \\text{if}\\; i \\in P \\\\\n\\mathbf{z}^l_i, & \\text{otherwise}\n\\end{cases}\n\\tag{2.16}\\]\nThe intervention function can be learned by minimizing the normal cross-entropy loss with a next token prediction objective, optimizing only the parameters of the intervention function. Activation addition (ActAdd) can then be described as a special case in this broader framework, where the intervention function \\(\\xi\\) is constant and applied at all generation steps. In the experiments of Chapter 7, we use ActAdd and ReFT as baselines for our proposed steering method.\nThe final steering approach we discuss in this section involves the use of sparse autoencoders[SAEs; Huben et al. (2024)] for conditioning model behavior. SAEs have become widely adopted for analyzing the representations learned by transformer LMs thanks to their ability to address polysemanticity, i.e. the entanglement of multiple concepts within learned model representations. Indeed, neurons in transformer LMs were observed to activate on diverse and semantically distinct contexts, with concepts being encoded in a distributed manner across multiple units (Smolensky, 1986; Olah, 2023). In light of this, and given the disparity between the relatively low-dimensional representations learned by transformer LMs and the vast array of abilities they acquire during training, latent concept representations were speculated to be encoded in superposition across various model units (Arora et al., 2018), i.e. that multiple neurons jointly encode the presence of a single concept (Figure 2.5, left). A concrete example of this phenomenon is given by Elhage et al. (2022), where superposition is observed in presence of a long tail of sparse concepts in the training dataset.\n\n\n\n\n\n\nFigure 2.5: Left: Concepts encoded in a 2-dimensional parameter space. (a) Polysemanticity can be observed when concept do not align with the standard basis, i.e. they are encoded jointly by multiple units. (b) If concepts align perfectly with neurons, these neurons are monosemantic. (c) When the number of concepts exceeds the number of parameters, polysemanticity is inevitable and superposition is observed. Right: Sparse autoencoder (SAE) trained to reconstruct a model’s internal representations \\(\\mathbf{z}\\). Interpretable SAE concepts are found in rows of \\(\\mathbf{W}_{\\text{dec}}\\). Biases are omitted for clarity.\n\n\n\nA possible strategy to disentangle concepts in superposition involves finding an overcomplete feature basis via dictionary learning (Olshausen and Field, 1997; Donoho and Elad, 2003). SAEs are simple autoencoder neural networks, i.e. models trained to reconstruct their input, that can be trained to reconstruct internal representations \\(\\mathbf{z} \\in \\mathbb{R}^{d}\\) of a neural network exhibiting superposition. Their training objective encourages the model to learn a sparse coding of the input representation through an ad-hoc loss term, resulting in a sparse dictionary of learned concepts. Huben et al. (2024) and Bricken et al. (2023) propose training SAEs on transformer LM representations using the form:\n\\[\n\\begin{aligned}\n    \\text{SAE}(\\mathbf{z}) &= h(\\mathbf{z})\\,\\mathbf{W}_{\\text{dec}} + \\mathbf{b}_{\\text{dec}} \\\\\n    \\text{with}\\; h(\\mathbf{z}) &= \\sigma\\big((\\mathbf{z} - \\mathbf{b}_{\\text{dec}})\\mathbf{W}_{\\text{enc}} + \\mathbf{b}_{\\text{enc}}\\big) \\\\\n\\end{aligned}\n\\tag{2.17}\\]\nusing the loss function:\n\\[\\mathcal{L}(\\mathbf{z}) = \\|\\mathbf{z} - \\text{SAE}(\\mathbf{z})\\|_2^2 + \\alpha \\|h(\\mathbf{z})\\|_1 \\tag{2.18}\\]\nwhere \\(\\sigma\\) is a non-linear activation function, \\(\\mathbf{W}_{\\text{enc}}\\) and \\(\\mathbf{W}_{\\text{dec}}\\) are the encoder and decoder learned weight matrices, respectively, and \\(\\alpha\\) is a hyperparameter controlling the sparsity of the learned representation. The first term in Equation 2.18 is the reconstruction term, accounting for the quality of reconstruction, while the second term is the sparsity term, which promotes sparsity. The SAE architecture is illustrated in Figure 2.5 (right).\nIf \\(h(\\mathbf{z}) \\in \\mathbb{R}^{m}\\) and \\(m \\gg d\\), \\(\\mathbf{z}\\) can be approximated as a sparse linear combination of the learned rows in the dictionary \\({\\mathbf{W}_{\\text{dec}} \\in \\mathbb{R}^{m \\times d}}\\), ideally representing monosemantic concepts. Similarly to activation addition, these concepts can be used to steer model behavior by scaling them using a steering coefficient before reconstruction, resulting in a modified representation \\(\\tilde{\\mathbf{z}}\\). We use a similar approach in our SAE-based steering method we present in Chapter 7.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "chapters/chap-2-background.html#sec-chap2-mt",
    "href": "chapters/chap-2-background.html#sec-chap2-mt",
    "title": "2  Background",
    "section": "2.4 Machine Translation",
    "text": "2.4 Machine Translation\nMachine translation is a long-standing task in natural language processing, with the goal of automatically translating text from a source language to another target language. In this section, we provide a brief overview of the evolution of machine translation approaches, describe how transformer LM architectures are commonly used for machine translation, and how such models can handle multiple languages and contextual information.\nThe history of machine translation can be summarized in three main phases. Between the 1960s and the 1980s, the first successes of machine translation were attained by rule-based systems exploiting various techniques, ranging from direct translation using dictionaries with a set of reordering rules to ambitious methods aiming to exploit an interlingua to act as a bridge when mapping meaning across languages (Hutchins, 2001). As for most rule-based methods, however, these approaches were limited by the need of ad-hoc rules, which could hardly account for less frequent and challenging settings. From the 1990s onwards, the statistical paradigm took foot by exploiting large bilingual corpora made available by the birth of the World Wide Web to train statistical language models parametrized as tables of co-occurrence probabilities (Och et al., 1999), with popular approaches aiming to segment challenging sentences into simpler phrases for ease of translation via co-occurrences (Koehn et al., 2003) or syntactic analysis (Hadiwinoto, 2017). In 2013, the advent of word embeddings coincided with the first MT systems based on continuous language representations parametrized by neural networks (Kalchbrenner and Blunsom, 2013), marking the advent of the neural MT (NMT) paradigm that remains the current state-of-the-art for machine translation. While the architecture of NMT systems has barely changed since the introduction of the transformer, as for most NLP tasks the introduction of large pre-trained language models has led to general-purpose models able to handle various translation-related task via light tuning and ad-hoc prompting (Alves et al., 2024).\nProvided that machine translation involves the generation of a sequence of translated target tokens, it is straightforward to see how such task can fit well into the sequence-to-sequence framework adopted by neural language models. Given a sequence of tokens \\(\\mathbf{x} = (x_1, x_2, \\ldots, x_{S_s})\\) in the source language \\(s\\), a language model can be trained to generate a sequence of target tokens \\(\\mathbf{y} = (y_1, y_2, \\ldots, y_{S_t})\\) in the target language \\(t\\) using the classic cross-entropy loss function. The transformer module we presented in Section 2.1.3 corresponds to the decoder-only architecture currently preferred for language modeling, involving a single stack of blocks. However, the original model proposed by Vaswani et al. (2017) followed the traditional encoder-decoder structure adopted in MT, with an additional dedicated component for encoding source information and influencing the generation of target tokens.\nThe encoder-decoder transformer architecture for machine translation is illustrated in Figure 2.6. The encoder processes the source sentence \\(\\mathbf{x}\\) and produces a sequence of contextualized representations \\(\\mathbf{Z}^{L_{\\text{enc}}}_{\\text{enc}} \\in \\mathbb{R}^{S_s \\times d_\\text{enc}}\\) capturing the meaning of the source sentence. When generating the \\(i\\)-th token in the target sentence, every block of the decoder then attends to the target prefix \\(\\mathbf{y}_{&lt;i}\\) using the self-attention module (MHSA) presented in Section 2.1.2, and complements this with a multi-head cross-attention (MHCA) mechanism integrating information from encoder representations \\(\\mathbf{Z}^{L_{\\text{enc}}}_{\\text{enc}}\\). Functionally, the cross-attention module is identical to self-attention, but employs encoder representations to generate key and value vectors, while the query vectors are generated from the decoder representations.\n\n\n\n\n\n\nFigure 2.6: Transformer encoder-decoder architecture for neural machine translation. The encoder processes the source sentence and produces a sequence of contextualized representations, while the decoder generates the target sentence using causal self-attention (MHSA) and cross-attention (MHCA) mechanisms. The last decoder state is projected to the vocabulary space by the prediction head, and the next word is selected.\n\n\n\nWhile encoder-decoder transformers were traditionally trained from scratch on the machine translation task, the current state-of-the-art adapts pre-trained decoder-only LLMs with ad-hoc supervised tuning (Cui et al., 2025; Rei et al., 2024; Xu et al., 2024). Our experiments reflect this paradigm shift: initial MT experiments in Chapter 4, Chapter 8 and Chapter 9 employ traditional encoder-decoder, single-purpose translation models, while in Chapter 6 and Chapter 7 we generate translations by prompting general-purpose LLMs. Finally, Chapter 10 evaluates methods on both model types.\nMultilingual machine translation Even before the advent of LLM-based translation systems, an important trend in MT research involved the training of massively multilingual MT (MMT) models capable of producing direct translations across hundreds of translation directions (Aharoni et al., 2019). Such approach was shown to bring improvements over previous methods requiring an intermediate translation step into a high-resource pivot language when two less-resourced languages were used as source and target (Kim et al., 2019). MMT models are typically trained on large multilingual web corpora with similarity-matched sentence pairs in different languages (Schwenk et al., 2021), using special language tags such as &lt;eng_Latn&gt; as prefixes to mark source and target languages. After training, a translation into a specific language can be produced by prepending the respective language tag to the target sequence, biasing model generation towards tokens matching that language. This thesis makes ample use of encoder-decoder MMT models, such as mBART-50 (Tang et al., 2021), trained to translate from English to 50 languages (one-to-many MMT), M2M-100 (Fan et al., 2021), with many-to-many translation between 100 languages, and finally No Language Left Behind [NLLB; NLLB Team et al. (2024)], covering 200 languages in all directions. Decoder-only LLMs are generally trained on variable amounts of multilingual data6, and hence exhibit some degree of multilingual ability without additional MT tuning.\nContext-aware machine translation Inter-sentential context is often fundamental for resolving discourse-level ambiguities during translation (Müller et al., 2018; Bawden et al., 2018; Voita et al., 2019; Fernandes et al., 2023b). Traditional MT systems were trained at segment level due to their limited ability in handling long context, potentially losing important contextual information that spans beyond sentence boundaries, resulting in lower performances in realistic settings (Läubli et al., 2018; Toral et al., 2018a). Context-aware MT approaches aimed to address this limitation by incorporating document-level information to improve translation quality and consistency, leading to improved performance when translating cohesive discourse phenomena such as anaphora resolution, lexical cohesion, and maintaining consistent terminology within a document (Voita et al., 2018; Maruf and Haffari, 2018). Initial context-aware approaches for NMT employed methods ranging from concatenating multiple source sentences to employing hierarchical attention mechanisms that explicitly model document structure (Miculicich et al., 2018; Zhang et al., 2018). We use one such methods, namely concatenating context and current source text using a special &lt;brk&gt; tag, for the NMT models we analyze in Chapter 4. Recent LLM-based translation systems can naturally process longer contexts and maintain better consistency across document boundaries (Wang et al., 2023; Briakou et al., 2024).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "chapters/chap-2-background.html#sec-chap2-pe",
    "href": "chapters/chap-2-background.html#sec-chap2-pe",
    "title": "2  Background",
    "section": "2.5 MT Post-Editing and Evaluation",
    "text": "2.5 MT Post-Editing and Evaluation\nThe landscape of machine translation has undergone a fundamental transformation in recent decades, shifting from a tool primarily designed for professional translators to a technology accessed by millions of lay users worldwide (Savoldi et al., 2025). In this section, we review MT post-editing tools and practices, and discuss how MT outputs are evaluated by means of automatic metrics and human annotators.\n\n2.5.1 Post-editing MT\nSince the inception of MT technologies in professional translation workflow, human post-editing has been a crucial step to ensure quality and mitigate potential critical errors, especially for low-resource settings (Wagner, 1983; Church and Hovy, 1993). The industry distinguishes between two primary post-editing levels: light post-editing, which focuses on correcting only critical errors affecting comprehension while tolerating stylistic imperfections, and full post-editing, which aims to achieve human translation quality standards. The choice between these approaches involves trade-offs between effort investment and quality requirements, with light post-editing being faster while maintaining acceptable quality for many use cases (Plitt and Masselot, 2010). Seminal post-editing studies highlighted an increase in translators’ productivity following MT adoption (Guerberof, 2009; Green et al., 2013; Läubli et al., 2013; Plitt and Masselot, 2010; Parra Escartín and Arcedillo, 2015). However, they also struggled to identify generalizable findings due to confounding factors like output quality, content domains, and high variance across language pairs and human subjects. With the advent of NMT, productivity gains of the new approach were extensively compared to those of statistical MT (Castilho et al., 2017; Bentivogli et al., 2016; Toral et al., 2018b; Läubli et al., 2019). Initial results were promising for NMT due to its better fluency and overall results. Moreover, translators were shown to prefer NMT over SMT for post-editing, although a pronounced productivity increase was not always present. In more recent times, various works explored the usage of adaptive MT systems that learn from post-editing feedback in real-time (Turchi et al., 2017; Karimova et al., 2018), with the goal of progressively reducing repetitive corrections and adapting to translator preferences. Notably, recent estimates confirm that human-machine collaboration can match or even exceed the quality of human-only translations, with potential cost reductions estimated at around 60% the price of full human post-editing (Liu et al., 2024).\nThe main metric of evaluation for post-editing in the industry is productivity, often operationalized as the amount of source characters or word revised per minute. On the other hand, post-editing research often complements productivity measurements with editing effort alongside its temporal, technical and cognitive components (Krings, 2001), corresponding to editing time, number of keystrokes and pauses between keystrokes during the editing process, respectively. Importantly, the cognitive and temporal demands of post-editing were found to vary significantly depending on various factors, such as error types and user expertise. For example, Daems et al. (2017) found that certain error categories have disproportionate impacts on post-editing effort, with adequacy errors often requiring more cognitive resources than fluency errors, even though the latter may be more immediately apparent to users (Martindale and Carpuat, 2018). Domain-specific considerations further complicate this landscape, as technical domains may tolerate certain stylistic variations while requiring precise terminology, whereas literary translation may prioritize creative renditions of meaning (Guerberof-Arenas and Toral, 2022).\nProfessional translators typically post-edit texts through computer-assisted translation (CAT) tools, which are interfaces designed to enhance human translators’ productivity by providing access to keyboard shortcuts, quality estimation (which we discuss in Section 2.6) and other assistive technologies (Bowker, 2002). A common functionality of CATs is the integration of translation memories (TMs), which are bilingual databases storing previously translated content that can be retrieved and reused for similar segments, mimicking the functioning of early example-based MT systems (Garcia, 2009). Additional features often include terminology management systems (termbases) for maintaining consistency in technical terms and brand names, automatic text segmentation, and quality assurance modules such as spellcheckers for detecting errors and inconsistencies. Modern CAT tools have evolved from standalone desktop software to cloud-based platforms accessible via web browsers (Moran et al., 2014; Federico et al., 2014), with recent surveys indicating that 88% of professional translators use at least one CAT tool for their work.7 While many CAT tools nowadays offer multiple advanced features, including LLM-based AI assistants, in our user studies of Chapter 8 and Chapter 9, we employ simple research-oriented interfaces with minimal text editing functionalities to ensure equal proficiency across subjects. In Chapter 8 we employ PET (Aziz et al., 2012), a simple desktop-based post-editing tool supporting various languages, while in Chapter 9 we use a custom-built web interface supporting editing over highlighted error spans.\n\n\n2.5.2 MT Evaluation\nThe industrial context had historically an important influence on MT evaluation practices, encouraging researchers to focus on evaluation efficiency, combining automatic metrics with human assessment, and metrics that could provide concrete benefits when employed in professional translation workflows.\nAutomatic MT Metrics. Automatic evaluation metrics for machine translation have been widely adopted since the early 2000s, with the most popular metrics being BLEU (Papineni et al., 2002). BLEU is a simple and inexpensive metric measuring lexical similarity between a candidate translation \\(\\hat y\\) and its given reference \\(y\\) as the number of \\(n\\)-grams \\(G_n = {\\hat y_1, \\dots, \\hat y_n, \\hat y_2, \\dots, \\hat y_{n+1}, \\dots}\\) shared between them, normalized by the total n-gram count:\n\\[p_n(y, \\hat y) = \\frac{\\sum_{s \\in G_n} \\min(C(s,\\hat y), C(s,y))}{\\sum_{s \\in G_n} C(s,\\hat y)}\\]\nwhere \\(C(s, y)\\) is the count of n-gram \\(s\\) in sequence \\(y\\). The complete BLEU score also incorporates a brevity penalty to discourage overly short translations. BLEU is computed at segment-level for an entire corpus of candidate and reference translations, and averaged to obtain a corpus-level score. Multiple variants of BLEU have been proposed to account for length bias, multiple references, with other metrics such as chrF (Popović, 2015) adopting similar lexicon-based approaches at the character level, or aligning n-grams across the two sequences (Banerjee and Lavie, 2005). Other lexical metrics such as the Translation Error Rate (Snover et al., 2006) or Word Error Rate (WER) have been used to connect the quality of the candidate sequence to the number of edits required to convert it into the reference, grounding the evaluation in post-editing technical effort. While these metrics provide rapid assessment of translation quality with minimal computational overhead, they suffer from several limitations: sensitivity to lexical variations that may not reflect translation quality differences, poor correlation with human judgments for high-quality neural MT outputs, and limited generalization across different writing systems (Bugliarello et al., 2020).\nFollowing calls from the MT research community (Freitag et al., 2022), the limitations of lexical metrics led to the widespread adoption of learned metrics trained to predict translation quality from large amounts of annotated examples. Most of the widely used learned MT metrics employ transformer-based encoder-only pretrained LMs such as BERT (Devlin et al., 2019) or the cross-lingual model XLM (Conneau and Lample, 2019). Among the most notable metrics, Bleurt (Sellam et al., 2020) is a BERT-based model using multi-task loss on synthetic data to perform regression of human quality judgments, while comet (Rei et al., 2020) feeds source text, candidate and reference translation triples to a dual cross-lingual encoder structure that jointly learns to estimate quality and rank multiple candidate translations. In most of our MT evaluations we employ the comet metric due to its excellent performance across hundreds of languages, which resulted in top-scoring submissions at multiple WMT metrics shared tasks (Rei et al., 2020; Rei et al., 2021; Rei et al., 2022a).8 However, learned metrics introduce their own challenges, including non-trivial computational requirements, potential biases inherited from training data, and questions about generalization to out-of-domain content (Amrhein and Sennrich, 2022)\nHuman evaluation of MT. Human evaluation, despite its challenges due to inconsistencies across annotators, cultural and linguistic biases, and high costs, remains the gold standard for assessing machine translation quality, providing crucial insights that automatic metrics may fail to capture (Freitag et al., 2021). Historically, human assessment of MT was centered around the notions of adequacy (also accuracy or fidelity), comprehensibility and fluency (or grammaticality) (White et al., 1994; Callison-Burch et al., 2007), with adequacy measuring how well the original meaning is conveyed, comprehensibility reflecting how understandable MT is without the original source, and fluency judging whether appropriate target grammar is employed (Popović, 2020). MT evaluation campaigns since 2017 adopted a continuous direct assessment (DA) of translation quality using scalar ratings— for example, using a 0-100 scale as in Graham et al. (2013) —or comparative ranking of multiple system outputs (Bojar et al., 2017).\nMore recently, the introduction of the Multidimensional Quality Metric (MQM) (Lommel et al., 2013) has provided more structured evaluation protocols. MQM is an established framework allowing annotators to identify and categorize specific spans in a translated text as accuracy, fluency, and style issues, and assign them a level of severity (typically, a 3-way classification into minor/major/critical). Freitag et al. (2021) experiments with various scoring configurations, resulting in the scoring formula:\n\\[\\text{MQM} = (\\text{\\# Major Err.} \\times 5) + (\\text{\\# Minor Err.} \\times 1) + (\\text{\\# Punct. Err.} \\times 0.1)\\]\nwith higher scores corresponding to worse translation, resulting in a high correlation with judgments from expert raters. However, such scheme has been criticized due to its potential length bias, with recent proposals for calibrated and non-linear scoring models accounting for similar issues (Lommel et al., 2024). An example description of MQM error categories and severity levels we employed for our study in Chapter 9 is presented in Table 9.1.\nRecent evaluation campaigns such as WMT 2024 (Kocmi et al., 2024a) have increasingly adopted the MQM protocol for their evaluation, emphasizing in particular the importance of expert vs. non-expert annotators, with studies showing that translation professionals provide more consistent and reliable judgments compared to crowd-sourced annotations (Freitag et al., 2021). The advent of large language models has introduced new challenges for human evaluation, as the quality gap between human and machine translation continues to narrow, requiring more fine-grained assessment criteria and larger annotator pools to achieve reliable results (Kocmi et al., 2024a). The main limiting factor towards the diffusion of the MQM evaluation protocol is its cost, since it involves a thorough annotation of error spans. Recently, the Error Span Annotation (ESA) protocol (Kocmi et al., 2024b) was introduced as a potential compromise between DA and MQM ratings, soliciting annotators to provide a 0-100 quality rating only after a light pass of error span identification, without requiring a full MQM error type categorization. The error annotation is intended to prime annotators to ground their quality judgments in empirical evidence, and ESA scores were observed to correlate strongly with MQM ones, while being 32% cheaper to obtain (Kocmi et al., 2024b). For this reason, we adopt a variant of the ESA protocol when conducting the quality assessment phase of our QE4PE study in Chapter 9. Zouhar et al. (2025) propose to use a language model to assist in the error span identification process, potentially further reducing the cost and effort involved in the ESA protocol.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "chapters/chap-2-background.html#sec-chap2-qe",
    "href": "chapters/chap-2-background.html#sec-chap2-qe",
    "title": "2  Background",
    "section": "2.6 Quality Estimation for MT",
    "text": "2.6 Quality Estimation for MT\nThe automatic MT metrics presented in Section 2.5 require the use of a reference translation to measure the quality of a given candidate. While effective, these metrics cannot be employed to evaluate translation candidates on the fly, for example before presenting them to human post-editors, or as a ranking procedure in advanced decoding strategies (Rei et al., 2022b). Moreover, the presence of low-quality references can lead to biased evaluations of MT quality that do not reflect the translation quality without tying it to a specific gold standard (Freitag et al., 2023). Quality estimation metrics (QE), also known as reference-free MT metrics, are an alternative category of techniques designed to address these limitations by predicting translation quality without requiring reference translations (Specia et al., 2018). Contrary to traditional MT evaluation, QE can be performed at various levels of granularity. On the one hand, when operating at the segment or document levels, QE methods typically returns a score between 0 and 1 reflecting the overall quality of the translation, which can be then used to guide post-editors to focus on problematic segments (Tamchyna, 2021). On the other hand, word-level QE metrics can provide more granular information about translation issues, and typically operate by marking individual words with binary OK/BAD labels or, more recently, following the severity scheme introduced by the MQM framework.\nInitial approaches to QE were mostly based on the uncertainty extracted from MT models (Blatz et al., 2004; Specia et al., 2009), but with time began focusing on supervised approaches involving ad-hoc model training (Turchi et al., 2013; Turchi et al., 2014; Kepler et al., 2019; Thompson and Post, 2020, inter alia). Advances in segment- and word-level QE research are regularly assessed in annual WMT campaigns (Fomicheva et al., 2021; Zerva et al., 2022; Zerva et al., 2024; Blain et al., 2023), where the best-performing QE systems have recently employed transformer-based language models trained to predict quality scores, in a fashion similar to reference-based metrics. In particular, reference-less counterparts to the comet models were introduced for QE applications, including a smaller model for efficient inference (Rei et al., 2022b).\nMore recently, the widespread adoption of the MQM paradigm and the advances in LLM capabilities led to new QE metrics predicting quality at various granularity levels. Notably, Kocmi and Federmann (2023) prompt GPT-4 with an annotation scheme mimicking MQM to produce fine-grained quality assessments, from which they derive a segment-level score, while Fernandes et al. (2023a) develop a similar AutoMQM framework using the PaLM-2 LLM. While these approaches usually employ proprietary models, Guerreiro et al. (2024) propose a state-of-the-art open-source QE model extending comet to jointly predict quality estimation at the word and the sentence level, combining sentence-level and word-level error span prediction for improved explainability of results. xcomet metrics come in a 3.5B (XL) and 10.7B (XXL) size and support both reference-based and reference-less usage, hence enabling usage for quality estimation purposes. Concretely, xcomet models are transformer encoders fine-tuned from pre-trained XLMR encoders (Goyal et al., 2021) using a mix of sentence-level Direct Assessment scores and word-level MQM error spans. We use their resulting systems for our user study of Chapter 9 and our metric comparison in Chapter 10.\nAside from supervised models, a return to unsupervised methods exploiting models uncertainty and their internal mechanisms was brought on by Fomicheva et al. (2020). In their work, such approaches were shown to rival state-of-the-art supervised QE models in predicting translation quality at the segment level. These methods typically rely on the model’s confidence in its predictions, often using metrics such as predictive probability or the entropy of the predictive distribution to mark low-confidence tokens as potential errors. The appeal of such methods lies in their efficiency, exploiting the knowledge of the MT model for error detection without requiring additional training on expensive human annotations. While such methods have been the object of multiple studies (Dale et al., 2023; Xu et al., 2023; Himmi et al., 2024; surveyed by Leiter et al., 2024), including a shared task dedicated to explainable QE metrics (Fomicheva et al., 2021), their evaluation was typically focused on segment-level evaluation quality, with word-level error spans being generally obtained by attributing the predictions of supervised segment-level metrics (Rubino et al., 2021; Rei et al., 2023). By contrast, recent work on LLMs evaluates various metrics to detect errors from the generator model, without additional systems involved, both at the sentence (Fadeeva et al., 2023) and at the token level (Fadeeva et al., 2024). Our evaluation of Chapter 10 involves various unsupervised metrics at the word level, employing the edits from our user studies of previous chapters as sources of word-level error spans to evaluate unsupervised word-level QE methods across multiple label sets. A notable technique for unsupervised QE is Monte Carlo Dropout (MCD) (Gal and Ghahramani, 2016). The dropout mechanism (Srivastava et al., 2014), commonly used for regularization during training, is employed at inference time by MCD to produce a set of noisy predictions from a unique model, approximating Bayesian inference. For a given input \\(\\mathbf{x}\\), \\(T\\) forward passes are performed through the network. In each pass \\(t \\in T\\), a different random dropout mask \\(\\Theta_t\\) is applied on model parameters, resulting in slightly different output probabilities \\(p(\\mathbf{x} \\mid \\Theta_t)\\). The set of \\(T\\) predictions \\(\\{p(\\mathbf{x} \\mid \\Theta_1), \\dots, p(\\mathbf{x} \\mid \\Theta_T)\\}\\) can be seen as samples from an approximate posterior distribution. These can be used, for example, to quantify model uncertainty as the variance of the set of probabilities for a specific token. We employ such method, showing promising performances in our evaluation of Chapter 10, to produce unsupervised error highlights for our QE4PE user study in Chapter 9.\nFrom a practical standpoint, QE methods are widely used in the translation industry for triaging automatic translations, with integrations in popular CAT tools to present users with segment-level quality scores (Tamchyna, 2021). While QE usage has been found helpful to increase the confidence and speed of human assessment (Mehandru et al., 2023; Zouhar et al., 2025), an incautious usage of these techniques can lead to a misplaced over-reliance on model predictions (Zouhar et al., 2021). Moreover, the effectiveness of QE-assisted post-editing depends critically on the accuracy of quality predictions, with inaccurate highlights potentially misleading translators and reducing overall productivity (Shenoy et al., 2021). Interfaces supporting word-level error highlights were developed for studying MT post-editing (Coppers et al., 2018; Herbig et al., 2020) and code reviewing (Sun et al., 2022; Vasconcelos et al., 2025), with results suggesting that striking the right balance of user-provided information is fundamental to improve the editing experience and prevent cognitive overload. Our user study of Chapter 9 is one of few works going beyond accuracy evaluations to measure the actual impact of word-level QE systems when integrated in human post-editing workflows.\n\n\n\n\nSamira Abnar and Willem Zuidema. 2020. Quantifying attention flow in transformers. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault, editors, Proceedings of the 58th annual meeting of the association for computational linguistics, pages 4190–4197, Online. Association for Computational Linguistics.\n\n\nReduan Achtibat, Sayed Mohammad Vakilzadeh Hatefi, Maximilian Dreyer, Aakriti Jain, Thomas Wiegand, Sebastian Lapuschkin, and Wojciech Samek. 2024. AttnLRP: Attention-aware layer-wise relevance propagation for transformers. In Proceedings of the 41st international conference on machine learning, Vienna, Austria. JMLR.org.\n\n\nJulius Adebayo, Justin Gilmer, Michael Muelly, Ian Goodfellow, Moritz Hardt, and Been Kim. 2018. Sanity checks for saliency maps. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in neural information processing systems, volume 31, pages 9505–9515, Montréal, Canada. Curran Associates, Inc.\n\n\nJulius Adebayo, Michael Muelly, Harold Abelson, and Been Kim. 2022. Post hoc explanations may be ineffective for detecting unknown spurious correlation. In International conference on learning representations.\n\n\nJulius Adebayo, Michael Muelly, Ilaria Liccardi, and Been Kim. 2020. Debugging tests for model explanations. In Proceedings of the 34th international conference on neural information processing systems, Red Hook, NY, USA. Curran Associates Inc.\n\n\nRoee Aharoni, Melvin Johnson, and Orhan Firat. 2019. Massively multilingual neural machine translation. In Jill Burstein, Christy Doran, and Thamar Solorio, editors, Proceedings of the 2019 conference of the north American chapter of the association for computational linguistics: Human language technologies, volume 1 (long and short papers), pages 3874–3884, Minneapolis, Minnesota. Association for Computational Linguistics.\n\n\nJ Alammar. 2021. Ecco: An open source library for the explainability of transformer language models. In Heng Ji, Jong C. Park, and Rui Xia, editors, Proceedings of the 59th annual meeting of the association for computational linguistics and the 11th international joint conference on natural language processing: System demonstrations, pages 249–257, Online. Association for Computational Linguistics.\n\n\nDuarte Miguel Alves, José Pombal, Nuno M Guerreiro, Pedro Henrique Martins, João Alves, Amin Farajian, Ben Peters, Ricardo Rei, Patrick Fernandes, Sweta Agrawal, Pierre Colombo, José G. C. de Souza, and Andre Martins. 2024. Tower: An open multilingual large language model for translation-related tasks. In First conference on language modeling.\n\n\nChantal Amrhein and Rico Sennrich. 2022. Identifying weaknesses in machine translation metrics through minimum Bayes risk decoding: A case study for COMET. In Yulan He, Heng Ji, Sujian Li, Yang Liu, and Chua-Hui Chang, editors, Proceedings of the 2nd conference of the asia-pacific chapter of the association for computational linguistics and the 12th international joint conference on natural language processing (volume 1: Long papers), pages 1125–1141, Online only. Association for Computational Linguistics.\n\n\nAndy Arditi, Oscar Obeso, Aaquib Syed, Daniel Paleka, Nina Panickssery, Wes Gurnee, and Neel Nanda. 2024. Refusal in language models is mediated by a single direction. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, Advances in neural information processing systems, volume 37, pages 136037–136083, Red Hook, NY, USA. Curran Associates, Inc.\n\n\nSanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, and Andrej Risteski. 2018. Linear algebraic structure of word senses, with applications to polysemy. Transactions of the Association for Computational Linguistics, 6:483–495.\n\n\nPepa Atanasova, Jakob Grue Simonsen, Christina Lioma, and Isabelle Augenstein. 2020. A diagnostic study of explainability techniques for text classification. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, Proceedings of the 2020 conference on empirical methods in natural language processing (EMNLP), pages 3256–3274, Online. Association for Computational Linguistics.\n\n\nGiuseppe Attanasio, Eliana Pastor, Chiara Di Bonaventura, and Debora Nozza. 2023. Ferret: A framework for benchmarking explainers on transformers. In Danilo Croce and Luca Soldaini, editors, Proceedings of the 17th conference of the european chapter of the association for computational linguistics: System demonstrations, pages 256–266, Dubrovnik, Croatia. Association for Computational Linguistics.\n\n\nWilker Aziz, Sheila Castilho, and Lucia Specia. 2012. PET: A tool for post-editing and assessing machine translation. In Nicoletta Calzolari, Khalid Choukri, Thierry Declerck, Mehmet Uğur Doğan, Bente Maegaard, Joseph Mariani, Asuncion Moreno, Jan Odijk, and Stelios Piperidis, editors, Proceedings of the eighth international conference on language resources and evaluation (LREC‘12), pages 3982–3987, Istanbul, Turkey. European Language Resources Association (ELRA).\n\n\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. 2016. Layer normalization. Arxiv Preprint.\n\n\nAlexander AND Montavon Bach Sebastian AND Binder. 2015. On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation. PLOS ONE, 10(7):1–46.\n\n\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to align and translate. In Yoshua Bengio and Yann LeCun, editors, Proceedings of the 3rd international conference on learning representations (ICLR), San Diego, CA, USA.\n\n\nSatanjeev Banerjee and Alon Lavie. 2005. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Jade Goldstein, Alon Lavie, Chin-Yew Lin, and Clare Voss, editors, Proceedings of the ACL workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization, pages 65–72, Ann Arbor, Michigan. Association for Computational Linguistics.\n\n\nJasmijn Bastings, Sebastian Ebert, Polina Zablotskaia, Anders Sandholm, and Katja Filippova. 2022. “Will you find these shortcuts?” A protocol for evaluating the faithfulness of input salience methods for text classification. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Proceedings of the 2022 conference on empirical methods in natural language processing, pages 976–991, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\n\n\nJasmijn Bastings and Katja Filippova. 2020. The elephant in the interpretability room: Why use attention as explanation when we have saliency methods? In Afra Alishahi, Yonatan Belinkov, Grzegorz Chrupała, Dieuwke Hupkes, Yuval Pinter, and Hassan Sajjad, editors, Proceedings of the third BlackboxNLP workshop on analyzing and interpreting neural networks for NLP, pages 149–155, Online. Association for Computational Linguistics.\n\n\nRachel Bawden, Rico Sennrich, Alexandra Birch, and Barry Haddow. 2018. Evaluating discourse phenomena in neural machine translation. In Marilyn Walker, Heng Ji, and Amanda Stent, editors, Proceedings of the 2018 conference of the north American chapter of the association for computational linguistics: Human language technologies, volume 1 (long papers), pages 1304–1313, New Orleans, Louisiana. Association for Computational Linguistics.\n\n\nYonatan Belinkov. 2022. Probing classifiers: Promises, shortcomings, and advances. Computational Linguistics, 48(1):207–219.\n\n\nLuisa Bentivogli, Arianna Bisazza, Mauro Cettolo, and Marcello Federico. 2016. Neural versus phrase-based machine translation quality: A case study. In Jian Su, Kevin Duh, and Xavier Carreras, editors, Proceedings of the 2016 conference on empirical methods in natural language processing, pages 257–267, Austin, Texas. Association for Computational Linguistics.\n\n\nBlair Bilodeau, Natasha Jaques, Pang Wei Koh, and Been Kim. 2024. Impossibility theorems for feature attribution. Proceedings of the National Academy of Sciences, 121(2):e2304406120.\n\n\nFrederic Blain, Chrysoula Zerva, Ricardo Rei, Nuno M. Guerreiro, Diptesh Kanojia, José G. C. de Souza, Beatriz Silva, Tânia Vaz, Yan Jingxuan, Fatemeh Azadi, Constantin Orasan, and André Martins. 2023. Findings of the WMT 2023 shared task on quality estimation. In Philipp Koehn, Barry Haddow, Tom Kocmi, and Christof Monz, editors, Proceedings of the eighth conference on machine translation, pages 629–653, Singapore. Association for Computational Linguistics.\n\n\nJohn Blatz, Erin Fitzgerald, George Foster, Simona Gandrabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis, and Nicola Ueffing. 2004. Confidence estimation for machine translation. In COLING 2004: Proceedings of the 20th international conference on computational linguistics, pages 315–321, Geneva, Switzerland. COLING.\n\n\nOndřej Bojar, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry Haddow, Shujian Huang, Matthias Huck, Philipp Koehn, Qun Liu, Varvara Logacheva, Christof Monz, Matteo Negri, Matt Post, Raphael Rubino, Lucia Specia, and Marco Turchi. 2017. Findings of the 2017 conference on machine translation (WMT17). In Ondřej Bojar, Christian Buck, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes, Philipp Koehn, and Julia Kreutzer, editors, Proceedings of the second conference on machine translation, pages 169–214, Copenhagen, Denmark. Association for Computational Linguistics.\n\n\nLynne Bowker. 2002. Computer-aided translation technology: A practical introduction. University of Ottawa Press.\n\n\nEleftheria Briakou, Jiaming Luo, Colin Cherry, and Markus Freitag. 2024. Translating step-by-step: Decomposing the translation process for improved translation quality of long-form texts. In Barry Haddow, Tom Kocmi, Philipp Koehn, and Christof Monz, editors, Proceedings of the ninth conference on machine translation, pages 1301–1317, Miami, Florida, USA. Association for Computational Linguistics.\n\n\nTrenton Bricken, Adly Templeton, Joshua Batson, Brian Chen, Adam Jermyn, Tom Conerly, Nick Turner, Cem Anil, Carson Denison, Amanda Askell, Robert Lasenby, Yifan Wu, Shauna Kravec, Nicholas Schiefer, Tim Maxwell, Nicholas Joseph, Zac Hatfield-Dodds, Alex Tamkin, Karina Nguyen, et al. 2023. Towards monosemanticity: Decomposing language models with dictionary learning. Transformer Circuits Thread.\n\n\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, et al. 2020. Language models are few-shot learners. In Proceedings of the 34th international conference on neural information processing systems, Red Hook, NY, USA. Curran Associates Inc.\n\n\nEmanuele Bugliarello, Sabrina J. Mielke, Antonios Anastasopoulos, Ryan Cotterell, and Naoaki Okazaki. 2020. It‘s easier to translate out of English than into it: Measuring neural translation difficulty by cross-mutual information. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault, editors, Proceedings of the 58th annual meeting of the association for computational linguistics, pages 1640–1649, Online. Association for Computational Linguistics.\n\n\nChris Callison-Burch, Cameron Fordyce, Philipp Koehn, Christof Monz, and Josh Schroeder. 2007. (Meta-) evaluation of machine translation. In Chris Callison-Burch, Philipp Koehn, Cameron Shaw Fordyce, and Christof Monz, editors, Proceedings of the second workshop on statistical machine translation, pages 136–158, Prague, Czech Republic. Association for Computational Linguistics.\n\n\nSheila Castilho, Joss Moorkens, Federico Gaspari, Iacer Calixto, John Tinsley, and Andy Way. 2017. Is neural machine translation the new state of the art? The Prague Bulletin of Mathematical Linguistics, 108(1):109–120.\n\n\nKenneth W. Church and Eduard H. Hovy. 1993. Good applications for crummy machine translation. Machine Translation, 8(4):239–258.\n\n\nKevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D. Manning. 2019. What does BERT look at? An analysis of BERT‘s attention. In Tal Linzen, Grzegorz Chrupała, Yonatan Belinkov, and Dieuwke Hupkes, editors, Proceedings of the 2019 ACL workshop BlackboxNLP: Analyzing and interpreting neural networks for NLP, pages 276–286, Florence, Italy. Association for Computational Linguistics.\n\n\nAlexis Conneau and Guillaume Lample. 2019. Cross-lingual language model pretraining. In H. Wallach, H. Larochelle, A. Beygelzimer, F. dAlché-Buc, E. Fox, and R. Garnett, editors, Advances in neural information processing systems, volume 32. Curran Associates, Inc.\n\n\nSven Coppers, Jan Van den Bergh, Kris Luyten, Karin Coninx, Iulianna Van der Lek-Ciudin, Tom Vanallemeersch, and Vincent Vandeghinste. 2018. Intellingo: An intelligible translation environment. In Proceedings of the 2018 CHI conference on human factors in computing systems, pages 1–13.\n\n\nIan Covert, Scott Lundberg, and Su-In Lee. 2021. Explaining by removing: A unified framework for model explanation. Journal of Machine Learning Research, 22(209):1–90.\n\n\nJonathan Crabbé and Mihaela van der Schaar. 2023. Evaluating the robustness of interpretability methods through explanation invariance and equivariance. In Thirty-seventh conference on neural information processing systems.\n\n\nMenglong Cui, Pengzhi Gao, Wei Liu, Jian Luan, and Bin Wang. 2025. Multilingual machine translation with open large language models at practical scale: An empirical study. In Luis Chiruzzo, Alan Ritter, and Lu Wang, editors, Proceedings of the 2025 conference of the nations of the americas chapter of the association for computational linguistics: Human language technologies (volume 1: Long papers), pages 5420–5443, Albuquerque, New Mexico. Association for Computational Linguistics.\n\n\nJoke Daems, Sonia Vandepitte, Robert J. Hartsuiker, and Lieve Macken. 2017. Identifying the machine translation error types with the greatest impact on post-editing effort. Frontiers in Psychology, 8.\n\n\nDavid Dale, Elena Voita, Loic Barrault, and Marta R. Costa-jussà. 2023. Detecting and mitigating hallucinations in machine translation: Model internal workings alone do well, sentence similarity Even better. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st annual meeting of the association for computational linguistics (volume 1: Long papers), pages 36–50, Toronto, Canada. Association for Computational Linguistics.\n\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Jill Burstein, Christy Doran, and Thamar Solorio, editors, Proceedings of the 2019 conference of the north American chapter of the association for computational linguistics: Human language technologies, volume 1 (long and short papers), pages 4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics.\n\n\nQingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Jingyuan Ma, Rui Li, Heming Xia, Jingjing Xu, Zhiyong Wu, Baobao Chang, Xu Sun, Lei Li, and Zhifang Sui. 2024. A survey on in-context learning. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Proceedings of the 2024 conference on empirical methods in natural language processing, pages 1107–1128, Miami, Florida, USA. Association for Computational Linguistics.\n\n\nDavid L. Donoho and Michael Elad. 2003. Optimally sparse representation in general (nonorthogonal) dictionaries via &#x2113;&lt;sup&gt;1&lt;/sup&gt; minimization. Proceedings of the National Academy of Sciences, 100(5):2197–2202.\n\n\nNelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan, Shauna Kravec, Zac Hatfield-Dodds, Robert Lasenby, Dawn Drain, Carol Chen, Roger Grosse, Sam McCandlish, Jared Kaplan, Dario Amodei, Martin Wattenberg, and Christopher Olah. 2022. Toy models of superposition. Transformer Circuits Thread.\n\n\nNelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, et al. 2021. A mathematical framework for transformer circuits. Transformer Circuits Thread. https://transformer-circuits.pub/2021/framework/index.html.\n\n\nEkaterina Fadeeva, Aleksandr Rubashevskii, Artem Shelmanov, Sergey Petrakov, Haonan Li, Hamdy Mubarak, Evgenii Tsymbalov, Gleb Kuzmin, Alexander Panchenko, Timothy Baldwin, Preslav Nakov, and Maxim Panov. 2024. Fact-checking the output of large language models via token-level uncertainty quantification. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Findings of the association for computational linguistics: ACL 2024, pages 9367–9385, Bangkok, Thailand. Association for Computational Linguistics.\n\n\nEkaterina Fadeeva, Roman Vashurin, Akim Tsvigun, Artem Vazhentsev, Sergey Petrakov, Kirill Fedyanin, Daniil Vasilev, Elizaveta Goncharova, Alexander Panchenko, Maxim Panov, Timothy Baldwin, and Artem Shelmanov. 2023. LM-polygraph: Uncertainty estimation for language models. In Yansong Feng and Els Lefever, editors, Proceedings of the 2023 conference on empirical methods in natural language processing: System demonstrations, pages 446–461, Singapore. Association for Computational Linguistics.\n\n\nAngela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep Baines, Onur Çelebi, Guillaume Wenzek, Vishrav Chaudhary, Naman Goyal, Tom Birch, Vitaliy Liptchinsky, Sergey Edunov, Edouard Grave, Michael Auli, and Armand Joulin. 2021. Beyond english-centric multilingual machine translation. Journal of Machine Learning Research, 22(107):1–48.\n\n\nMarcello Federico, Nicola Bertoldi, Marco Trombetti, and Alessandro Cattelan. 2014. MateCat: An open source CAT tool for MT post-editing. In Proceedings of the 11th conference of the association for machine translation in the americas: tutorials, Vancouver, Canada. Association for Machine Translation in the Americas.\n\n\nThomas Fel. 2024. Sparks of explainability: Recent advancements in explaining large vision models. PhD thesis, University of Toulouse.\n\n\nPatrick Fernandes, Daniel Deutsch, Mara Finkelstein, Parker Riley, André Martins, Graham Neubig, Ankush Garg, Jonathan Clark, Markus Freitag, and Orhan Firat. 2023a. The devil is in the errors: Leveraging large language models for fine-grained machine translation evaluation. In Philipp Koehn, Barry Haddow, Tom Kocmi, and Christof Monz, editors, Proceedings of the eighth conference on machine translation, pages 1066–1083, Singapore. Association for Computational Linguistics.\n\n\nPatrick Fernandes, Kayo Yin, Emmy Liu, André Martins, and Graham Neubig. 2023b. When does translation require context? A data-driven, multilingual exploration. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st annual meeting of the association for computational linguistics (volume 1: Long papers), pages 606–626, Toronto, Canada. Association for Computational Linguistics.\n\n\nJavier Ferrando, Gerard I. Gállego, and Marta R. Costa-jussà. 2022. Measuring the mixing of contextual information in the transformer. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Proceedings of the 2022 conference on empirical methods in natural language processing, pages 8698–8714, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\n\n\nJavier Ferrando, Gabriele Sarti, Arianna Bisazza, and Marta R. Costa-jussà. 2024. A primer on the inner workings of transformer-based language models. Arxiv Preprint.\n\n\nJaden Fried Fiotto-Kaufman, Alexander Russell Loftus, Eric Todd, Jannik Brinkmann, Koyena Pal, Dmitrii Troitskii, Michael Ripa, Adam Belfki, Can Rager, Caden Juang, Aaron Mueller, Samuel Marks, Arnab Sen Sharma, Francesca Lucchetti, Nikhil Prakash, Carla E. Brodley, Arjun Guha, Jonathan Bell, Byron C Wallace, et al. 2025. NNsight and NDIF: Democratizing access to open-weight foundation model internals. In The thirteenth international conference on learning representations.\n\n\nMarina Fomicheva, Piyawat Lertvittayakumjorn, Wei Zhao, Steffen Eger, and Yang Gao. 2021. The Eval4NLP shared task on explainable quality estimation: Overview and results. In Yang Gao, Steffen Eger, Wei Zhao, Piyawat Lertvittayakumjorn, and Marina Fomicheva, editors, Proceedings of the 2nd workshop on evaluation and comparison of NLP systems, pages 165–178, Punta Cana, Dominican Republic. Association for Computational Linguistics.\n\n\nMarina Fomicheva, Shuo Sun, Lisa Yankovskaya, Frédéric Blain, Francisco Guzmán, Mark Fishel, Nikolaos Aletras, Vishrav Chaudhary, and Lucia Specia. 2020. Unsupervised quality estimation for neural machine translation. Transactions of the Association for Computational Linguistics, 8:539–555.\n\n\nMarkus Freitag, George Foster, David Grangier, Viresh Ratnakar, Qijun Tan, and Wolfgang Macherey. 2021. Experts, errors, and context: A large-scale study of human evaluation for machine translation. Transactions of the Association for Computational Linguistics, 9:1460–1474.\n\n\nMarkus Freitag, Nitika Mathur, Chi-kiu Lo, Eleftherios Avramidis, Ricardo Rei, Brian Thompson, Tom Kocmi, Frederic Blain, Daniel Deutsch, Craig Stewart, Chrysoula Zerva, Sheila Castilho, Alon Lavie, and George Foster. 2023. Results of WMT23 metrics shared task: Metrics might be guilty but references are not innocent. In Philipp Koehn, Barry Haddow, Tom Kocmi, and Christof Monz, editors, Proceedings of the eighth conference on machine translation, pages 578–628, Singapore. Association for Computational Linguistics.\n\n\nMarkus Freitag, Ricardo Rei, Nitika Mathur, Chi-kiu Lo, Craig Stewart, Eleftherios Avramidis, Tom Kocmi, George Foster, Alon Lavie, and André F. T. Martins. 2022. Results of WMT22 metrics shared task: Stop using BLEU – neural metrics are better and more robust. In Philipp Koehn, Loïc Barrault, Ondřej Bojar, Fethi Bougares, Rajen Chatterjee, Marta R. Costa-jussà, Christian Federmann, Mark Fishel, Alexander Fraser, Markus Freitag, Yvette Graham, Roman Grundkiewicz, Paco Guzman, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes, Tom Kocmi, André Martins, Makoto Morishita, et al., editors, Proceedings of the seventh conference on machine translation (WMT), pages 46–68, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics.\n\n\nYarin Gal and Zoubin Ghahramani. 2016. Dropout as a bayesian approximation: Representing model uncertainty in deep learning. In Maria Florina Balcan and Kilian Q. Weinberger, editors, Proceedings of the 33rd international conference on machine learning, volume 48, pages 1050–1059, New York, NY, USA. Proceedings of Machine Learning Research (PLMR).\n\n\nIgnacio Garcia. 2009. Beyond translation memory: Computers and the professional translator. The Journal of Specialised Translation.\n\n\nMor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. 2021. Transformer feed-forward layers are key-value memories. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Proceedings of the 2021 conference on empirical methods in natural language processing, pages 5484–5495, Online; Punta Cana, Dominican Republic. Association for Computational Linguistics.\n\n\nIan Goodfellow, Yoshua Bengio, and Aaron Courville. 2016. Deep learning. MIT Press.\n\n\nNaman Goyal, Jingfei Du, Myle Ott, Giri Anantharaman, and Alexis Conneau. 2021. Larger-scale transformers for multilingual masked language modeling. In Anna Rogers, Iacer Calixto, Ivan Vulić, Naomi Saphra, Nora Kassner, Oana-Maria Camburu, Trapit Bansal, and Vered Shwartz, editors, Proceedings of the 6th workshop on representation learning for NLP (RepL4NLP-2021), pages 29–33, Online. Association for Computational Linguistics.\n\n\nYvette Graham, Timothy Baldwin, Alistair Moffat, and Justin Zobel. 2013. Continuous measurement scales in human evaluation of machine translation. In Antonio Pareja-Lora, Maria Liakata, and Stefanie Dipper, editors, Proceedings of the 7th linguistic annotation workshop and interoperability with discourse, pages 33–41, Sofia, Bulgaria. Association for Computational Linguistics.\n\n\nSpence Green, Jeffrey Heer, and Christopher D. Manning. 2013. The efficacy of human post-editing for language translation. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, pages 439–448, New York, NY, USA. Association for Computing Machinery.\n\n\nAna Guerberof. 2009. Productivity and quality in MT post-editing. In Beyond translation memories: New tools for translators workshop, Ottawa, Canada.\n\n\nAna Guerberof-Arenas and Antonio Toral. 2022. Creativity in translation: Machine translation as a constraint for literary texts. Translation Spaces, 11(2):184–212.\n\n\nNuno M. Guerreiro, Ricardo Rei, Daan van Stigt, Luisa Coheur, Pierre Colombo, and André F. T. Martins. 2024. Xcomet: Transparent machine translation evaluation through fine-grained error detection. Transactions of the Association for Computational Linguistics, 12:979–995.\n\n\nAbhijeet Gupta, Gemma Boleda, Marco Baroni, and Sebastian Padó. 2015. Distributional vectors encode referential attributes. In Lluís Màrquez, Chris Callison-Burch, and Jian Su, editors, Proceedings of the 2015 conference on empirical methods in natural language processing, pages 12–21, Lisbon, Portugal. Association for Computational Linguistics.\n\n\nChristian Hadiwinoto. 2017. Book review: Syntax-based statistical machine translation by philip Williams, rico Sennrich, matt post and philipp Koehn. Computational Linguistics, 43(4):893–896.\n\n\nZellig S. Harris. 1954. Distributional structure. Word, 10(2-3):146–162.\n\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In 2016 IEEE conference on computer vision and pattern recognition (CVPR), pages 770–778, Los Alamitos, CA, USA. IEEE Computer Society.\n\n\nNico Herbig, Tim Düwel, Santanu Pal, Kalliopi Meladaki, Mahsa Monshizadeh, Antonio Krüger, and Josef van Genabith. 2020. MMPE: A Multi-Modal Interface for Post-Editing Machine Translation. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault, editors, Proceedings of the 58th annual meeting of the association for computational linguistics, pages 1691–1702, Online. Association for Computational Linguistics.\n\n\nAnas Himmi, Guillaume Staerman, Marine Picot, Pierre Colombo, and Nuno M Guerreiro. 2024. Enhanced hallucination detection in neural machine translation through simple detector aggregation. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Proceedings of the 2024 conference on empirical methods in natural language processing, pages 18573–18583, Miami, Florida, USA. Association for Computational Linguistics.\n\n\nSepp Hochreiter. 1998. The vanishing gradient problem during learning recurrent neural nets and problem solutions. Int. J. Uncertain. Fuzziness Knowl.-Based Syst., 6(2):107–116.\n\n\nSepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory. Neural computation, 9(8):1735–1780.\n\n\nAri Holtzman, Peter West, Vered Shwartz, Yejin Choi, and Luke Zettlemoyer. 2021. Surface form competition: Why the highest probability answer isn‘t always right. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Proceedings of the 2021 conference on empirical methods in natural language processing, pages 7038–7051, Online; Punta Cana, Dominican Republic. Association for Computational Linguistics.\n\n\nJeremy Howard and Sebastian Ruder. 2018. Universal language model fine-tuning for text classification. In Iryna Gurevych and Yusuke Miyao, editors, Proceedings of the 56th annual meeting of the association for computational linguistics (volume 1: Long papers), pages 328–339, Melbourne, Australia. Association for Computational Linguistics.\n\n\nRobert Huben, Hoagy Cunningham, Logan Riggs Smith, Aidan Ewart, and Lee Sharkey. 2024. Sparse autoencoders find highly interpretable features in language models. In The twelfth international conference on learning representations.\n\n\nWilliam J. Hutchins. 2001. Machine translation over fifty years. Histoire Épistémologie Langage, 23:7–31.\n\n\nAlon Jacovi and Yoav Goldberg. 2020. Towards faithfully interpretable NLP systems: How should we define and evaluate faithfulness? In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault, editors, Proceedings of the 58th annual meeting of the association for computational linguistics, pages 4198–4205, Online. Association for Computational Linguistics.\n\n\nSarthak Jain and Byron C. Wallace. 2019. Attention is not Explanation. In Jill Burstein, Christy Doran, and Thamar Solorio, editors, Proceedings of the 2019 conference of the north American chapter of the association for computational linguistics: Human language technologies, volume 1 (long and short papers), pages 3543–3556, Minneapolis, Minnesota. Association for Computational Linguistics.\n\n\nStanisław Jastrzebski, Devansh Arpit, Nicolas Ballas, Vikas Verma, Tong Che, and Yoshua Bengio. 2018. Residual connections encourage iterative inference. In International conference on learning representations.\n\n\nNal Kalchbrenner and Phil Blunsom. 2013. Recurrent continuous translation models. In David Yarowsky, Timothy Baldwin, Anna Korhonen, Karen Livescu, and Steven Bethard, editors, Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1700–1709, Seattle, Washington, USA. Association for Computational Linguistics.\n\n\nSariya Karimova, Patrick Simianer, and Stefan Riezler. 2018. A user-study on online adaptation of neural machine translation to human post-edits. Machine Translation, 32(4):309–324.\n\n\nFabio Kepler, Jonay Trénous, Marcos Treviso, Miguel Vera, and André F. T. Martins. 2019. OpenKiwi: An open source framework for quality estimation. In Marta R. Costa-jussà and Enrique Alfonseca, editors, Proceedings of the 57th annual meeting of the association for computational linguistics: System demonstrations, pages 117–122, Florence, Italy. Association for Computational Linguistics.\n\n\nYunsu Kim, Petre Petrov, Pavel Petrushkov, Shahram Khadivi, and Hermann Ney. 2019. Pivot-based transfer learning for neural machine translation between non-English languages. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors, Proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing (EMNLP-IJCNLP), pages 866–876, Hong Kong, China. Association for Computational Linguistics.\n\n\nGoro Kobayashi, Tatsuki Kuribayashi, Sho Yokoi, and Kentaro Inui. 2020. Attention is not only a weight: Analyzing transformers with vector norms. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, Proceedings of the 2020 conference on empirical methods in natural language processing (EMNLP), pages 7057–7075, Online. Association for Computational Linguistics.\n\n\nGoro Kobayashi, Tatsuki Kuribayashi, Sho Yokoi, and Kentaro Inui. 2021. Incorporating Residual and Normalization Layers into Analysis of Masked Language Models. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Proceedings of the 2021 conference on empirical methods in natural language processing, pages 4547–4568, Online; Punta Cana, Dominican Republic. Association for Computational Linguistics.\n\n\nTom Kocmi, Eleftherios Avramidis, Rachel Bawden, Ondřej Bojar, Anton Dvorkovich, Christian Federmann, Mark Fishel, Markus Freitag, Thamme Gowda, Roman Grundkiewicz, Barry Haddow, Marzena Karpinska, Philipp Koehn, Benjamin Marie, Christof Monz, Kenton Murray, Masaaki Nagata, Martin Popel, Maja Popović, et al. 2024a. Findings of the WMT24 general machine translation shared task: The LLM era is here but MT is not solved yet. In Barry Haddow, Tom Kocmi, Philipp Koehn, and Christof Monz, editors, Proceedings of the ninth conference on machine translation, pages 1–46, Miami, Florida, USA. Association for Computational Linguistics.\n\n\nTom Kocmi and Christian Federmann. 2023. GEMBA-MQM: Detecting translation quality error spans with GPT-4. In Philipp Koehn, Barry Haddow, Tom Kocmi, and Christof Monz, editors, Proceedings of the eighth conference on machine translation, pages 768–775, Singapore. Association for Computational Linguistics.\n\n\nTom Kocmi, Vilém Zouhar, Eleftherios Avramidis, Roman Grundkiewicz, Marzena Karpinska, Maja Popović, Mrinmaya Sachan, and Mariya Shmatova. 2024b. Error span annotation: A balanced approach for human evaluation of machine translation. In Barry Haddow, Tom Kocmi, Philipp Koehn, and Christof Monz, editors, Proceedings of the ninth conference on machine translation, pages 1440–1453, Miami, Florida, USA. Association for Computational Linguistics.\n\n\nPhilipp Koehn, Franz J. Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the 2003 human language technology conference of the north American chapter of the association for computational linguistics, pages 127–133.\n\n\nArne Köhn. 2015. What‘s in an embedding? Analyzing word embeddings through multilingual evaluation. In Lluís Màrquez, Chris Callison-Burch, and Jian Su, editors, Proceedings of the 2015 conference on empirical methods in natural language processing, pages 2067–2073, Lisbon, Portugal. Association for Computational Linguistics.\n\n\nNarine Kokhlikyan, Vivek Miglani, Miguel Martin, Edward Wang, Bilal Alsallakh, Jonathan Reynolds, Alexander Melnikov, Natalia Kliushkina, Carlos Araya, Siqi Yan, and Orion Reblitz-Richardson. 2020. Captum: A unified and generic model interpretability library for PyTorch. ArXiv.\n\n\nHans P. Krings. 2001. Repairing texts: Empirical investigations of machine translation post-editing processes. Kent State University Press.\n\n\nSatyapriya Krishna, Tessa Han, Alex Gu, Steven Wu, Shahin Jabbari, and Himabindu Lakkaraju. 2024. The disagreement problem in explainable machine learning: A practitioner’s perspective. Transactions on Machine Learning Research.\n\n\nIsaac Lage, Emily Chen, Jeffrey He, Menaka Narayanan, Been Kim, Sam Gershman, and Finale Doshi-Velez. 2019. An evaluation of the human-interpretability of explanation. ArXiv, abs/1902.00006.\n\n\nSamuel Läubli, Chantal Amrhein, Patrick Düggelin, Beatriz Gonzalez, Alena Zwahlen, and Martin Volk. 2019. Post-editing productivity with neural machine translation: An empirical assessment of speed and quality in the banking and finance domain. In Mikel Forcada, Andy Way, Barry Haddow, and Rico Sennrich, editors, Proceedings of machine translation summit XVII: Research track, pages 267–272, Dublin, Ireland. European Association for Machine Translation.\n\n\nSamuel Läubli, Mark Fishel, Gary Massey, Maureen Ehrensberger-Dow, and Martin Volk. 2013. Assessing post-editing efficiency in a realistic translation environment. In Sharon O’Brien, Michel Simard, and Lucia Specia, editors, Proceedings of the 2nd workshop on post-editing technology and practice, Nice, France.\n\n\nSamuel Läubli, Rico Sennrich, and Martin Volk. 2018. Has machine translation achieved human parity? A case for document-level evaluation. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun’ichi Tsujii, editors, Proceedings of the 2018 conference on empirical methods in natural language processing, pages 4791–4796, Brussels, Belgium. Association for Computational Linguistics.\n\n\nSeungjun Lee, Jungseob Lee, Hyeonseok Moon, Chanjun Park, Jaehyung Seo, Sugyeong Eo, Seonmin Koo, and Heuiseok Lim. 2023. A survey on evaluation metrics for machine translation. Mathematics, 11(4).\n\n\nChristoph Leiter, Piyawat Lertvittayakumjorn, Marina Fomicheva, Wei Zhao, Yang Gao, and Steffen Eger. 2024. Towards explainable evaluation metrics for machine translation. Journal of Machine Learning Research, 25(75):1–49.\n\n\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-augmented generation for knowledge-intensive NLP tasks. In Proceedings of the 34th international conference on neural information processing systems, Red Hook, NY, USA. Curran Associates Inc.\n\n\nJiwei Li, Xinlei Chen, Eduard Hovy, and Dan Jurafsky. 2016. Visualizing and understanding neural models in NLP. In Kevin Knight, Ani Nenkova, and Owen Rambow, editors, Proceedings of the 2016 conference of the north American chapter of the association for computational linguistics: Human language technologies, pages 681–691, San Diego, California. Association for Computational Linguistics.\n\n\nZhongtao Liu, Parker Riley, Daniel Deutsch, Alison Lui, Mengmeng Niu, Apurva Shah, and Markus Freitag. 2024. Beyond human-only: Evaluating human-machine collaboration for collecting high-quality translation data. In Barry Haddow, Tom Kocmi, Philipp Koehn, and Christof Monz, editors, Proceedings of the ninth conference on machine translation, pages 1095–1106, Miami, Florida, USA. Association for Computational Linguistics.\n\n\nArle Richard Lommel, Aljoscha Burchardt, and Hans Uszkoreit. 2013. Multidimensional quality metrics: A flexible system for assessing translation quality. In Proceedings of translating and the computer 35, London, UK. Aslib.\n\n\nArle Lommel, Serge Gladkoff, Alan Melby, Sue Ellen Wright, Ingemar Strandvik, Katerina Gasova, Angelika Vaasa, Andy Benzo, Romina Marazzato Sparano, Monica Foresi, Johani Innis, Lifeng Han, and Goran Nenadic. 2024. The multi-range theory of translation quality measurement: MQM scoring models and statistical quality control. In Marianna Martindale, Janice Campbell, Konstantin Savenkov, and Shivali Goel, editors, Proceedings of the 16th conference of the association for machine translation in the americas (volume 2: presentations), pages 75–94, Chicago, USA. Association for Machine Translation in the Americas.\n\n\nYao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2022. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Proceedings of the 60th annual meeting of the association for computational linguistics (volume 1: Long papers), pages 8086–8098, Dublin, Ireland. Association for Computational Linguistics.\n\n\nScott M. Lundberg and Su-In Lee. 2017. A unified approach to interpreting model predictions. In Proceedings of the 31st international conference on neural information processing systems, volume 30, pages 4768–4777, Long Beach, California, USA. Curran Associates Inc.\n\n\nAndreas Madsen, Siva Reddy, and Sarath Chandar. 2022. Post-hoc interpretability for neural NLP: A survey. ACM Comput. Surv., 55(8).\n\n\nSamuel Marks and Max Tegmark. 2024. The geometry of truth: Emergent linear structure in large language model representations of true/false datasets. In Proceedings of the 1st conference on language modeling (COLM).\n\n\nMarianna Martindale and Marine Carpuat. 2018. Fluency over adequacy: A pilot study in measuring user trust in imperfect MT. In Colin Cherry and Graham Neubig, editors, Proceedings of the 13th conference of the association for machine translation in the Americas (volume 1: Research track), pages 13–25, Boston, MA. Association for Machine Translation in the Americas.\n\n\nSameen Maruf and Gholamreza Haffari. 2018. Document context neural machine translation with memory networks. In Iryna Gurevych and Yusuke Miyao, editors, Proceedings of the 56th annual meeting of the association for computational linguistics (volume 1: Long papers), pages 1275–1284, Melbourne, Australia. Association for Computational Linguistics.\n\n\nR. Thomas McCoy, Ellie Pavlick, and Tal Linzen. 2019. Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference. In Anna Korhonen, David Traum, and Lluís Màrquez, editors, Proceedings of the 57th annual meeting of the association for computational linguistics, pages 3428–3448, Florence, Italy. Association for Computational Linguistics.\n\n\nNikita Mehandru, Sweta Agrawal, Yimin Xiao, Ge Gao, Elaine Khoong, Marine Carpuat, and Niloufar Salehi. 2023. Physician detection of clinical harm in machine translation: Quality estimation aids in reliance and backtranslation identifies critical errors. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 conference on empirical methods in natural language processing, pages 11633–11647, Singapore. Association for Computational Linguistics.\n\n\nLesly Miculicich, Dhananjay Ram, Nikolaos Pappas, and James Henderson. 2018. Document-level neural machine translation with hierarchical attention networks. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun’ichi Tsujii, editors, Proceedings of the 2018 conference on empirical methods in natural language processing, pages 2947–2954, Brussels, Belgium. Association for Computational Linguistics.\n\n\nVivek Miglani, Aobo Yang, Aram Markosyan, Diego Garcia-Olano, and Narine Kokhlikyan. 2023. Using captum to explain generative language models. In Liling Tan, Dmitrijs Milajevs, Geeticka Chauhan, Jeremy Gwinnup, and Elijah Rippeth, editors, Proceedings of the 3rd workshop for natural language processing open source software (NLP-OSS 2023), pages 165–173, Singapore. Association for Computational Linguistics.\n\n\nTomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013. Linguistic regularities in continuous space word representations. In Lucy Vanderwende, Hal Daumé III, and Katrin Kirchhoff, editors, Proceedings of the 2013 conference of the north American chapter of the association for computational linguistics: Human language technologies, pages 746–751, Atlanta, Georgia. Association for Computational Linguistics.\n\n\nHosein Mohebbi, Willem Zuidema, Grzegorz Chrupała, and Afra Alishahi. 2023. Quantifying context mixing in transformers. In Andreas Vlachos and Isabelle Augenstein, editors, Proceedings of the 17th conference of the european chapter of the association for computational linguistics, pages 3378–3400, Dubrovnik, Croatia. Association for Computational Linguistics.\n\n\nJohn Moran, Christian Saam, and Dave Lewis. 2014. Towards desktop-based CAT tool instrumentation. In Sharon O’Brien, Michel Simard, and Lucia Specia, editors, Proceedings of the 11th conference of the association for machine translation in the americas, pages 99–112, Vancouver, Canada. Association for Machine Translation in the Americas.\n\n\nMathias Müller, Annette Rios, Elena Voita, and Rico Sennrich. 2018. A large-scale test set for the evaluation of context-aware pronoun translation in neural machine translation. In Ondřej Bojar, Rajen Chatterjee, Christian Federmann, Mark Fishel, Yvette Graham, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes, Philipp Koehn, Christof Monz, Matteo Negri, Aurélie Névéol, Mariana Neves, Matt Post, Lucia Specia, Marco Turchi, and Karin Verspoor, editors, Proceedings of the third conference on machine translation: Research papers, pages 61–72, Brussels, Belgium. Association for Computational Linguistics.\n\n\nNLLB Team, Marta R. Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula, Loic Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti, et al. 2024. Scaling neural machine translation to 200 languages. Nature, 630(8018):841–846.\n\n\nnostalgebraist. 2020. Interpreting GPT: The logit lens. AI Alignment Forum.\n\n\nFranz Josef Och, Christoph Tillmann, and Hermann Ney. 1999. Improved alignment models for statistical machine translation. In 1999 joint SIGDAT conference on empirical methods in natural language processing and very large corpora.\n\n\nChris Olah. 2023. Distributed representations: Composition & superposition. Transformer Circuits Thread.\n\n\nBruno A. Olshausen and David J. Field. 1997. Sparse coding with an overcomplete basis set: A strategy employed by V1? Vision Research, 37(23):3311–3325.\n\n\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: A method for automatic evaluation of machine translation. In Pierre Isabelle, Eugene Charniak, and Dekang Lin, editors, Proceedings of the 40th annual meeting of the association for computational linguistics, pages 311–318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.\n\n\nKiho Park, Yo Joong Choe, and Victor Veitch. 2023. The linear representation hypothesis and the geometry of large language models. In Causal representation learning workshop at NeurIPS 2023.\n\n\nCarla Parra Escartín and Manuel Arcedillo. 2015. Machine translation evaluation made fuzzier: A study on post-editing productivity and evaluation metrics in commercial settings. In Proceedings of machine translation summit XV: papers, Miami, USA.\n\n\nJeffrey Pennington, Richard Socher, and Christopher Manning. 2014. GloVe: Global vectors for word representation. In Alessandro Moschitti, Bo Pang, and Walter Daelemans, editors, Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1532–1543, Doha, Qatar. Association for Computational Linguistics.\n\n\nFabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim Rocktäschel, Yuxiang Wu, Alexander H. Miller, and Sebastian Riedel. 2020. How context affects language models’ factual predictions. In Automated knowledge base construction.\n\n\nCharles Pierse. 2021. Transformers interpret.\n\n\nMirko Plitt and François Masselot. 2010. A Productivity Test of Statistical Machine Translation Post-Editing in a Typical Localisation Context. The Prague Bulletin of Mathematical Linguistics, 93(1).\n\n\nMaja Popović. 2015. ChrF: Character n-gram F-score for automatic MT evaluation. In Ondřej Bojar, Rajan Chatterjee, Christian Federmann, Barry Haddow, Chris Hokamp, Matthias Huck, Varvara Logacheva, and Pavel Pecina, editors, Proceedings of the tenth workshop on statistical machine translation, pages 392–395, Lisbon, Portugal. Association for Computational Linguistics.\n\n\nMaja Popović. 2020. Informative manual evaluation of machine translation output. In Donia Scott, Nuria Bel, and Chengqing Zong, editors, Proceedings of the 28th international conference on computational linguistics, pages 5059–5069, Barcelona, Spain (Online). International Committee on Computational Linguistics.\n\n\nRicardo Rei, José G. C. de Souza, Duarte Alves, Chrysoula Zerva, Ana C Farinha, Taisiya Glushkova, Alon Lavie, Luisa Coheur, and André F. T. Martins. 2022a. COMET-22: Unbabel-IST 2022 submission for the metrics shared task. In Philipp Koehn, Loïc Barrault, Ondřej Bojar, Fethi Bougares, Rajen Chatterjee, Marta R. Costa-jussà, Christian Federmann, Mark Fishel, Alexander Fraser, Markus Freitag, Yvette Graham, Roman Grundkiewicz, Paco Guzman, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes, Tom Kocmi, André Martins, Makoto Morishita, et al., editors, Proceedings of the seventh conference on machine translation (WMT), pages 578–585, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics.\n\n\nRicardo Rei, Ana C Farinha, José G. C. de Souza, Pedro G. Ramos, André F. T. Martins, Luisa Coheur, and Alon Lavie. 2022b. Searching for COMETINHO: The little metric that could. In Helena Moniz, Lieve Macken, Andrew Rufener, Loïc Barrault, Marta R. Costa-jussà, Christophe Declercq, Maarit Koponen, Ellie Kemp, Spyridon Pilos, Mikel L. Forcada, Carolina Scarton, Joachim Van den Bogaert, Joke Daems, Arda Tezcan, Bram Vanroy, and Margot Fonteyne, editors, Proceedings of the 23rd annual conference of the european association for machine translation, pages 61–70, Ghent, Belgium. European Association for Machine Translation.\n\n\nRicardo Rei, Ana C Farinha, Chrysoula Zerva, Daan van Stigt, Craig Stewart, Pedro Ramos, Taisiya Glushkova, André F. T. Martins, and Alon Lavie. 2021. Are references really needed? Unbabel-IST 2021 submission for the metrics shared task. In Loic Barrault, Ondrej Bojar, Fethi Bougares, Rajen Chatterjee, Marta R. Costa-jussa, Christian Federmann, Mark Fishel, Alexander Fraser, Markus Freitag, Yvette Graham, Roman Grundkiewicz, Paco Guzman, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes, Philipp Koehn, Tom Kocmi, Andre Martins, Makoto Morishita, et al., editors, Proceedings of the sixth conference on machine translation, pages 1030–1040, Online. Association for Computational Linguistics.\n\n\nRicardo Rei, Nuno M. Guerreiro, Marcos Treviso, Luisa Coheur, Alon Lavie, and André Martins. 2023. The inside story: Towards better understanding of machine translation neural evaluation metrics. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st annual meeting of the association for computational linguistics (volume 2: Short papers), pages 1089–1105, Toronto, Canada. Association for Computational Linguistics.\n\n\nRicardo Rei, Jose Pombal, Nuno M. Guerreiro, João Alves, Pedro Henrique Martins, Patrick Fernandes, Helena Wu, Tania Vaz, Duarte Alves, Amin Farajian, Sweta Agrawal, Antonio Farinhas, José G. C. De Souza, and André Martins. 2024. Tower v2: Unbabel-IST 2024 submission for the general MT shared task. In Barry Haddow, Tom Kocmi, Philipp Koehn, and Christof Monz, editors, Proceedings of the ninth conference on machine translation, pages 185–204, Miami, Florida, USA. Association for Computational Linguistics.\n\n\nRicardo Rei, Craig Stewart, Ana C Farinha, and Alon Lavie. 2020. COMET: A neural framework for MT evaluation. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, Proceedings of the 2020 conference on empirical methods in natural language processing (EMNLP), pages 2685–2702, Online. Association for Computational Linguistics.\n\n\nMarco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. \"Why should i trust you?\": Explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, pages 1135–1144, New York, NY, USA. Association for Computing Machinery.\n\n\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky. 2020. A primer in BERTology: What we know about how BERT works. Transactions of the Association for Computational Linguistics, 8:842–866.\n\n\nRaphael Rubino, Atsushi Fujita, and Benjamin Marie. 2021. Error identification for machine translation with metric embedding and attention. In Yang Gao, Steffen Eger, Wei Zhao, Piyawat Lertvittayakumjorn, and Marina Fomicheva, editors, Proceedings of the 2nd workshop on evaluation and comparison of NLP systems, pages 146–156, Punta Cana, Dominican Republic. Association for Computational Linguistics.\n\n\nDavid E. Rumelhart and James L. McClelland. 1987. Learning internal representations by error propagation. In Parallel distributed processing: Explorations in the microstructure of cognition: foundations, pages 318–362. MIT Press.\n\n\nVictor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, et al. 2022. Multitask prompted training enables zero-shot task generalization. In Proceedings of the tenth international conference on learning representations (ICLR).\n\n\nGabriele Sarti, Grzegorz Chrupała, Malvina Nissim, and Arianna Bisazza. 2024. Quantifying the plausibility of context reliance in neural machine translation. In The twelfth international conference on learning representations (ICLR 2024), Vienna, Austria. OpenReview.\n\n\nBeatrice Savoldi, Alan Ramponi, Matteo Negri, and Luisa Bentivogli. 2025. Translation in the hands of many: Centering lay users in machine translation interactions.\n\n\nDaniel Scalena, Gabriele Sarti, and Malvina Nissim. 2024. Multi-property steering of large language models with dynamic activation composition. In Yonatan Belinkov, Najoung Kim, Jaap Jumelet, Hosein Mohebbi, Aaron Mueller, and Hanjie Chen, editors, Proceedings of the 7th BlackboxNLP workshop: Analyzing and interpreting neural networks for NLP, pages 577–603, Miami, Florida, US. Association for Computational Linguistics.\n\n\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms.\n\n\nHolger Schwenk, Guillaume Wenzek, Sergey Edunov, Edouard Grave, Armand Joulin, and Angela Fan. 2021. CCMatrix: Mining billions of high-quality parallel sentences on the web. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, Proceedings of the 59th annual meeting of the association for computational linguistics and the 11th international joint conference on natural language processing (volume 1: Long papers), pages 6490–6500, Online. Association for Computational Linguistics.\n\n\nThibault Sellam, Dipanjan Das, and Ankur Parikh. 2020. BLEURT: Learning robust metrics for text generation. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault, editors, Proceedings of the 58th annual meeting of the association for computational linguistics, pages 7881–7892, Online. Association for Computational Linguistics.\n\n\nRico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural machine translation of rare words with subword units. In Katrin Erk and Noah A. Smith, editors, Proceedings of the 54th annual meeting of the association for computational linguistics (volume 1: Long papers), pages 1715–1725, Berlin, Germany. Association for Computational Linguistics.\n\n\nRaksha Shenoy, Nico Herbig, Antonio Krüger, and Josef van Genabith. 2021. Investigating the helpfulness of word-level quality estimation for post-editing machine translation output. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Proceedings of the 2021 conference on empirical methods in natural language processing, pages 10173–10185, Online; Punta Cana, Dominican Republic. Association for Computational Linguistics.\n\n\nKaren Simonyan, Andrea Vedaldi, and Andrew Zisserman. 2014. Deep inside convolutional networks: Visualising image classification models and saliency maps. In Yoshua Bengio and Yann LeCun, editors, 2nd international conference on learning representations, (ICLR), Banff, AB, Canada.\n\n\nLeon Sixt, Maximilian Granz, and Tim Landgraf. 2020. When explanations lie: Why many modified BP attributions fail. In Hal Daumé III and Aarti Singh, editors, Proceedings of the 37th international conference on machine learning, volume 119, pages 9046–9057. PMLR.\n\n\nDaniel Smilkov, Nikhil Thorat, Been Kim, Fernanda Viégas, and Martin Wattenberg. 2017. SmoothGrad: Removing noise by adding noise.\n\n\nPaul Smolensky. 1986. Neural and conceptual interpretation of PDP models.\n\n\nMatthew Snover, Bonnie Dorr, Rich Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proceedings of the 7th conference of the association for machine translation in the americas: Technical papers, pages 223–231, Cambridge, Massachusetts, USA. Association for Machine Translation in the Americas.\n\n\nLucia Specia, Carolina Scarton, Gustavo Henrique Paetzold, and Graeme Hirst. 2018. Quality estimation for machine translation. Morgan & Claypool Publishers.\n\n\nLucia Specia, Marco Turchi, Nicola Cancedda, Nello Cristianini, and Marc Dymetman. 2009. Estimating the sentence-level quality of machine translation systems. In Lluís Màrquez and Harold Somers, editors, Proceedings of the 13th annual conference of the european association for machine translation, Barcelona, Spain. European Association for Machine Translation.\n\n\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15(56):1929–1958.\n\n\nJianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. 2024. RoFormer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063.\n\n\nJiao Sun, Swabha Swayamdipta, Jonathan May, and Xuezhe Ma. 2022. Investigating the benefits of free-form rationales. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Findings of the association for computational linguistics: EMNLP 2022, pages 5867–5882, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\n\n\nMukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017. Axiomatic attribution for deep networks. In Proceedings of the 34th international conference on machine learning (ICML), volume 70, pages 3319–3328, Sydney, Australia. Journal of Machine Learning Research (JMLR).\n\n\nAleš Tamchyna. 2021. Deploying MT quality estimation on a large scale: Lessons learned and open questions. In Janice Campbell, Ben Huyck, Stephen Larocca, Jay Marciano, Konstantin Savenkov, and Alex Yanishevsky, editors, Proceedings of machine translation summit XVIII: Users and providers track, pages 291–305, Virtual. Association for Machine Translation in the Americas.\n\n\nYuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Naman Goyal, Vishrav Chaudhary, Jiatao Gu, and Angela Fan. 2021. Multilingual translation from denoising pre-training. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, Findings of the association for computational linguistics: ACL-IJCNLP 2021, pages 3450–3466, Online. Association for Computational Linguistics.\n\n\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019. BERT rediscovers the classical NLP pipeline. In Anna Korhonen, David Traum, and Lluís Màrquez, editors, Proceedings of the 57th annual meeting of the association for computational linguistics, pages 4593–4601, Florence, Italy. Association for Computational Linguistics.\n\n\nIan Tenney, Ryan Mullins, Bin Du, Shree Pandya, Minsuk Kahng, and Lucas Dixon. 2024. Interactive prompt debugging with sequence salience. Arxiv.\n\n\nIan Tenney, James Wexler, Jasmijn Bastings, Tolga Bolukbasi, Andy Coenen, Sebastian Gehrmann, Ellen Jiang, Mahima Pushkarna, Carey Radebaugh, Emily Reif, and Ann Yuan. 2020. The language interpretability tool: Extensible, interactive visualizations and analysis for NLP models. In Qun Liu and David Schlangen, editors, Proceedings of the 2020 conference on empirical methods in natural language processing: System demonstrations, pages 107–118, Online. Association for Computational Linguistics.\n\n\nBrian Thompson and Matt Post. 2020. Automatic machine translation evaluation in many languages via zero-shot paraphrasing. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, Proceedings of the 2020 conference on empirical methods in natural language processing (EMNLP), pages 90–121, Online. Association for Computational Linguistics.\n\n\nCurt Tigges, Oskar J. Hollinsworth, Atticus Geiger, and Neel Nanda. 2024. Language models linearly represent sentiment. In Yonatan Belinkov, Najoung Kim, Jaap Jumelet, Hosein Mohebbi, Aaron Mueller, and Hanjie Chen, editors, Proceedings of the 7th BlackboxNLP workshop: Analyzing and interpreting neural networks for NLP, pages 58–87, Miami, Florida, US. Association for Computational Linguistics.\n\n\nAntonio Toral, Sheila Castilho, Ke Hu, and Andy Way. 2018a. Attaining the unattainable? Reassessing claims of human parity in neural machine translation. In Ondřej Bojar, Rajen Chatterjee, Christian Federmann, Mark Fishel, Yvette Graham, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes, Philipp Koehn, Christof Monz, Matteo Negri, Aurélie Névéol, Mariana Neves, Matt Post, Lucia Specia, Marco Turchi, and Karin Verspoor, editors, Proceedings of the third conference on machine translation: Research papers, pages 113–123, Brussels, Belgium. Association for Computational Linguistics.\n\n\nAntonio Toral, Martijn Wieling, and Andy Way. 2018b. Post-editing effort of a novel with statistical and neural machine translation. Frontiers in Digital Humanities, 5:1–11.\n\n\nHugo Touvron, Louis Martin, Kevin R. Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Daniel M. Bikel, Lukas Blecher, Cristian Cantòn Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. ArXiv.\n\n\nMarco Turchi, Antonios Anastasopoulos, José G. C. de Souza, and Matteo Negri. 2014. Adaptive quality estimation for machine translation. In Kristina Toutanova and Hua Wu, editors, Proceedings of the 52nd annual meeting of the association for computational linguistics (volume 1: Long papers), pages 710–720, Baltimore, Maryland. Association for Computational Linguistics.\n\n\nMarco Turchi, Matteo Negri, M. Amin Farajian, and Marcello Federico. 2017. Continuous learning from human post-edits for neural machine translation. The Prague Bulletin of Mathematical Linguistics, 108:233–244.\n\n\nMarco Turchi, Matteo Negri, and Marcello Federico. 2013. Coping with the subjectivity of human judgements in MT quality estimation. In Ondrej Bojar, Christian Buck, Chris Callison-Burch, Barry Haddow, Philipp Koehn, Christof Monz, Matt Post, Herve Saint-Amand, Radu Soricut, and Lucia Specia, editors, Proceedings of the eighth workshop on statistical machine translation, pages 240–251, Sofia, Bulgaria. Association for Computational Linguistics.\n\n\nVladimir N. Vapnik. 1995. The nature of statistical learning theory. Springer-Verlag New York, Inc.\n\n\nHelena Vasconcelos, Gagan Bansal, Adam Fourney, Q. Vera Liao, and Jennifer Wortman Vaughan. 2025. Generation probabilities are not enough: Uncertainty highlighting in AI code completions. ACM Trans. Comput.-Hum. Interact., 32(1).\n\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in neural information processing systems, volume 30. Curran Associates, Inc.\n\n\nElena Voita, Rico Sennrich, and Ivan Titov. 2019. When a good translation is wrong in context: Context-aware machine translation improves on deixis, ellipsis, and lexical cohesion. In Anna Korhonen, David Traum, and Lluís Màrquez, editors, Proceedings of the 57th annual meeting of the association for computational linguistics, pages 1198–1212, Florence, Italy. Association for Computational Linguistics.\n\n\nElena Voita, Pavel Serdyukov, Rico Sennrich, and Ivan Titov. 2018. Context-aware neural machine translation learns anaphora resolution. In Iryna Gurevych and Yusuke Miyao, editors, Proceedings of the 56th annual meeting of the association for computational linguistics (volume 1: Long papers), pages 1264–1274, Melbourne, Australia. Association for Computational Linguistics.\n\n\nElizabeth Wagner. 1983. Rapid post-editing of systran. In Veronica Lawson, editor, Proceedings of translating and the computer 5: Tools for the trade, London, UK. Aslib.\n\n\nLongyue Wang, Chenyang Lyu, Tianbo Ji, Zhirui Zhang, Dian Yu, Shuming Shi, and Zhaopeng Tu. 2023. Document-level machine translation with large language models. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 conference on empirical methods in natural language processing, pages 16646–16661, Singapore. Association for Computational Linguistics.\n\n\nAlex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mohananey, Wei Peng, Sheng-Fu Wang, and Samuel R. Bowman. 2020. BLiMP: The benchmark of linguistic minimal pairs for English. Transactions of the Association for Computational Linguistics, 8:377–392.\n\n\nJohn S. White, Theresa A. O’Connell, and Francis E. O’Mara. 1994. The ARPA MT evaluation methodologies: Evolution, lessons, and future approaches. In Proceedings of the first conference of the association for machine translation in the americas, Columbia, Maryland, USA.\n\n\nSarah Wiegreffe and Yuval Pinter. 2019. Attention is not not explanation. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors, Proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing (EMNLP-IJCNLP), pages 11–20, Hong Kong, China. Association for Computational Linguistics.\n\n\nZhengxuan Wu, Aryaman Arora, Zheng Wang, Atticus Geiger, Dan Jurafsky, Christopher D. Manning, and Christopher Potts. 2024. ReFT: Representation finetuning for language models. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, Advances in neural information processing systems, volume 37, pages 63908–63962. Curran Associates, Inc.\n\n\nHaoran Xu, Young Jin Kim, Amr Sharaf, and Hany Hassan Awadalla. 2024. A paradigm shift in machine translation: Boosting translation performance of large language models. In The twelfth international conference on learning representations.\n\n\nWeijia Xu, Sweta Agrawal, Eleftheria Briakou, Marianna J. Martindale, and Marine Carpuat. 2023. Understanding and detecting hallucinations in neural machine translation via model introspection. Transactions of the Association for Computational Linguistics, 11:546–564.\n\n\nKayo Yin and Graham Neubig. 2022. Interpreting language models with contrastive explanations. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Proceedings of the 2022 conference on empirical methods in natural language processing, pages 184–198, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\n\n\nMatthew D. Zeiler, Graham W. Taylor, and Rob Fergus. 2011. Adaptive deconvolutional networks for mid and high level feature learning. In 2011 international conference on computer vision (ICCV), pages 2018–2025.\n\n\nChrysoula Zerva, Frederic Blain, José G. C. De Souza, Diptesh Kanojia, Sourabh Deoghare, Nuno M. Guerreiro, Giuseppe Attanasio, Ricardo Rei, Constantin Orasan, Matteo Negri, Marco Turchi, Rajen Chatterjee, Pushpak Bhattacharyya, Markus Freitag, and André Martins. 2024. Findings of the quality estimation shared task at WMT 2024: Are LLMs closing the gap in QE? In Barry Haddow, Tom Kocmi, Philipp Koehn, and Christof Monz, editors, Proceedings of the ninth conference on machine translation, pages 82–109, Miami, Florida, USA. Association for Computational Linguistics.\n\n\nChrysoula Zerva, Frédéric Blain, Ricardo Rei, Piyawat Lertvittayakumjorn, José G. C. de Souza, Steffen Eger, Diptesh Kanojia, Duarte Alves, Constantin Orăsan, Marina Fomicheva, André F. T. Martins, and Lucia Specia. 2022. Findings of the WMT 2022 shared task on quality estimation. In Philipp Koehn, Loïc Barrault, Ondřej Bojar, Fethi Bougares, Rajen Chatterjee, Marta R. Costa-jussà, Christian Federmann, Mark Fishel, Alexander Fraser, Markus Freitag, Yvette Graham, Roman Grundkiewicz, Paco Guzman, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes, Tom Kocmi, André Martins, Makoto Morishita, et al., editors, Proceedings of the seventh conference on machine translation (WMT), pages 69–99, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics.\n\n\nBiao Zhang and Rico Sennrich. 2019. Root mean square layer normalization. In Proceedings of the 33rd international conference on neural information processing systems, Red Hook, NY, USA. Curran Associates Inc.\n\n\nJiacheng Zhang, Huanbo Luan, Maosong Sun, Feifei Zhai, Jingfang Xu, Min Zhang, and Yang Liu. 2018. Improving the transformer translation model with document-level context. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun’ichi Tsujii, editors, Proceedings of the 2018 conference on empirical methods in natural language processing, pages 533–542, Brussels, Belgium. Association for Computational Linguistics.\n\n\nAndy Zou, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard Ren, Alexander Pan, Xuwang Yin, Mantas Mazeika, Ann-Kathrin Dombrowski, Shashwat Goel, Nathaniel Li, Michael J. Byun, Zifan Wang, Alex Mallen, Steven Basart, Sanmi Koyejo, Dawn Song, Matt Fredrikson, et al. 2024. Enhancing neural network transparency through representation analysis. OpenReview.\n\n\nVilém Zouhar, Tom Kocmi, and Mrinmaya Sachan. 2025. AI-assisted human evaluation of machine translation. In Luis Chiruzzo, Alan Ritter, and Lu Wang, editors, Proceedings of the 2025 conference of the nations of the americas chapter of the association for computational linguistics: Human language technologies (volume 1: Long papers), pages 4936–4950, Albuquerque, New Mexico. Association for Computational Linguistics.\n\n\nVilém Zouhar, Michal Novák, Matúš Žilinec, Ondřej Bojar, Mateo Obregón, Robin L. Hill, Frédéric Blain, Marina Fomicheva, Lucia Specia, and Lisa Yankovskaya. 2021. Backtranslation feedback improves user confidence in MT, not quality. In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou, editors, Proceedings of the 2021 conference of the north american chapter of the association for computational linguistics: Human language technologies, pages 151–161, Online. Association for Computational Linguistics.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "chapters/chap-2-background.html#footnotes",
    "href": "chapters/chap-2-background.html#footnotes",
    "title": "2  Background",
    "section": "",
    "text": "More details on neural networks can be found in Goodfellow et al. (2016).↩︎\nBias terms can be omitted, following the practice of recent models such as Llama (Touvron et al., 2023)↩︎\nhttps://openai.com/index/chatgpt↩︎\nProbability scores are commonly used as differentiation targets, see discussion in Bastings et al. (2022).↩︎\nhttps://github.com/PAIR-code/saliency↩︎\nSince the push towards proprietary model serving, details about the distribution of training data across languages in tech reports are often scarce.↩︎\nhttps://go.proz.com/blog/cat-tool-use-by-translators-who-is-using↩︎\nA comprehensive overview of MT metrics was released by Lee et al. (2023).↩︎",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "chapters/chap-3-inseq.html",
    "href": "chapters/chap-3-inseq.html",
    "title": "3  Attributing Language Model Generations with the Inseq Toolkit",
    "section": "",
    "text": "3.1 Related Work\nRecent years saw an increase in studies and tools aimed at improving our behavioral or mechanistic understanding of neural language models (Belinkov and Glass, 2019).\nMany studies applied such techniques to modern deep learning architectures, including transformers (Vaswani et al., 2017), leveraging gradients (Baehrens et al., 2010; Sundararajan et al., 2017), attention patterns (Xu et al., 2015; Clark et al., 2019) and input perturbations (Zeiler and Fergus, 2014; Feng et al., 2018) to quantify input importance, often leading to controversial outcomes in terms of faithfulness, plausibility and overall usefulness of such explanations (Adebayo et al., 2018; Jain and Wallace, 2019; Jacovi and Goldberg, 2020; Zafar et al., 2021).\nHowever, input attribution techniques have mainly been applied to classification settings (Atanasova et al., 2020; Wallace et al., 2020; Madsen et al., 2022; Chrysostomou and Aletras, 2022), with relatively little interest in the more convoluted mechanisms underlying generation. Classification attribution is a single-step process resulting in one importance score per input token, often allowing for intuitive interpretations in relation to the predicted class. Sequential attribution1 instead involves a computationally expensive multi-step iteration producing a matrix \\(A_{ij}\\) representing the importance of every input \\(i\\) in the prediction of every generation outcome \\(j\\) (Figure 3.1).\nMoreover, since previous generation steps causally influence following predictions, they must be dynamically incorporated into the set of attributed inputs throughout the process. Lastly, while classification typically involves a limited set of classes and simple output selection (e.g., argmax after softmax), generation often operates with large vocabularies and non-trivial decoding strategies (Eikema and Aziz, 2020). These differences limited the use of input attribution methods for generation settings, with relatively few works improving attribution efficiency (Vafa et al., 2021; Ferrando et al., 2022) and the informativeness of explanations (Yin and Neubig, 2022).\nHaving established a theoretical background on input attribution methods in Section 2.2, we introduce Inseq, a Python library that democratizes access to interpretability analyses of generative language models. Inseq centralizes access to a broad set of input attribution methods, sourced in part from the Captum (Kokhlikyan et al., 2020) framework, enabling a fair comparison of different techniques for all sequence-to-sequence and decoder-only models in the popular 🤗 transformers library (Wolf et al., 2020). Thanks to its intuitive interface, users can easily integrate interpretability analyses into sequence generation experiments with just 3 lines of code (Figure 3.2). Nevertheless, Inseq is also highly flexible, including cutting-edge attribution methods with built-in post-processing features (Section 3.2.2), supporting customizable attribution targets and enabling constrained decoding of arbitrary sequences (Section 3.2.3).\nIn terms of usability, Inseq greatly simplifies access to local and global explanations, offering built-in support for a command-line interface (CLI), optimized batching that enables dataset-wide attribution, and various methods for visualizing, serializing, and reloading attribution outcomes and generated sequences (Section 3.2.4). Ultimately, Inseq aims to make sequence models first-class citizens in interpretability research and drive future advances in interpretability for generative applications.\nTools for NLP Interpretability Although many post-hoc interpretability libraries were released recently, only a few support sequential input attribution. Notably, LIT (Tenney et al., 2020), a structured framework for analyzing models across modalities, and Ecco (Alammar, 2021), a library specialized in interactive visualizations of model internals. LIT is an all-in-one, GUI-based tool for analyzing model behaviors across entire datasets. However, the library does not provide out-of-the-box support for 🤗 transformers models, requiring the definition of custom wrappers to ensure compatibility. Moreover, it has a steep learning curve due to its advanced UI, which can be inconvenient when working with a small number of examples. All these factors limit LIT usability for researchers working with custom models, needing access to extracted scores, or being less familiar with interpretability research. On the other hand, Ecco is closer to our work, being based on 🤗 transformers and having started to support encoder-decoder models concurrently with Inseq development. Despite a marginal overlap in their functionalities, the two libraries provide orthogonal benefits: Inseq’s flexible interface makes it especially suitable for methodical quantitative analyses involving repeated evaluations, while Ecco excels in qualitative analyses aimed at visualizing model internals. Other popular tools such as ERASER (DeYoung et al., 2020), Thermostat (Feldhus et al., 2021), transformers-interpret (Pierse, 2021) and ferret (Attanasio et al., 2023) do not support sequence models.",
    "crumbs": [
      "Attributing Context Usage in Multilingual NLP",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Attributing Language Model Generations with the Inseq Toolkit</span>"
    ]
  },
  {
    "objectID": "chapters/chap-3-inseq.html#design",
    "href": "chapters/chap-3-inseq.html#design",
    "title": "3  Attributing Language Model Generations with the Inseq Toolkit",
    "section": "3.2 Design",
    "text": "3.2 Design\nInseq combines sequence models sourced from 🤗 transformers (Wolf et al., 2020) and attribution methods mainly sourced from Captum (Kokhlikyan et al., 2020). While only text-based tasks are currently supported, the library’s modular design would enable the inclusion of other modeling frameworks, e.g. fairseq (Ott et al., 2019), and modalities (e.g. speech) without requiring substantial redesign. Optional dependencies include 🤗 datasets (Lhoest et al., 2021) and Rich.2 Figure 3.3 presents the Inseq hierarchy of models and attribution methods. The model-method connection enables out-of-the-box attribution using the selected method. Framework-specific and architecture-specific classes enable the extension of Inseq to new modeling architectures and frameworks.\n\n\n\n\n\n\nFigure 3.3: Inseq models and attribution methods. Concrete classes combine abstract framework and architecture attribution models classes, and are derived from abstract attribution methods’ categories.\n\n\n\n\n3.2.1 Guiding Principles\n\nResearch and Generation-oriented: Inseq should support interpretability analyses of a broad set of sequence generation models without focusing narrowly on specific architectures or tasks. Moreover, the inclusion of new, cutting-edge methods should be prioritized to enable fair comparisons with well-established ones.\nScalable: The library should provide an optimized interface to a wide range of use cases, models and setups, ranging from interactive attributions of individual examples using toy models to compiling statistics of large language models’ predictions for entire datasets.\nBeginner-friendly: Inseq should provide built-in access to popular frameworks for sequence generation modeling and be fully usable by non-experts at a high level of abstraction, providing sensible defaults for supported attribution methods.\nExtensible: Inseq should support a high degree of customization for experienced users, with out-of-the-box support for user-defined solutions to enable future investigations into models’ behaviors.\n\n\n\n3.2.2 Input Attribution and Post-processing\n\n\n\n\n\n\n\nMethod\nSource\n\\(f(l)\\)\n\n\n\n\nG\n(Input ×) Gradient\nSimonyan et al. (2014)\n✅\n\n\nDeepLIFT\nShrikumar et al. (2017)\n✅\n\n\nGradientSHAP\nLundberg and Lee (2017)\n❌\n\n\nIntegrated Gradients\nSundararajan et al. (2017)\n✅\n\n\nDiscretized IG\nSanyal and Ren (2021)\n❌\n\n\nSequential IG\nEnguehard (2023)\n❌\n\n\nI\nAttention Weights\nBahdanau et al. (2015)\n✅\n\n\nP\nOcclusion (Blank-out)\nZeiler and Fergus (2014)\n❌\n\n\nLIME\nRibeiro et al. (2016)\n❌\n\n\nValue Zeroing\nMohebbi et al. (2023)\n✅\n\n\nReAGent\nZhao and Shan (2024)\n❌\n\n\nS\n(Log) Probability\n-\n\n\n\nSoftmax Entropy\n-\n\n\n\nTarget Cross-entropy\n-\n\n\n\nPerplexity\n-\n\n\n\nKL Divergence\n-\n\n\n\nContrastive Logits/Prob. \\(\\Delta\\)\nYin and Neubig (2022)\n\n\n\n\\(\\mu\\) MC Dropout Prob.\nGal and Ghahramani (2016)\n\n\n\nPCXMI\nFernandes et al. (2023)\n\n\n\nIn-context PVI\nLu et al. (2023)\n\n\n\n\n\n\nTable 3.1: Overview of gradient-based (G), internals-based (I) and perturbation-based (P) attribution methods and built-in step functions (S) available in Inseq. \\(f(l)\\) marks methods allowing for attribution of arbitrary intermediate layers. Bolded methods were introduced with Inseq v0.6.\n\n\n\nAt its core, Inseq provides a simple interface for applying input attribution techniques to sequence generation tasks. We categorize methods in three groups, gradient-based, internals-based and perturbation-based, depending on their underlying approach to importance quantification.3 Table 3.1 presents the complete list of supported methods. Aside from popular model-agnostic methods, Inseq notably provides built-in support for attention weight attribution and a range of cutting-edge methods not supported in any other toolkit, such as Discretized Integrated Gradients (Sanyal and Ren, 2021), Sequential Integrated Gradients (Enguehard, 2023), Value Zeroing (Mohebbi et al., 2023), and ReAGent (Zhao and Shan, 2024). Moreover, multiple methods support the importance attribution of custom intermediate model layers, simplifying studies on representational structures and information mixing in sequential models, as seen in our case study of Section 3.3.2.\nSource and target-side attribution When using encoder-decoder architectures, users can set the attribute_target parameter to include or exclude the generated prefix in the attributed inputs. In most cases, this should be desirable to account for recently generated tokens when explaining model behaviors, such as when to terminate the generation (e.g. relying on the presence of _yes in the target prefix to predict &lt;/s&gt; in Figure 3.2, right matrix). However, attributing the source side separately could be helpful, for example, to derive word alignments from importance scores.\nPost-processing of attribution outputs Aggregation is a fundamental but often overlooked step in attribution-based analyses since most methods produce neuron-level or subword-level importance scores that would otherwise be difficult to interpret. Inseq includes several Aggregator classes to perform attribution aggregation across various dimensions. For example, the input word Explanation could be tokenized into two subword tokens Expl and anation, and each token would receive \\(N\\) importance scores, where \\(N\\) is the model embedding dimension. In this case, aggregators could first merge subword-level scores into word-level scores, and then merge granular embedding-level scores to obtain a single token-level score that is easier to interpret. Moreover, aggregation could prove especially helpful for long-form generation tasks such as summarization, where word-level importance scores could be aggregated to obtain a measure of sentence-level relevance. Notably, Inseq allows chaining multiple aggregators like in the example above using the AggregatorPipeline class, and provides a PairAggregator to aggregate different attribution maps, simplifying the conduction of contrastive analyses as in Section 3.3.1.4\n\n\n3.2.3 Customizing generation and attribution\nDuring attribution, Inseq first generates target tokens using 🤗 transformers and then attributes them step-by-step. If a custom target string is specified alongside model inputs, the generation step is instead skipped, and the provided text is attributed by constraining the decoding of its tokens.5 Constrained attribution can be used, among other things, for contrastive comparisons of minimal pairs and to obtain model justifications for desired outputs.\nCustom step functions At every attribution step, Inseq can extract scores of interest (e.g. probabilities, entropy) that can be useful, among other things, to quantify model uncertainty (e.g. how likely the generated _yes token was given the context in Figure 3.2). We collectively refer to functions computing these scores as step functions. Inseq provides access to multiple built-in step functions (Table 3.1, S), enabling the computation of these scores, and allows users to create and register new custom ones. Step scores are computed together with the attribution, returned as separate sequences in the output, and visualized alongside importance scores (e.g. the \\(p(y_t|y_{&lt;t})\\) row in Figure 3.1).\nStep functions as attribution targets For methods relying on model outputs to predict input importance (gradient and perturbation-based), input attributions are commonly obtained from the model’s output logits or class probabilities (Bastings et al., 2022). However, recent work has shown the effectiveness of using targets, such as the probability difference of a contrastive output pair, to answer interesting questions like “What inputs drive the prediction of \\(y\\) rather than \\(\\hat{y}\\)?” (Yin and Neubig, 2022). For example, the gradient \\(\\nabla(p(\\text{barking}) - p(\\text{crying}))\\) given the prompt *“Can you stop the dog from ___“* will highlight the role of the entity dog in selecting barking, disentangling the semantic component from grammatical correctness by providing a crying as grammatically valid choice. Figure 3.4 provides an example of such an approach for gender bias detection in machine translation. Inseq users can leverage any built-in or custom-defined step function as an attribution target, enabling advanced use cases like contrastive comparisons.\n\n\n\n\n\nimport inseq\n\nmodel = inseq.load_model(\n    \"Helsinki-NLP/opus-mt-en-it\",\n    \"saliency\"\n)\nattr_out = model.attribute(\n    \"I said hi to the manager\",\n    \"Ho salutato il manager\",\n    contrast_targets=\\\n        \"Ho salutato la manager\",\n    attributed_fn=\\\n        \"contrast_prob_diff\",\n    step_scores=[\n        \"probability\",\n        \"contrast_prob_diff\"\n ]\n)\n\n\n \n\n\n\n\n\n\n\n\n\n\n\nFigure 3.4: Source-to-target attributions aggregated at the token level, indicating the importance of the stereotypical noun “manager” to generate the Italian masculine pronoun “il” (original) over the feminine “la” (contrastive case).\n\n\n\n\n\n3.2.4 Usability Features\nBatched and span-focused attributions The library provides built-in batching capabilities, enabling users to go beyond single sentences and attribute even entire datasets in a single function call. When the attribution of a specific span of interest is needed, Inseq also allows specifying a start and end position for the attribution process. This functionality greatly accelerates the attribution process for studies on localized phenomena (e.g. pronoun coreference in MT models).\nAlignment of contrastive options Inseq supports customizable word alignments, i.e. indices aligning tokens in the original and contrastive generated texts, to support contrastive comparisons between texts of different lengths, including automatic alignments using the multilingual LaBSE encoder (Feng et al., 2022) to streamline their application.\nCLI, serialization and visualization The Inseq library offers an API to attribute single examples or entire 🤗 Datasets from the command line and save resulting outputs and visualizations to a file. Attribution outputs can be saved and loaded in JSON format, along with their respective metadata, to easily identify the provenance of the contents. Attributions can be visualized in the console or IPython notebooks and exported as HTML files.\nQuantized and distributed attribution Supporting the attribution of large models is critical given recent scaling tendencies (Kaplan et al., 2020). All models that allow for quantization using bitsandbytes (Dettmers et al., 2022) can be loaded directly in 4-bit and 8-bit formats from 🤗 transformers, and their attributions can be computed normally using Inseq at a fraction of the original computational cost.6 Relatedly, Inseq is also compatible with the Petals framework (Borzunov et al., 2023), which supports gradient-based attribution across language models whose computation is distributed across multiple machines. This can alleviate the need for high-end GPUs to run LLMs, enabling the distributed computation of attribution scores.7",
    "crumbs": [
      "Attributing Context Usage in Multilingual NLP",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Attributing Language Model Generations with the Inseq Toolkit</span>"
    ]
  },
  {
    "objectID": "chapters/chap-3-inseq.html#sec-chap3-case-studies",
    "href": "chapters/chap-3-inseq.html#sec-chap3-case-studies",
    "title": "3  Attributing Language Model Generations with the Inseq Toolkit",
    "section": "3.3 Case Studies",
    "text": "3.3 Case Studies\n\n3.3.1 Gender Bias in Machine Translation\nIn the first case study, we use Inseq to investigate gender bias in MT models. Studying the social biases embedded in these models is crucial to understanding and mitigating the representational and allocative harms they may engender (Blodgett et al., 2020). Savoldi et al. (2021) note that the study of bias in MT could benefit from explainability techniques to identify spurious cues exploited by the model and the interaction of different features that can lead to intersectional bias.\nSynthetic Setup: Turkish to English The Turkish language uses the gender-neutral pronoun o, which can be translated into English as either he, she, or it, making it interesting to study gender bias in MT when associated with a language such as English, for which models will tend to choose a gendered pronoun form. Previous works have leveraged translations from gender-neutral languages to demonstrate the presence of gender bias in translation systems (Cho et al., 2019; Prates et al., 2020; Farkas and Németh, 2022). We repeat this simple setup using a Turkish-to-English MarianMT model (Tiedemann, 2020) and compute different metrics to quantify gender bias using Inseq.\n\n\n\n\n\n\n\nBase\n♀ \\(\\rightarrow\\) ♂\n\n\n\n\\(x_\\text{pron}\\)\n\\(x_\\text{occ}\\)\n\\(x_\\text{pron}\\)\n\\(x_\\text{occ}\\)\n\n\n\n\n\\(p(y_\\text{pron})\\)\n0.01\n-0.44*\n\n\n\\(\\nabla\\)\n-0.16\n0.25*\n0.23*\n-0.00\n\n\nIG\n-0.08\n0.09\n0.11\n0.17\n\n\nI×G\n-0.11\n0.22*\n0.22*\n-0.01\n\n\n\n\n\nTable 3.2: Gender Bias in Turkish-to-English MT: Kendall’s \\(\\tau\\) correlation of MT model metrics with U.S. labor statistics. * = Significant correlation (\\(p&lt;.05\\)).\n\n\n\nWe select 49 Turkish occupation terms verified by a native speaker (see Section A.1.1) and use them to infill the template sentence O bir ____ (He/She is a(n) ____). For each translation, we compute attribution scores for source Turkish pronoun (\\(x_\\text{pron}\\)) and occupation (\\(x_\\text{occ}\\)) tokens8 when generating the target English pronoun (\\(y_\\text{pron}\\)) using Integrated Gradients (IG), Gradients (\\(\\nabla\\)), and Input \\(\\times\\) Gradient (I\\(\\times\\)G).9 We also collect target pronoun probabilities (\\(p(y_\\text{pron})\\)), rank the 49 occupation terms using these metrics, and finally compute Kendall’s \\(\\tau\\) correlation with the percentage of women working in the respective fields, using U.S. labor statistics as in previous works (e.g., Caliskan et al., 2017; Rudinger et al., 2018). Table 3.2 presents our results.\nIn the base case, we correlate the different metrics with how much the gender distribution deviates from an equal distribution (\\(50-50\\%\\)) for each occupation (i.e., the gender bias irrespective of the direction). We observe a strong gender bias, with she being chosen only for 5 out of 49 translations and gender-neutral variants never being produced by the MT model. We find a low correlation between pronoun probability and the degree of gender stereotype associated with the occupation. Moreover, we note a weaker correlation for IG compared to the other two methods. For those, attribution scores for \\(x_\\text{occ}\\) show significant correlations with labor statistics, supporting the intuition that the MT model will accord higher importance to source occupation terms associated to gender-stereotypical occupations when predicting the gendered target pronoun.\nIn the gender-swap case (♀️ \\(\\rightarrow\\) ♂️), we use the PairAggregator class to contrastively compare attribution scores and probabilities when translating the pronoun as She or He.10 We correlate the resulting scores with the percentage of women working in the respective occupation and find strong correlations for \\(p(y_\\text{pron})\\), which supports the validity of contrastive approaches in uncovering gender bias.\nQualitative Example: English to Dutch We also qualitatively analyze biased MT outputs, showing how attributions can help develop hypotheses about models’ behavior. Table 3.3 (top) shows the I \\(\\times\\) G attributions for English-to-Dutch translation using M2M-100 (Fan et al., 2021).\n\n\n\n\n\n\n\n\nSource\nDe\nleraar\nverliest\nzijn\nbaan\n\n\n\n\nThe\n0.10\n0.08\n0.04\n0.03\n0.02\n\n\nteacher\n0.11\n0.20\n0.06\n0.03\n0.05\n\n\nloses\n0.11\n0.09\n0.25\n0.07\n0.07\n\n\nher\n0.15\n0.09\n0.10\n0.21\n0.07\n\n\njob\n0.10\n0.08\n0.08\n0.10\n0.24\n\n\nTarget\nDe\nleraar\nverliest\nzijn\nbaan\n\n\nDe\n\n0.23\n0.05\n0.06\n0.04\n\n\nleraar\n\n\n0.17\n0.13\n0.03\n\n\nverliest\n\n\n\n0.18\n0.08\n\n\nzijn\n\n\n\n\n0.26\n\n\n\\(p(y_t)\\)\n0.69\n0.28\n0.35\n0.65\n0.29\n\n\n\n\n\n\n\n\n\n\nSource\nDe\n♂ → Ø\nverliest\nhaar\nbaan\n\n\n\n\nThe\n0.00\n-0.02\n0.00\n0.00\n0.00\n\n\nteacher\n0.00\n-0.05\n-0.01\n-0.01\n-0.01\n\n\nloses\n0.00\n-0.02\n-0.01\n-0.02\n-0.01\n\n\nher\n0.00\n-0.01\n-0.01\n-0.10\n0.01\n\n\njob\n0.00\n-0.02\n-0.01\n-0.02\n-0.02\n\n\nTarget\nDe\n♂ → Ø\nverliest\nhaar\nbaan\n\n\nDe\n\n-0.07\n-0.01\n0.01\n-0.01\n\n\n♂ → Ø\n\n\n0.09\n0.18\n0.02\n\n\nverliest\n\n\n\n-0.03\n0.00\n\n\nhaar\n\n\n\n\n0.00\n\n\n\\(\\Delta p(y_t)\\)\n0.00\n-0.23\n0.13\n0.20\n0.00\n\n\n\n\n\nTable 3.3: Top: Attribution of pronoun gender mistranslation using M2M-100. Bottom: Target attribution difference when swapping the target noun gender (♂️ \\(\\to\\) Ø) from leraar (male) to leerkracht (gender-neutral).\n\n\n\nThe model mistranslates the pronoun her into the masculine form zijn (his). We find that the wrongly translated pronoun exhibits high probability but does not associate substantial importance to the source occupation term teacher. Instead, we find good relative importance for the preceding word and leraar (male teacher). This suggests a strong prior bias for masculine variants, as shown by the pronoun zijn and the noun leraar, which may be a possible cause for this mistranslation. When considering the contrastive example obtained by swapping leraar with its gender-neutral variant leerkracht (Table 3.3, bottom), we find increased importance of the target occupation in determining the correctly-gendered target pronoun haar (her). Our results highlight the tendency of MT models to attend inputs sequentially rather than relying on context, hinting at the known benefits of context-aware models for pronoun translation (Voita et al., 2018).\n\n\n3.3.2 Locating Factual Knowledge inside GPT-2\nFor our second case study, we experiment with a novel attribution-based technique to locate factual knowledge encoded in the layers of GPT-2 1.5B (Radford et al., 2019). Specifically, we aim to reproduce the results of Meng et al. (2022), showing the influence of intermediate layers in mediating the recall of factual statements such as The Eiffel Tower is located in the city of \\(\\rightarrow\\) Paris. Meng et al. (2022) estimated the effect of network components in the prediction of factual statements as the difference in probability of a correct target (e.g. Paris), given a corrupted subject embedding (e.g. for Eiffel Tower), before and after restoring clean activations for some input tokens at different layers of the network. Apart from the obvious importance of final token states in terminal layers, their results highlight the presence of an early site associated with the last subject token playing an important role in recalling the network’s factual knowledge (Figure 3.5, top).\n\n\n\n\n\n\nFigure 3.5: Top: Estimated causal importance of GPT-2 XL layers for predicting factual associations, as reported by Meng et al. (2022). Bottom: Average GPT-2 XL Gradient \\(\\times\\) Layer Activation scores obtained with Inseq using contrastive factual pairs as attribution targets.\n\n\n\nTo verify such results, we propose a novel knowledge location method, which we name Contrastive Attribution Tracing (CAT), adopting the contrastive attribution paradigm of Yin and Neubig (2022) to locate relevant network components by attributing minimal pairs of correct and wrong factual targets (e.g. Paris vs. Rome for the example above). To perform contrastive attribution, we use the Layer Gradient \\(\\times\\) Activation method, a layer-specific variant of Input \\(\\times\\) Gradient, to propagate gradients up to intermediate network activations rather than reaching input tokens. The resulting attribution scores hence answer the question “How important are layer \\(L\\) activations for prefix token \\(t\\) in predicting the correct factual target over a wrong one?”. We compute attribution scores for 1000 statements taken from the Counterfact Statement dataset (Meng et al., 2022) and present averaged results in Figure 3.5 (bottom).11 Our results closely align with those of the original authors, providing additional evidence that attribution methods can be used to identify salient network components and guide model editing, as demonstrated by Dai et al. (2022).\nWe introduced the proposed CAT method shortly before the attribution patching technique by Nanda (2023). Together, these two methods represent the most efficient knowledge location techniques based on gradient propagation, with our approach requiring only a single forward and backward pass of the attributed model. Patching-based approaches, such as causal mediation (Meng et al., 2022), on the other hand, provide causal guarantees of feature importance at the price of being more computationally intensive. Despite lacking the causal guarantees of such methods, CAT can provide an approximation of feature importance and greatly simplify the study of knowledge encoded in large language model representations, thanks to its efficiency.",
    "crumbs": [
      "Attributing Context Usage in Multilingual NLP",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Attributing Language Model Generations with the Inseq Toolkit</span>"
    ]
  },
  {
    "objectID": "chapters/chap-3-inseq.html#sec-conclusion",
    "href": "chapters/chap-3-inseq.html#sec-conclusion",
    "title": "3  Attributing Language Model Generations with the Inseq Toolkit",
    "section": "3.4 Conclusion",
    "text": "3.4 Conclusion\nWe introduced Inseq, a versatile and easy-to-use toolkit for interpreting sequence generation models. With many libraries focused on the study of classification models, Inseq is the first tool explicitly designed to analyze systems for tasks such as machine translation, code generation, and conversational applications. Researchers can easily add interpretability evaluations to their studies using our library to identify unwanted biases and interesting phenomena in their models’ predictions.\nWith the Inseq toolkit providing the foundational infrastructure for interpretability analysis, the following chapters will leverage the supported input attribution techniques to investigate context usage in context-aware machine translation systems Chapter 4 and multilingual language models for retrieval-augmented generation Chapter 5.\n\n\n\n\nJulius Adebayo, Justin Gilmer, Michael Muelly, Ian Goodfellow, Moritz Hardt, and Been Kim. 2018. Sanity checks for saliency maps. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in neural information processing systems, volume 31, pages 9505–9515, Montréal, Canada. Curran Associates, Inc.\n\n\nJ Alammar. 2021. Ecco: An open source library for the explainability of transformer language models. In Heng Ji, Jong C. Park, and Rui Xia, editors, Proceedings of the 59th annual meeting of the association for computational linguistics and the 11th international joint conference on natural language processing: System demonstrations, pages 249–257, Online. Association for Computational Linguistics.\n\n\nPepa Atanasova, Jakob Grue Simonsen, Christina Lioma, and Isabelle Augenstein. 2020. A diagnostic study of explainability techniques for text classification. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, Proceedings of the 2020 conference on empirical methods in natural language processing (EMNLP), pages 3256–3274, Online. Association for Computational Linguistics.\n\n\nGiuseppe Attanasio, Eliana Pastor, Chiara Di Bonaventura, and Debora Nozza. 2023. Ferret: A framework for benchmarking explainers on transformers. In Danilo Croce and Luca Soldaini, editors, Proceedings of the 17th conference of the european chapter of the association for computational linguistics: System demonstrations, pages 256–266, Dubrovnik, Croatia. Association for Computational Linguistics.\n\n\nDavid Baehrens, Timon Schroeter, Stefan Harmeling, Motoaki Kawanabe, Katja Hansen, and Klaus-Robert Müller. 2010. How to explain individual classification decisions. J. Mach. Learn. Res., 11:1803–1831.\n\n\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to align and translate. In Yoshua Bengio and Yann LeCun, editors, Proceedings of the 3rd international conference on learning representations (ICLR), San Diego, CA, USA.\n\n\nJasmijn Bastings, Sebastian Ebert, Polina Zablotskaia, Anders Sandholm, and Katja Filippova. 2022. “Will you find these shortcuts?” A protocol for evaluating the faithfulness of input salience methods for text classification. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Proceedings of the 2022 conference on empirical methods in natural language processing, pages 976–991, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\n\n\nYonatan Belinkov and James Glass. 2019. Analysis methods in neural language processing: A survey. Transactions of the Association for Computational Linguistics, 7:49–72.\n\n\nSu Lin Blodgett, Solon Barocas, Hal Daumé III, and Hanna Wallach. 2020. Language (technology) is power: A critical survey of “bias” in NLP. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault, editors, Proceedings of the 58th annual meeting of the association for computational linguistics, pages 5454–5476, Online. Association for Computational Linguistics.\n\n\nAlexander Borzunov, Dmitry Baranchuk, Tim Dettmers, Maksim Riabinin, Younes Belkada, Artem Chumachenko, Pavel Samygin, and Colin Raffel. 2023. Petals: Collaborative inference and fine-tuning of large models. In Danushka Bollegala, Ruihong Huang, and Alan Ritter, editors, Proceedings of the 61st annual meeting of the association for computational linguistics (volume 3: System demonstrations), pages 558–568, Toronto, Canada. Association for Computational Linguistics.\n\n\nAylin Caliskan, Joanna J. Bryson, and Arvind Narayanan. 2017. Semantics derived automatically from language corpora contain human-like biases. Science, 356(6334):183–186.\n\n\nWon Ik Cho, Ji Won Kim, Seok Min Kim, and Nam Soo Kim. 2019. On measuring gender bias in translation of gender-neutral pronouns. In Marta R. Costa-jussà, Christian Hardmeier, Will Radford, and Kellie Webster, editors, Proceedings of the first workshop on gender bias in natural language processing, pages 173–181, Florence, Italy. Association for Computational Linguistics.\n\n\nGeorge Chrysostomou and Nikolaos Aletras. 2022. An empirical study on explanations in out-of-domain settings. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Proceedings of the 60th annual meeting of the association for computational linguistics (volume 1: Long papers), pages 6920–6938, Dublin, Ireland. Association for Computational Linguistics.\n\n\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, et al. 2024. Scaling instruction-finetuned language models. Journal of Machine Learning Research, 25(70):1–53.\n\n\nKevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D. Manning. 2019. What does BERT look at? An analysis of BERT‘s attention. In Tal Linzen, Grzegorz Chrupała, Yonatan Belinkov, and Dieuwke Hupkes, editors, Proceedings of the 2019 ACL workshop BlackboxNLP: Analyzing and interpreting neural networks for NLP, pages 276–286, Florence, Italy. Association for Computational Linguistics.\n\n\nDamai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu Wei. 2022. Knowledge neurons in pretrained transformers. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Proceedings of the 60th annual meeting of the association for computational linguistics (volume 1: Long papers), pages 8493–8502, Dublin, Ireland. Association for Computational Linguistics.\n\n\nTim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. 2022. GPT3.int8(): 8-bit matrix multiplication for transformers at scale. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in neural information processing systems, volume 35, pages 30318–30332. Curran Associates, Inc.\n\n\nJay DeYoung, Sarthak Jain, Nazneen Fatema Rajani, Eric Lehman, Caiming Xiong, Richard Socher, and Byron C. Wallace. 2020. ERASER: A benchmark to evaluate rationalized NLP models. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault, editors, Proceedings of the 58th annual meeting of the association for computational linguistics, pages 4443–4458, Online. Association for Computational Linguistics.\n\n\nBryan Eikema and Wilker Aziz. 2020. Is MAP decoding all you need? The inadequacy of the mode in neural machine translation. In Donia Scott, Nuria Bel, and Chengqing Zong, editors, Proceedings of the 28th international conference on computational linguistics, pages 4506–4520, Barcelona, Spain (Online). International Committee on Computational Linguistics.\n\n\nJoseph Enguehard. 2023. Sequential integrated gradients: A simple but effective method for explaining language models. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Findings of the association for computational linguistics: ACL 2023, pages 7555–7565, Toronto, Canada. Association for Computational Linguistics.\n\n\nAngela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep Baines, Onur Çelebi, Guillaume Wenzek, Vishrav Chaudhary, Naman Goyal, Tom Birch, Vitaliy Liptchinsky, Sergey Edunov, Edouard Grave, Michael Auli, and Armand Joulin. 2021. Beyond english-centric multilingual machine translation. Journal of Machine Learning Research, 22(107):1–48.\n\n\nAnna Farkas and Renáta Németh. 2022. How to measure gender bias in machine translation: Real-world oriented machine translators, multiple reference points. Social Sciences & Humanities Open, 5(1):100239.\n\n\nNils Feldhus, Robert Schwarzenberg, and Sebastian Möller. 2021. Thermostat: A large collection of NLP model explanations and analysis tools. In Heike Adel and Shuming Shi, editors, Proceedings of the 2021 conference on empirical methods in natural language processing: System demonstrations, pages 87–95, Online; Punta Cana, Dominican Republic. Association for Computational Linguistics.\n\n\nFangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Arivazhagan, and Wei Wang. 2022. Language-agnostic BERT sentence embedding. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Proceedings of the 60th annual meeting of the association for computational linguistics (volume 1: Long papers), pages 878–891, Dublin, Ireland. Association for Computational Linguistics.\n\n\nShi Feng, Eric Wallace, Alvin Grissom II, Mohit Iyyer, Pedro Rodriguez, and Jordan Boyd-Graber. 2018. Pathologies of neural models make interpretations difficult. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun’ichi Tsujii, editors, Proceedings of the 2018 conference on empirical methods in natural language processing, pages 3719–3728, Brussels, Belgium. Association for Computational Linguistics.\n\n\nPatrick Fernandes, Kayo Yin, Emmy Liu, André Martins, and Graham Neubig. 2023. When does translation require context? A data-driven, multilingual exploration. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st annual meeting of the association for computational linguistics (volume 1: Long papers), pages 606–626, Toronto, Canada. Association for Computational Linguistics.\n\n\nJavier Ferrando, Gerard I. Gállego, Belen Alastruey, Carlos Escolano, and Marta R. Costa-jussà. 2022. Towards opening the black box of neural machine translation: Source and target interpretations of the transformer. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Proceedings of the 2022 conference on empirical methods in natural language processing, pages 8756–8769, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\n\n\nYarin Gal and Zoubin Ghahramani. 2016. Dropout as a bayesian approximation: Representing model uncertainty in deep learning. In Maria Florina Balcan and Kilian Q. Weinberger, editors, Proceedings of the 33rd international conference on machine learning, volume 48, pages 1050–1059, New York, NY, USA. Proceedings of Machine Learning Research (PLMR).\n\n\nAlon Jacovi and Yoav Goldberg. 2020. Towards faithfully interpretable NLP systems: How should we define and evaluate faithfulness? In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault, editors, Proceedings of the 58th annual meeting of the association for computational linguistics, pages 4198–4205, Online. Association for Computational Linguistics.\n\n\nSarthak Jain and Byron C. Wallace. 2019. Attention is not Explanation. In Jill Burstein, Christy Doran, and Thamar Solorio, editors, Proceedings of the 2019 conference of the north American chapter of the association for computational linguistics: Human language technologies, volume 1 (long and short papers), pages 3543–3556, Minneapolis, Minnesota. Association for Computational Linguistics.\n\n\nJared Kaplan, Sam McCandlish, T. J. Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeff Wu, and Dario Amodei. 2020. Scaling laws for neural language models. ArXiv.\n\n\nNarine Kokhlikyan, Vivek Miglani, Miguel Martin, Edward Wang, Bilal Alsallakh, Jonathan Reynolds, Alexander Melnikov, Natalia Kliushkina, Carlos Araya, Siqi Yan, and Orion Reblitz-Richardson. 2020. Captum: A unified and generic model interpretability library for PyTorch. ArXiv.\n\n\nQuentin Lhoest, Albert Villanova del Moral, Yacine Jernite, Abhishek Thakur, Patrick von Platen, Suraj Patil, Julien Chaumond, Mariama Drame, Julien Plu, Lewis Tunstall, Joe Davison, Mario Šaško, Gunjan Chhablani, Bhavitvya Malik, Simon Brandeis, Teven Le Scao, Victor Sanh, Canwen Xu, Nicolas Patry, et al. 2021. Datasets: A community library for natural language processing. In Heike Adel and Shuming Shi, editors, Proceedings of the 2021 conference on empirical methods in natural language processing: System demonstrations, pages 175–184, Online; Punta Cana, Dominican Republic. Association for Computational Linguistics.\n\n\nSheng Lu, Shan Chen, Yingya Li, Danielle Bitterman, Guergana Savova, and Iryna Gurevych. 2023. Measuring pointwise \\(\\mathcal{V}\\)-usable information in-context-ly. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Findings of the association for computational linguistics: EMNLP 2023, pages 15739–15756, Singapore. Association for Computational Linguistics.\n\n\nScott M. Lundberg and Su-In Lee. 2017. A unified approach to interpreting model predictions. In Proceedings of the 31st international conference on neural information processing systems, volume 30, pages 4768–4777, Long Beach, California, USA. Curran Associates Inc.\n\n\nAndreas Madsen, Nicholas Meade, Vaibhav Adlakha, and Siva Reddy. 2022. Evaluating the faithfulness of importance measures in NLP by recursively masking allegedly important tokens and retraining. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Findings of the association for computational linguistics: EMNLP 2022, pages 1731–1751, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\n\n\nKevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. 2022. Locating and editing factual associations in GPT. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in neural information processing systems, volume 35, pages 17359–17372. Curran Associates, Inc.\n\n\nHosein Mohebbi, Willem Zuidema, Grzegorz Chrupała, and Afra Alishahi. 2023. Quantifying context mixing in transformers. In Andreas Vlachos and Isabelle Augenstein, editors, Proceedings of the 17th conference of the european chapter of the association for computational linguistics, pages 3378–3400, Dubrovnik, Croatia. Association for Computational Linguistics.\n\n\nNeel Nanda. 2023. Attribution patching: Activation patching at industrial scale.\n\n\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. 2019. Fairseq: A fast, extensible toolkit for sequence modeling. In Waleed Ammar, Annie Louis, and Nasrin Mostafazadeh, editors, Proceedings of the 2019 conference of the north American chapter of the association for computational linguistics (demonstrations), pages 48–53, Minneapolis, Minnesota. Association for Computational Linguistics.\n\n\nCharles Pierse. 2021. Transformers interpret.\n\n\nMarcelo OR Prates, Pedro H Avelar, and Luís C Lamb. 2020. Assessing gender bias in machine translation: A case study with Google Translate. Neural Computing and Applications, 32:6363–6381.\n\n\nAlec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. OpenAI Blog.\n\n\nMarco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. \"Why should i trust you?\": Explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, pages 1135–1144, New York, NY, USA. Association for Computing Machinery.\n\n\nRachel Rudinger, Jason Naradowsky, Brian Leonard, and Benjamin Van Durme. 2018. Gender bias in coreference resolution. In Marilyn Walker, Heng Ji, and Amanda Stent, editors, Proceedings of the 2018 conference of the north American chapter of the association for computational linguistics: Human language technologies, volume 2 (short papers), pages 8–14, New Orleans, Louisiana. Association for Computational Linguistics.\n\n\nSoumya Sanyal and Xiang Ren. 2021. Discretized integrated gradients for explaining language models. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Proceedings of the 2021 conference on empirical methods in natural language processing, pages 10285–10299, Online; Punta Cana, Dominican Republic. Association for Computational Linguistics.\n\n\nGabriele Sarti, Nils Feldhus, Jirui Qi, Malvina Nissim, and Arianna Bisazza. 2024. Democratizing advanced attribution analyses of generative language models with the inseq toolkit. In xAI-2024 late-breaking work, demos and doctoral consortium joint proceedings, pages 289–296, Valletta, Malta. CEUR.org.\n\n\nGabriele Sarti, Nils Feldhus, Ludwig Sickert, Oskar van der Wal, Malvina Nissim, and Arianna Bisazza. 2023. Inseq: An interpretability toolkit for sequence generation models. In Danushka Bollegala, Ruihong Huang, and Alan Ritter, editors, Proceedings of the 61st annual meeting of the association for computational linguistics (volume 3: System demonstrations), pages 421–435, Toronto, Canada. Association for Computational Linguistics.\n\n\nBeatrice Savoldi, Marco Gaido, Luisa Bentivogli, Matteo Negri, and Marco Turchi. 2021. Gender bias in machine translation. Transactions of the Association for Computational Linguistics, 9:845–874.\n\n\nAvanti Shrikumar, Peyton Greenside, and Anshul Kundaje. 2017. Learning important features through propagating activation differences. In Doina Precup and Yee Whye Teh, editors, Proceedings of the 34th international conference on machine learning, volume 70, pages 3145–3153. PMLR.\n\n\nKaren Simonyan, Andrea Vedaldi, and Andrew Zisserman. 2014. Deep inside convolutional networks: Visualising image classification models and saliency maps. In Yoshua Bengio and Yann LeCun, editors, 2nd international conference on learning representations, (ICLR), Banff, AB, Canada.\n\n\nMukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017. Axiomatic attribution for deep networks. In Proceedings of the 34th international conference on machine learning (ICML), volume 70, pages 3319–3328, Sydney, Australia. Journal of Machine Learning Research (JMLR).\n\n\nIan Tenney, James Wexler, Jasmijn Bastings, Tolga Bolukbasi, Andy Coenen, Sebastian Gehrmann, Ellen Jiang, Mahima Pushkarna, Carey Radebaugh, Emily Reif, and Ann Yuan. 2020. The language interpretability tool: Extensible, interactive visualizations and analysis for NLP models. In Qun Liu and David Schlangen, editors, Proceedings of the 2020 conference on empirical methods in natural language processing: System demonstrations, pages 107–118, Online. Association for Computational Linguistics.\n\n\nJörg Tiedemann. 2020. The tatoeba translation challenge – realistic data sets for low resource and multilingual MT. In Loïc Barrault, Ondřej Bojar, Fethi Bougares, Rajen Chatterjee, Marta R. Costa-jussà, Christian Federmann, Mark Fishel, Alexander Fraser, Yvette Graham, Paco Guzman, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes, Philipp Koehn, André Martins, Makoto Morishita, Christof Monz, Masaaki Nagata, Toshiaki Nakazawa, et al., editors, Proceedings of the fifth conference on machine translation, pages 1174–1182, Online. Association for Computational Linguistics.\n\n\nKeyon Vafa, Yuntian Deng, David Blei, and Alexander Rush. 2021. Rationales for sequential predictions. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Proceedings of the 2021 conference on empirical methods in natural language processing, pages 10314–10332, Online; Punta Cana, Dominican Republic. Association for Computational Linguistics.\n\n\nJannis Vamvas and Rico Sennrich. 2021. On the limits of minimal pairs in contrastive evaluation. In Jasmijn Bastings, Yonatan Belinkov, Emmanuel Dupoux, Mario Giulianelli, Dieuwke Hupkes, Yuval Pinter, and Hassan Sajjad, editors, Proceedings of the fourth BlackboxNLP workshop on analyzing and interpreting neural networks for NLP, pages 58–68, Punta Cana, Dominican Republic. Association for Computational Linguistics.\n\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in neural information processing systems, volume 30. Curran Associates, Inc.\n\n\nElena Voita, Pavel Serdyukov, Rico Sennrich, and Ivan Titov. 2018. Context-aware neural machine translation learns anaphora resolution. In Iryna Gurevych and Yusuke Miyao, editors, Proceedings of the 56th annual meeting of the association for computational linguistics (volume 1: Long papers), pages 1264–1274, Melbourne, Australia. Association for Computational Linguistics.\n\n\nEric Wallace, Matt Gardner, and Sameer Singh. 2020. Interpreting predictions of NLP models. In Aline Villavicencio and Benjamin Van Durme, editors, Proceedings of the 2020 conference on empirical methods in natural language processing: Tutorial abstracts, pages 20–23, Online. Association for Computational Linguistics.\n\n\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, et al. 2020. Transformers: State-of-the-art natural language processing. In Qun Liu and David Schlangen, editors, Proceedings of the 2020 conference on empirical methods in natural language processing: System demonstrations, pages 38–45, Online. Association for Computational Linguistics.\n\n\nKelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio. 2015. Show, attend and tell: Neural image caption generation with visual attention. In Francis Bach and David Blei, editors, Proceedings of the 32nd international conference on machine learning, volume 37, pages 2048–2057, Lille, France. PMLR.\n\n\nKayo Yin and Graham Neubig. 2022. Interpreting language models with contrastive explanations. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Proceedings of the 2022 conference on empirical methods in natural language processing, pages 184–198, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\n\n\nMuhammad Bilal Zafar, Michele Donini, Dylan Slack, Cedric Archambeau, Sanjiv Das, and Krishnaram Kenthapadi. 2021. On the lack of robust interpretability of neural text classifiers. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, Findings of the association for computational linguistics: ACL-IJCNLP 2021, pages 3730–3740, Online. Association for Computational Linguistics.\n\n\nMatthew D. Zeiler and Rob Fergus. 2014. Visualizing and understanding convolutional networks. In David Fleet, Tomas Pajdla, Bernt Schiele, and Tinne Tuytelaars, editors, 13th european conference on computer vision (ECCV), pages 818–833, Switzerland. Springer International Publishing.\n\n\nZhixue Zhao and Boxuan Shan. 2024. ReAGent: A model-agnostic feature attribution method for generative language models. AAAI Workshop on Responsible Language Models (ReLM).",
    "crumbs": [
      "Attributing Context Usage in Multilingual NLP",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Attributing Language Model Generations with the Inseq Toolkit</span>"
    ]
  },
  {
    "objectID": "chapters/chap-3-inseq.html#footnotes",
    "href": "chapters/chap-3-inseq.html#footnotes",
    "title": "3  Attributing Language Model Generations with the Inseq Toolkit",
    "section": "",
    "text": "We use sequence generation to refer to all iterative tasks, including (but not limited to) natural language generation.↩︎\nhttps://github.com/Textualize/rich↩︎\nWe distinguish between gradient- and internals-based methods to account for their difference in scores’ granularity.↩︎\nSee Section A.1.2 for an example.↩︎\nUsers employing constrained decoding should be aware of its limitations in the presence of a high distributional discrepancy with natural model outputs (Vamvas and Sennrich, 2021).↩︎\nbitsandbytes 0.37.0 required for backward method, see Section A.1.3 for an example.↩︎\nTutorial: https://inseq.org/en/latest/examples/petals.html↩︎\nFor multi-token occupation terms, e.g., bilim insanı (scientist), the first token score was used.↩︎\nWe set \\(\\Delta &lt; 0.05\\) for IG to ensure convergence. Token-level aggregation is performed using the L2 norm.↩︎\nAn example is provided in Section A.1.2.↩︎\nFigure A.3 of Section A.1.3 presents some examples.↩︎",
    "crumbs": [
      "Attributing Context Usage in Multilingual NLP",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Attributing Language Model Generations with the Inseq Toolkit</span>"
    ]
  },
  {
    "objectID": "chapters/chap-4-pecore.html",
    "href": "chapters/chap-4-pecore.html",
    "title": "4  Quantifying Context Usage in Neural Machine Translation",
    "section": "",
    "text": "4.1 Introduction\nResearch in NLP interpretability defines various desiderata for rationales of model behaviors, i.e. the contributions of input tokens toward model predictions computed using input attribution (Madsen et al., 2022). One such property is plausibility, corresponding to the alignment between model rationales and salient input words identified by human annotators (Jacovi and Goldberg, 2020). Low-plausibility rationales typically occur alongside generalization failures or biased predictions and can be helpful in identifying cases where models are “right for the wrong reasons” (McCoy et al., 2019).\nHowever, while plausibility has an intuitive interpretation for classification tasks involving a single prediction, extending this methodology to generative language models presents several challenges. First, LMs have a large output space in which semantically equivalent tokens (e.g. “PC” and “computer”) are competing candidates for next-word prediction (Holtzman et al., 2021). Moreover, LMs’ generations are the product of optimization pressures to ensure independent properties such as semantic relatedness, topical coherence and grammatical correctness, which can hardly be captured by a single attribution score (Yin and Neubig, 2022). Finally, since autoregressive generation involves an iterative prediction process, model rationales could be extracted for every generated token. This raises the issue of which generated tokens can have plausible contextual explanations.\nRecent attribution techniques for explaining language models incorporate contrastive alternatives to disentangle different aspects of model predictions (e.g. the choice of “meowing” over “screaming” for “The cat is ___” is motivated by semantic appropriateness, but not by grammaticality) (Ferrando et al., 2023; Sarti et al., 2023). However, these studies circumvent the issues above by focusing their evaluation on a single generation step matching a phenomenon of interest. For example, given the sentence “The pictures of the cat ___”, a plausible rationale for the prediction of the word “are” should reflect the role of “pictures” in subject-verb agreement. While this approach can be helpful to validate model rationales, it confines plausibility assessment to a small set of handcrafted benchmarks where tokens with plausible explanations are known in advance. Moreover, it risks overlooking important patterns of context usage, including those that do not immediately match linguistic intuitions. In light of this, we suggest that identifying which generated tokens were most affected by contextual input information should be an integral part of plausibility evaluation for language generation tasks.\nTo achieve this goal, we propose a novel interpretability framework, which we dub Plausibility Evaluation of Context Reliance (PECoRe). PECoRe enables the end-to-end extraction of cue-target token pairs consisting of context-sensitive generated tokens and their respective influential contextual cues from language model generations, as shown in Figure 4.1. These pairs can uncover context dependence in naturally occurring generations and, for cases where human annotations are available, help quantify the plausibility of context usage in language models. Importantly, our approach is compatible with modern attribution methods using contrastive targets (Yin and Neubig, 2022), avoids relying on reference translations to avoid problematic distributional shifts (Vamvas and Sennrich, 2021b), and can be applied to unannotated inputs to identify context usage in model generations.\nAfter formalizing our proposed approach in Section 4.3, we apply PECoRe to contextual machine translation to study the plausibility of context reliance in bilingual and multilingual MT models. While PECoRe can easily be used alongside encoder-decoder and decoder-only language models for interpreting context usage in any text generation task, we focus our evaluation on MT because of its constrained output space facilitating automatic assessment and the availability of MT datasets annotated with human rationales of context usage. We thoroughly test PECoRe on well-known discourse phenomena, benchmarking several context sensitivity metrics and attribution methods to identify cue-target pairs. We conclude by applying PECoRe to unannotated examples and showcasing some reasonable and questionable cases of context reliance in MT model translations.1\nIn sum, we make the following contributions:",
    "crumbs": [
      "Attributing Context Usage in Multilingual NLP",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Quantifying Context Usage in Neural Machine Translation</span>"
    ]
  },
  {
    "objectID": "chapters/chap-4-pecore.html#sec-chap4-intro",
    "href": "chapters/chap-4-pecore.html#sec-chap4-intro",
    "title": "4  Quantifying Context Usage in Neural Machine Translation",
    "section": "",
    "text": "Figure 4.1: Examples of sentence-level English\\(\\rightarrow\\)Italian translation with lack-of-context errors and their correct contextual counterpart. In the contextual case context-sensitive source tokens are disambiguated using source (ⓢ) or target-based (ⓣ) contextual cues to produce correct context-sensitive target tokens. PECoRe enables the end-to-end extraction of cue-target pairs (e.g. she-alla pastorella, le pecore-le).\n\n\n\n\n\n\nWe introduce PECoRe, an interpretability framework to detect and attribute context reliance in language models. PECoRe enables a quantitative evaluation of plausibility for language generation beyond the limited artificial settings explored in previous literature.\nWe compare the effectiveness of context sensitivity metrics and input attribution methods for context-aware MT, showing the limitations of metrics currently in use.\nWe apply PECoRe to naturally-occurring translations to identify interesting discourse-level phenomena and discuss issues in the context usage abilities of context-aware MT models.",
    "crumbs": [
      "Attributing Context Usage in Multilingual NLP",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Quantifying Context Usage in Neural Machine Translation</span>"
    ]
  },
  {
    "objectID": "chapters/chap-4-pecore.html#sec-chap4-related-work",
    "href": "chapters/chap-4-pecore.html#sec-chap4-related-work",
    "title": "4  Quantifying Context Usage in Neural Machine Translation",
    "section": "4.2 Related Work",
    "text": "4.2 Related Work\nContext Usage in Language Generation An appropriate2 usage of input information is fundamental in tasks such as summarization (Maynez et al., 2020) to ensure the soundness of generated texts. While appropriateness is traditionally verified post-hoc using trained models (Durmus et al., 2020; Kryscinski et al., 2020; Goyal and Durrett, 2021), recent interpretability works aim to gauge input influence on model predictions using internal properties of language models, such as the mixing of contextual information across model layers (Kobayashi et al., 2020; Ferrando et al., 2022b; Mohebbi et al., 2023) or the layer-by-layer refinement of next token predictions (Geva et al., 2022; Belrose et al., 2023). Recent attribution methods can disentangle factors influencing generation in language models (Yin and Neubig, 2022) and were successfully used to detect and mitigate hallucinatory behaviors (Tang et al., 2022; Dale et al., 2023a; Dale et al., 2023b). Our proposed method adopts this intrinsic perspective to identify context reliance without ad hoc trained components.\nContext Usage in Neural Machine Translation Despite advances in context-aware MT (Voita et al., 2018; Voita et al., 2019; Lopes et al., 2020; Majumder et al., 2022; Jin et al., 2023; inter alia, surveyed by Maruf et al., 2021), only a few works explored whether context usage in MT models aligns with human intuition. Notably, some studies focused on which parts of context inform model predictions, finding that supposedly context-aware MT models are often incapable of using contextual information (Kim et al., 2019; Fernandes et al., 2021) and tend to pay attention to irrelevant words (Voita et al., 2018), with an overall poor agreement between human annotations and model rationales (Yin et al., 2021). Other works instead investigated which parts of generated texts are influenced by context, proposing various contrastive methods to detect gender biases, over- and under-translations (Vamvas and Sennrich, 2021a; Vamvas and Sennrich, 2022), and to identify various discourse-level phenomena in MT corpora (Fernandes et al., 2023). While these two directions have generally been investigated separately, our work proposes a unified framework to enable an end-to-end evaluation of context-reliance plausibility in language models.\nPlausibility evaluation in NLP Plausibility evaluation for NLP models has primarily focused on classification models (DeYoung et al., 2020; Atanasova et al., 2020; Attanasio et al., 2023). While few works investigate plausibility in language generation (Vafa et al., 2021; Ferrando et al., 2023), such evaluations typically involve a single generation step to complete a target sentence with a token connected to preceding information (e.g. subject/verb agreement, as in “The pictures of the cat [is/are]”), effectively biasing the evaluation by using a pre-selected token of interest. On the contrary, our framework proposes a more comprehensive evaluation of generation plausibility that includes the identification of context-sensitive generated tokens as an important prerequisite.",
    "crumbs": [
      "Attributing Context Usage in Multilingual NLP",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Quantifying Context Usage in Neural Machine Translation</span>"
    ]
  },
  {
    "objectID": "chapters/chap-4-pecore.html#sec-chap4-pecore-framework",
    "href": "chapters/chap-4-pecore.html#sec-chap4-pecore-framework",
    "title": "4  Quantifying Context Usage in Neural Machine Translation",
    "section": "4.3 The PECoRe Framework",
    "text": "4.3 The PECoRe Framework\nPECoRe is a two-step framework for identifying context dependence in generative language models. First, context-sensitive tokens identification (CTI) selects which tokens among those generated by the model were influenced by the presence of the preceding context (e.g. the feminine options “alla pastorella, le” in Figure 4.1). Then, contextual cues imputation (CCI) attributes the prediction of context-sensitive tokens to specific cues in the provided context (e.g. the feminine cues “she, Le pecore” in Figure 4.1). Cue-target pairs formed by influenced target tokens and their respective influential context cues can then be compared to human rationales to assess the models’ plausibility of context reliance for contextual phenomena of interest. Figure 4.2 provides an overview of the two steps applied to the context-aware MT setting discussed by this work. A more general formalization of the framework for language generation is proposed in the following sections.\n\n\n\n\n\n\nFigure 4.2: The PECoRe framework applied to an encoder-decoder MT model. Left: Context-sensitive token identification (CTI). ⓵: A context-aware MT model translates source context (\\(C_x\\)) and current (\\(x\\)) sentences into target context (\\(C_{\\hat y}\\)) and current (\\(\\hat y\\)) outputs. ⓶: \\(\\hat y\\) is force-decoded in the non-contextual setting instead of natural output \\(\\tilde y\\). ⓷: Contrastive metrics are collected throughout the model for every \\(\\hat y\\) token to compare the two settings. ⓸: Selector \\(s_\\text{cti}\\) maps metrics to binary context-sensitive labels for every \\(\\hat y_i\\). Right: Contextual cues imputation (CCI). ⓵: Non-contextual target \\(\\tilde y^*\\) is generated from contextual prefix \\(\\hat y_{&lt;t}\\). ⓶: Function \\(f_\\text{tgt}\\) is selected to contrast model predictions with (\\(\\hat y_t\\)) and without (\\(\\tilde y_t^*\\)) input context. ⓷: Attribution method \\(f_\\text{att}\\) using \\(f_\\text{tgt}\\) as target scores contextual cues driving \\(\\hat y_t\\) prediction. ⓸: Selector \\(s_\\text{cci}\\) selects relevant cues, and cue-target pairs are assembled.\n\n\n\n\n4.3.1 Notation\nLet \\(X_\\text{ctx}^{i}\\) be the sequence of contextual inputs containing \\(N\\) tokens from vocabulary \\(\\mathcal{V}\\), composed by current input \\(x\\), generation prefix \\(y_{&lt;i}\\) and context \\(C\\). Let \\(X_\\text{no-ctx}^{i}\\) be the non-contextual input in which \\(C\\) tokens are excluded.3 \\(P_\\text{ctx}^{i} = P\\left(x,\\, y_{&lt;i},\\,C,\\,\\theta\\right)\\) is the discrete probability distribution over \\(\\mathcal{V}\\) at generation step \\(i\\) of a language model with \\(\\theta\\) parameters receiving contextual inputs \\(X_\\text{ctx}^{i}\\). Similarly, \\(P_\\text{no-ctx}^{i} = P\\left(x,\\, y_{&lt;i},\\,\\theta\\right)\\) is the distribution obtained from the same model for non-contextual input \\(X_\\text{no-ctx}^{i}\\). Both distributions are equivalent to vectors in the probability simplex in \\(\\mathbb{R}^{|\\mathcal{V}|}\\), and we use \\(P_\\text{ctx}(y_i)\\) to denote the probability of next token \\(y_i\\) in \\(P_\\text{ctx}^{i}\\), i.e. \\(P(y_i\\,|\\,x,\\,y_{&lt;i},\\,C)\\).\n\n\n4.3.2 Context-sensitive Token Identification (CTI)\nCTI adapts the contrastive conditioning paradigm proposed by Vamvas and Sennrich (2021a) to detect input context influence on model predictions using the contrastive pair \\(P_\\text{ctx}^{i}, P_\\text{no-ctx}^{i}\\). Both distributions are relative to the contextual target sentence \\(\\hat y = \\{\\hat y_1 \\dots \\hat y_n\\}\\), corresponding to the sequence produced by a decoding strategy of choice in the presence of input context. In Figure 4.2, the contextual target sentence \\(\\hat y=\\) “Sont-elles à l’hôtel?” is generated when \\(x\\) and contexts \\(C_x, C_{\\hat y}\\) are provided as inputs, while non-contextual target sentence \\(\\tilde y =\\) “Ils sont à l’hôtel?” would be produced when only \\(x\\) is provided. In the latter case, \\(\\hat y\\) is instead force-decoded from the non-contextual setting to enable a direct comparison of matching outputs. We define a set of contrastive metrics \\(\\mathcal{M} = \\{m_1, \\dots, m_M\\}\\), where each \\(m: \\displaystyle \\Delta_{|\\mathcal{V}|} \\times \\Delta_{|\\mathcal{V}|} \\mapsto \\mathbb{R}\\) maps a contrastive pair of probability vectors to a continuous score. For example, the difference in next token probabilities for contextual and non-contextual settings, i.e. \\(P_\\text{diff}(\\hat y_i) = P_\\text{ctx}(\\hat y_i) - P_\\text{no-ctx}(\\hat y_i)\\), might be used for this purpose.4 Target tokens with high contrastive metric scores can be identified as context-sensitive, provided \\(C\\) is the only added parameter in the contextual setting. Finally, a selector function \\(s_\\text{cti}: \\displaystyle \\mathbb{R}^{| \\mathcal{M} |} \\mapsto \\{0,1\\}\\) (e.g. a statistical threshold selecting salient scores) is used to classify every \\(\\hat y_i\\) as context-sensitive or not.\n\n\n4.3.3 Contextual Cues Imputation (CCI)\nCCI applies the contrastive attribution paradigm (Yin and Neubig, 2022) to trace the generation of every context-sensitive token in \\(\\hat y\\) back to the context \\(C\\), identifying the cues that drive model predictions.\n\nDefinition 4.1 Let \\(s, s'\\) be the resulting scores of two attribution target functions \\(f_\\text{tgt}, f'_\\text {tgt}\\). An attribution method \\(f_\\text{att}\\) is if importance scores \\(A\\) are computed in relation to the outcome of its attribution target function, i.e. whenever the following condition is verified.\n\\[f_\\text{att}(x, y_{&lt;t}, C, \\theta, s) \\neq f_\\text{att}(x, y_{&lt;t}, C, \\theta, s') \\;\\; \\forall s \\neq s'\\]\n\nIn practice, common gradient-based attribution approaches (Simonyan et al., 2014; Sundararajan et al., 2017) are target-dependent as they rely on the outcome predicted by the model (typically the logit or the probability of the predicted class) as the differentiation target to backpropagate importance to model input features. Similarly, perturbation-based approaches (Zeiler and Fergus, 2014) use the variation in prediction probability for the predicted class when noise is added to some of the model inputs to quantify the importance of the noised features.\nOn the contrary, recent approaches that rely solely on model internals to define input importance are generally target-insensitive. For example, attention weights used as model rationales, either in their raw form or after a rollout procedure to obtain a unified score (Abnar and Zuidema, 2020), are independent of the predicted outcome. Similarly, value zeroing scores (Mohebbi et al., 2023) reflect only the representational dissimilarity across model layers before and after zeroing value vectors, and as such do not explicitly account for model predictions.\n\nDefinition 4.2 Let \\(\\mathcal{T}\\) be the set of indices corresponding to context-sensitive tokens identified by the CTI step, such that \\(t \\in \\hat y\\) and \\(\\forall t \\in \\mathcal{T}, s_\\text{cti}(m_1^{t}, \\dots, m_M^{t}) = 1\\). Let also \\(f_\\text{tgt}: \\Delta_{|\\mathcal{V}|} \\times \\dots \\mapsto \\mathbb{R}\\) be a contrastive attribution target function representing an attribution target of interest, for example, the difference in next-token probabilities between the contextual option \\(\\hat y_t\\) and the non-contextual option \\(\\tilde y^*_t\\) from the same contextual distribution \\(P_\\text{ctx}^{t}\\), plus any additional required parameter. The contrastive attribution method \\(f_\\text{att}\\) is a composite function quantifying the importance of contextual inputs to determine the output of \\(f_\\text{tgt}\\) for a given model with \\(\\theta\\) parameters.\n\n\\[f_\\text{att}(\\hat y_{t}) = f_\\text{att}(x, \\hat y_{&lt;t}, C, \\theta, f_\\text{tgt}) = f_\\text{att}\\big(x, \\hat y_{&lt;t}, C, \\theta, f_\\text{tgt}(P_\\text{ctx}^t, \\dots)\\big)\\]\n\nRemark 4.1. The non-contextual next token \\(\\tilde y^*_t\\) can be computed using the contextual prefix \\(\\hat y_{&lt;t} = \\{ \\hat y_1, \\dots, \\hat y_{t - 1}\\}\\) (e.g. \\(\\hat y_{&lt;t} =\\)“Sont-” in Figure 4.2) and non-contextual inputs \\(X_\\text{no-ctx}^{t}\\). This is conceptually equivalent to predicting the next token of a new non-contextual sequence \\(\\tilde y^*\\) which, contrary to the original \\(\\tilde y\\), starts from a forced contextual prefix \\(\\hat y_{&lt;t}\\) (e.g. “ils” in \\(\\tilde y^* =\\) “ils à l’hôtel?” in Figure 4.2).\n\n\nRemark 4.2. A \\(f_\\text{tgt}\\) making use of both \\(P_\\text{ctx}^{t}\\) and \\(P_\\text{no-ctx}^{t}\\), e.g. the KL divergence between the contextual and non-contextual probability distributions (Kullback and Leibler, 1951), can ultimately result in non-zero \\(f_\\text{att}(\\hat y_t)\\) scores, even when \\(\\hat y_t = \\tilde y^*_t\\), i.e. even when the next predicted token is the same, since probabilities \\(P_\\text{ctx}(\\hat y_t), P_{no-ctx}(\\tilde y^*_t)\\) are likely to differ beyond top-1 predictions. This is a desirable property of \\(f_\\text{att}\\), as it allows the attribution method to capture the influence of context on the model’s decision-making process, even in the case where the predicted token remains unchanged.\n\n\nRemark 4.3. Our formalization of \\(f_\\text{att}\\) generalizes the method proposed by Yin and Neubig (2022) to support any target-dependent attribution method, such as popular gradient-based approaches (Simonyan et al., 2014; Sundararajan et al., 2017), and any contrastive attribution target \\(f_\\text{tgt}\\).\n\n\\(f_\\text{att}\\) produces a sequence of attribution scores \\(A_t = \\{a_1, \\dots, a_N\\}\\) matching contextual input length \\(N\\). From those, only the subset \\(A_{t\\,\\text{ctx}}\\) of scores corresponding to context input sequence \\(C\\) are passed to selector function \\(s_\\text{cci}: \\displaystyle \\mathbb{R} \\mapsto \\{0,1\\}\\), which predicts a set \\(\\mathcal{C}_{t}\\) of indices corresponding to contextual cues identified by CCI, such that \\(\\forall c \\in \\mathcal{C}_t, \\forall a \\in A_{t\\,\\text{ctx}}, s_\\text{cci}(a_{c}) = 1\\).\nHaving collected all context-sensitive generated token indices \\(\\mathcal{T}\\) using CTI and their contextual cues through CCI (\\(C_t\\)), PECoRe ultimately returns a sequence \\(S_\\text{ct}\\) of all identified cue-target pairs:\n\\[\n\\begin{aligned}\n\\mathcal{T} &= \\text{CTI}(C, x, \\hat y, \\theta, \\mathcal{M}, s_\\text{cti}) = \\{t \\;|\\; s_\\text{cti}(m_1^t, \\dots, m_M^t) = 1 \\} \\\\\n\\mathcal{C} &= \\text{CCI}(\\mathcal{T}, C, x, \\hat y, \\theta,  f_\\text{att}, f_\\text{tgt}, s_\\text{cci}) = \\{ c \\;|\\; s_\\text{cci}(a_c) = 1 \\,\\forall a_c \\in A_{t\\,\\text{ctx}}, \\forall t \\in \\mathcal{T}\\} \\\\\nS &= \\texttt{PECoRe}(C, x, \\theta, s_\\text{cti}, s_\\text{cci}, \\mathcal{M}, f_\\text{att}, f_\\text{tgt}) = \\{ (C_c, \\hat y_t) \\;|\\; \\forall t \\in \\mathcal{T}, \\forall c \\in \\mathcal{C}_t, \\forall \\mathcal{C}_t \\in \\mathcal{C} \\}\n\\end{aligned}\n\\]\nA pseudocode implementation for the PECoRe algorithm is provided in Algorithm 1.\n\n\n\\begin{algorithm} \\caption{PECoRe cue-target extraction process} \\begin{algorithmic} \\Require $C, x$ (Input context and current sequences), $\\theta$ (Model parameters), $s_{\\text{cti}}, s_{\\text{cci}}$ (Selector functions), $\\mathcal{M}$ (Contrastive metrics), $f_\\text{att}$ (Contrastive attribution method), $f_\\text{tgt}$ (Contrastive attribution target function) \\Procedure{PECoRe}{$C, x, \\theta, s_\\text{cti}, s_\\text{cci}, \\mathcal{M}, f_\\text{att}, f_\\text{tgt}$} \\State $\\hat y = \\textnormal{generate(}C, x, \\theta$) using any decoding strategy and parameters \\State $\\mathcal{T} = \\textnormal{CTI(}C, x, \\hat y, \\theta, \\mathcal{M}, s_\\text{cti}\\textnormal{)}$ \\ForAll{$t \\in \\mathcal{T}$} \\State $\\mathcal{C}_t = \\textnormal{CCI(}t, C, x, \\hat y, \\theta, f_\\text{att}, f_\\text{tgt}, s_\\text{cci}\\textnormal{)}$ \\ForAll{$c \\in \\mathcal{C}_t$} \\State Store $(C_t^c, \\hat y_t)$ in $S_\\text{ct}$ \\EndFor \\EndFor \\State \\textbf{return} $S_\\text{ct}$ // Set of cue-target pairs \\EndProcedure \\Procedure{CTI}{$C, x, \\hat y, \\theta, \\mathcal{M}, s_\\text{cti}$} \\State $\\mathcal{T} = \\emptyset$ // Empty set for context-sensitive indices of $\\hat y$ tokens \\ForAll{$\\hat{y}_i \\in \\hat{y}$} \\ForAll{$m \\in \\mathcal{M}$} \\State $m^i = m \\big(P_{\\text{ctx}}(\\hat{y}_i), P_{\\text{no-ctx}}(\\hat{y}_i) \\big)$ \\EndFor \\If{$(s_{\\text{cti}}(m_1^i, \\dots, m_M^i) = 1$)} \\State Store $i$ in set $\\mathcal{T}$ \\EndIf \\EndFor \\State \\textbf{return} $\\mathcal{T}$ \\EndProcedure \\Procedure{CCI}{$t, C, x, \\hat y, \\theta, f_\\text{att}, f_\\text{tgt}, s_\\text{cci}$} \\State $\\mathcal{C}_t = \\emptyset$ // Empty set for contextual cues for target token $t$ \\State Generate constrained non-contextual target current sequence $\\tilde y^*$ from $\\hat y_{&lt;t}$ \\State Use attribution method $f_\\text{att}$ with target $f_\\text{tgt}$ to get importance scores $A_t$ \\State Identify the subset $A_{t\\,\\text{ctx}}$ corresponding to tokens of context $C = \\{ C_1, \\dots, C_K\\}$ \\ForAll{$a_i \\in A_{t\\,\\text{ctx}} = \\{a_1, \\dots, a_K\\}$} \\If{$s_\\text{cci}(a_i) = 1$} \\State Store $C_i$ in $\\mathcal{C}_t$ \\EndIf \\EndFor \\State \\textbf{return} $\\mathcal{C}_t$ \\EndProcedure \\end{algorithmic} \\end{algorithm}",
    "crumbs": [
      "Attributing Context Usage in Multilingual NLP",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Quantifying Context Usage in Neural Machine Translation</span>"
    ]
  },
  {
    "objectID": "chapters/chap-4-pecore.html#context-reliance-plausibility-in-context-aware-mt",
    "href": "chapters/chap-4-pecore.html#context-reliance-plausibility-in-context-aware-mt",
    "title": "4  Quantifying Context Usage in Neural Machine Translation",
    "section": "4.4 Context Reliance Plausibility in Context-aware MT",
    "text": "4.4 Context Reliance Plausibility in Context-aware MT\nThis section describes our evaluation of PECoRe in a controlled setup. We experiment with several contrastive metrics and attribution methods for CTI and CCI (Section 4.4.2, Section 4.4.5), evaluating them in isolation to quantify the performance of individual components. An end-to-end evaluation is also performed in Section 4.4.5 to establish the applicability of PECoRe in a naturalistic setting.\n\n4.4.1 Experimental Setup\nEvaluation Datasets Evaluating generation plausibility requires human annotations for context-sensitive tokens in target sentences and disambiguating cues in their preceding context. To our knowledge, the SCAT dataset (Yin et al., 2021) is the only resource matching these requirements. SCAT is an English\\(\\rightarrow\\)French corpus with human annotations of anaphoric pronouns and disambiguating context on OpenSubtitles2018 dialogue translations (Lison et al., 2018; Lopes et al., 2020). SCAT examples were extracted automatically using lexical heuristics and thus contain only a limited set of anaphoric pronouns (it, they \\(\\rightarrow\\) il/elle, ils/elles), with no guarantees of contextual cues being found in preceding context.\nThe original SCAT test set contains 1000 examples with automatically identified context-sensitive pronouns it/they (marked by &lt;p&gt;...&lt;/p&gt;) and human-annotated contextual cues aiding their disambiguation (marked by &lt;hon&gt;...&lt;/hoff&gt;). Of these, we find 38 examples containing malformed tags and several more examples where an unrelated word containing it or they was wrongly marked as context-sensitive (e.g. the soccer ball h&lt;p&gt;it&lt;/p&gt; your chest). Moreover, due to the original extraction process adopted for SCAT, there is no guarantee that contextual cues will be contained in the preceding context, as they could also appear in the same sentence, defeating the purpose of our context usage evaluation. Thus, we prefilter the entire corpus to retain only sentences with well-formed tags and inter-sentential contextual cues identified by the original annotators. Moreover, a manual inspection procedure is carried out to validate the original cue tags and discard problematic sentences, obtaining a final set of 250 examples with inter-sentential pronoun coreference, which we name SCAT+5.\nAdditionally, we manually annotate contextual cues in DiscEval-MT (Bawden et al., 2018), another English\\(\\rightarrow\\)French corpus containing handcrafted examples for anaphora resolution (ana) and lexical choice (lex). In the case of DiscEval-MT, we use minimal pairs in the original dataset to automatically mark differing tokens as context-sensitive. Then, contextual cues are manually labeled separately by two annotators with good familiarity with both English and French. Cue annotations are compared across the two splits, resulting in very high agreement due the simplicity of the corpus (\\(97\\%\\) overlap for ana, \\(90\\%\\) for lex).6\nOur final evaluation set contains 250 SCAT+ and 400 DiscEval-MT translations across two discourse phenomena. Table 4.1 provides some examples for the three data splits.\n\n\n\n\n\n\nSCAT+\n\n\n\\(C_x\\): I loathe that song. But why did you bite poor Birdie’s head off? Because I’ve heard it more times than I care to. It haunts me. Just stop, for a moment.\n\n\n\\(C_y\\): Je hais cette chanson (song, feminine). Mais pourquoi avoir parlé ainsi à la pauvre Birdie ? Parce que j’ai entendu ce chant plus que de fois que je ne le peux. Elle (she) me hante. Arrêtez-vous un moment.\n\n\n\\(x\\): How does it haunt you?\n\n\n\\(y\\): Comment peut-elle (she) vous hanter?\n\n\n\\(C_x\\): - Ah! Sven! It’s been so long. - Riley, it’s good to see you. - You, too. How’s the boat? Uh, it creaks, it groans.\n\n\n\\(C_y\\): Sven ! - Riley, contente de te voir. - Content aussi. Comment va le bateau (boat, masculine)? Il (he) craque de partout.\n\n\n\\(x\\): Not as fast as it used to be.\n\n\n\\(y\\): Il (he) n’est pas aussi rapide qu’avant.\n\n\nDiscEval-MT ana\n\n\n\\(C_x\\): But how do you know the woman isn’t going to turn out like all the others?\n\n\n\\(C_y\\): Mais comment tu sais que la femme (woman, feminine) ne finira pas comme toutes les autres?\n\n\n\\(x\\): This one’s different.\n\n\n\\(y\\): Celle-ci (This one, feminine) est différente.\n\n\n\\(C_x\\): Can you authenticate these signatures, please?\n\n\n\\(C_y\\): Pourriez-vous authentifier ces signatures (feminine), s’il vous plaît?\n\n\n\\(x\\): Yes, they’re mines.\n\n\n\\(y\\): Oui, ce sont les miennes (mines, feminine).\n\n\nDiscEval-MT lex\n\n\n\\(C_x\\): Do you think you can shoot it from here?\n\n\n\\(C_y\\): Tu penses que tu peux le tirer (shoot) dessus à partir d’ici?\n\n\n\\(x\\): Hand me that bow.\n\n\n\\(y\\): Passe-moi cet arc (bow, weapon).\n\n\n\\(C_x\\): Can I help you with the wrapping?\n\n\n\\(C_y\\): Est-ce que je peux t’aider pour l’emballage (wrapping)?\n\n\n\\(x\\): Hand me that bow.\n\n\n\\(y\\): Passe-moi ce ruban (bow, gift wrap).\n\n\n\n\n\nTable 4.1: Examples from the SCAT+ and DiscEval-MT datasets used in our analysis with highlighted context-sensitive tokens and contextual cues used for plausibility evaluation using PECoRe. Glosses are added for French words of interest to facilitate understanding.\n\n\n\nModels We evaluate two bilingual Opus models (Tiedemann and Thottingal, 2020) using the transformer base architecture (Vaswani et al., 2017, Small and Large), and mBART-50 1-to-many (Tang et al., 2021), a larger multilingual MT model supporting 50 target languages, using the 🤗 transformers library (Wolf et al., 2020). We fine-tune models using extended translation units (Tiedemann and Scherrer, 2017) with contextual inputs marked by break tags such as source context &lt;brk&gt; source current to produce translations in the format target context &lt;brk&gt; target current, where context and current target sentences are generated. We perform context-aware fine-tuning on 242k IWSLT 2017 English\\(\\rightarrow\\)French examples (Cettolo et al., 2017), using a dynamic context size of 0-4 preceding sentences to ensure robustness to different context lengths and allow contextless usage. To further improve models’ context sensitivity, we continue fine-tuning on the SCAT training split, containing 11k examples with inter- and intra-sentential pronoun anaphora.\n\n\n\n\n\n\nModel\nSCAT+\nDiscEval-MT (ana)\nDiscEval-MT (lex)\n\n\nbleu\nok\nok-cs\nbleu\nok\nok-cs\nbleu\nok\nok-cs\n\n\n\n\nOpus Small (def.)\n29.1\n0.14\n-\n43.9\n0.40\n-\n30.5\n0.29\n-\n\n\nOpus Small S+T\\(_{\\text{ctx}}\\)\n39.1\n0.81\n0.59\n48.1\n0.60\n0.24\n33.5\n0.36\n0.07\n\n\nOpus Large (def.)\n29.0\n0.16\n-\n39.2\n0.41\n-\n31.2\n0.31\n-\n\n\nOpus Large S+T\\(_{\\text{ctx}}\\)\n40.3\n0.83\n0.58\n48.9\n0.68\n0.31\n34.8\n0.38\n0.10\n\n\nmBART-50 (def.)\n23.8\n0.26\n-\n33.4\n0.42\n-\n24.5\n0.25\n-\n\n\nmBART-50 S+T\\(_{\\text{ctx}}\\)\n37.6\n0.82\n0.55\n49.0\n0.62\n0.32\n29.3\n0.30\n0.07\n\n\n\n\n\nTable 4.2: Translation quality of English\\(\\rightarrow\\)French MT models before (def.) and after (S+T\\(_\\text{ctx}\\)) context-aware MT fine-tuning. ok: % of translations with correct disambiguation for discourse phenomena. ok-cs: % of translations where the correct disambiguation is achieved only when context is provided.\n\n\n\nModel Disambiguation Accuracy We estimate contextual disambiguation accuracy by verifying whether annotated (gold) context-sensitive words are found in model outputs. Results before and after context-aware fine-tuning are shown in Table 4.2. We find that fine-tuning improves translation quality and disambiguation accuracy across all tested models, with larger gains for anaphora resolution datasets that closely match the fine-tuning data. To gain further insight into these results, we use context-aware models to translate examples with and without context and identify a subset of context-sensitive translations (ok-cs) for which the correct target word is generated only when input context is provided to the model. Interestingly, we find a non-negligible amount of translations that are correctly disambiguated even in the absence of input context (corresponding to ok minus ok-cs in Table 4.2). For these examples, the correct prediction of ambiguous words aligns with model biases, such as defaulting to masculine gender for anaphoric pronouns (Stanovsky et al., 2019) or using the most frequent sense for word sense disambiguation. Provided that such examples are unlikely to exhibit context reliance, we focus particularly on the ok-cs subset results in our following evaluation.\n\n\n4.4.2 Metrics for Context-sensitive Target Identification\nThe following contrastive metrics are evaluated for detecting context-sensitive tokens in the CTI step.\nRelative Context Saliency We use contrastive gradient norm attribution (Yin and Neubig, 2022) to compute input importance towards predicting the next token \\(\\hat y_i\\) with and without input context. Positive importance scores are obtained for every input token using the L2 gradient vectors norm (Bastings et al., 2022), and relative context saliency is obtained as the proportion between the normalized importance for context tokens \\(c \\in C_x, C_y\\) and the overall input importance, following previous work quantifying MT input contributions (Voita et al., 2021; Ferrando et al., 2022a; Edman et al., 2024).\n\\[\\nabla_\\text{ctx} (P_\\text{ctx}^{i}, P_\\text{no-ctx}^{i}) = \\frac{\\sum_{c \\in C_x, C_y} \\big\\| \\nabla_c \\big( P_\\text{ctx}(\\hat y_i) - P_\\text{no-ctx}(\\hat y_i) \\big) \\big\\|}{\\sum_{t \\in X_\\text{ctx}^{i}} \\big\\| \\nabla_t \\big( P_\\text{ctx}(\\hat y_i) - P_\\text{no-ctx}(\\hat y_i) \\big) \\big\\|}\\]\nLikelihood Ratio (LR) and Pointwise Contextual Cross-mutual Information (P-CXMI) Proposed by Vamvas and Sennrich (2021a) and Fernandes et al. (2023), respectively, both metrics frame context dependence as a ratio of contextual and non-contextual probabilities.\n\\[\\text{LR}(P_\\text{ctx}^{i}, P_\\text{no-ctx}^{i}) = \\frac{P_\\text{ctx}(\\hat{y}_i)}{P_\\text{ctx}(\\hat{y}_i) + P_\\text{no-ctx}(\\hat{y}_i)}\\]\n\\[\\text{P-CXMI}(P_\\text{ctx}^{i}, P_\\text{no-ctx}^{i}) = - \\log \\frac{P_\\text{ctx}(\\hat{y}_i)}{P_\\text{no-ctx}(\\hat{y}_i)}\\]\nKL-Divergence (Kullback and Leibler, 1951) between \\(P_\\text{ctx}^{i}\\) and \\(P_\\text{no-ctx}^{i}\\) is the only metric we evaluate that considers the full distribution rather than the probability of the predicted token. We include it to test the intuition that the impact of context inclusion might extend beyond top-1 token probabilities.\n\\[D_\\text{KL}(P_\\text{ctx}^{i} \\| P_\\text{no-ctx}^{i}) = \\sum_{\\hat{y}_i \\in \\mathcal{V}} P_\\text{ctx}(\\hat{y}_i) \\log \\frac{P_\\text{ctx}(\\hat{y}_i)}{P_\\text{no-ctx}(\\hat{y}_i)}\\]\n\n\n4.4.3 Plausibility Evaluation Metrics\nIn practice, the CTI and CCI steps in PECoRe produce a sequence of continuous scores that are later binarized using selectors \\(s_\\text{cti}, s_\\text{cci}\\), introduced in Section 4.3. To evaluate their validity, those are compared to a sequence \\(I_h\\) of the same length containing binary values, where 1s correspond to the cues identified by human annotators, while the rest are set to 0. In our experiments, we use two standard plausibility metrics introduced by DeYoung et al. (2020):\nToken-level Macro F1 is the harmonic mean of precision and recall at the token level, using \\(I_h\\) as the ground truth and the post-selector binarized scores as predictions. Macro-averaging is used to account for the sparsity of cues in \\(I_h\\). We use this metric in our primary analysis, as the discretization step is more likely to reflect realistic plausibility performance, since it matches more closely the annotation process used to derive \\(I_h\\). We note that Macro F1 can be considered a lower bound for plausibility, as the results depend heavily on the choice of the selector used for discretization.\nArea Under Precision-Recall Curve (AUPRC) is computed as the area under the curve obtained by varying a threshold over token importance scores and computing the precision and recall for resulting discretized \\(I_m\\) predictions while keeping \\(I_h\\) as the ground truth. Contrary to Macro F1, AUPRC is selector-independent and accounts for tokens’ relative ranking and degree of importance. Consequently, it acts as an upper bound for plausibility, as if the optimal selector was used. Results using AUPRC are presented in Section A.2.2 for completeness, but we focus on Macro F1 in the primary analysis.\n\n\n4.4.4 CTI Plausibility Results\nFigure 4.3 presents our metrics evaluation for CTI, with results for the full test sets and the subsets of context-sensitive sentences (ok-cs) highlighted in Table 4.2. To keep our evaluation simple, we use a naive \\(s_\\text{cti}\\) selector tagging all tokens with metric scores one standard deviation above the per-example mean as context-sensitive. We also include a stratified random baseline matching the frequency of occurrence of context-sensitive tokens in each dataset. Datapoints in Figure 4.3 are sentence-level macro F1 scores computed for every dataset example.\n\n\n\n\n\n\nFigure 4.3: Macro F1 of contrastive metrics for context-sensitive target token identification (CTI) using Opus Large on the full datasets (left) or on ok-cs context-sensitive subsets (right).\n\n\n\nPointwise metrics (LR, P-CXMI) show high plausibility for the context-sensitive subsets ok-cs across all datasets and models, but achieve lower performances on the full test set, especially for lexical choice phenomena less present in MT models’ training. KL-Divergence performs on par with or better than pointwise metrics, suggesting that distributional shifts beyond top prediction candidates can provide helpful information for detecting context sensitivity. On the contrary, the poor performance of context saliency suggests that aggregate context reliance cannot reliably predict context sensitivity. A manual examination of misclassified examples reveals several context-sensitive tokens that were not annotated as such, as they did not match the dataset’s phenomena of interest, but were still identified by CTI metrics. Table 4.3 presents several examples illustrating the contextual influence of French pronoun formality, whereas SCAT+ examples focus solely on gender disambiguation for anaphoric pronouns. This suggests that our evaluation of CTI metrics’ plausibility can be considered a lower bound for actual method accuracy, as it is restricted to the two phenomena available in the datasets we used (anaphora resolution and lexical choice), rather than the broad set of contextual dependence phenomena. These results further underscore the importance of data-driven, end-to-end approaches like PECoRe in limiting the influence of selection bias during evaluation.\n\n\n\n\n\n\nPronoun Grammatical Formality, SCAT+\n\n\n\\(C_x\\): […] That demon that was in you, it wants you. But not like before. I think it loves you.\n\n\n\\(C_y\\): […] Ce démon qui était en vous, il vous veut. Mais pas comme avant. Je pense qu’il vous aime.\n\n\n\\(x\\): And it’s powerless without you.\n\n\n\\(y\\): Et il est impuissant sans vous (you, 2nd p. plur., formal).\n\n\n\\(C_x\\): You threaten my father again, I’ll kill you myself… on this road. You hear me?\n\n\n\\(C_y\\): Tu menaces encore mon père, je te tuerai moi-même… sur cette route. Tu m’entends?\n\n\n\\(x\\): Now it is with you as well.\n\n\n\\(y\\): Maintenant elle est aussi avec toi (you, 2nd p. sing., informal).\n\n\n\\(C_x\\): She went back to Delhi. What do you think? […] Girls, I tell you.\n\n\n\\(C_y\\): Elle est revenue à Delhi. Qu’en penses-tu? […] Les filles, je te le dis.\n\n\n\\(x\\): I wish they were all like you.\n\n\n\\(y\\): J’aimerais qu’elles soient toutes comme toi (you, 2nd p. sing., informal).\n\n\n\n\n\nTable 4.3: Examples of SCAT+ sentences with context-sensitive target tokens identified by CTI but not originally labeled as context-dependent in the dataset, since they do not match the gendered pronoun rule match used to create SCAT+. Relevant formality contextual cues are highlighted, and glosses are added for French words of interest to facilitate understanding.\n\n\n\n\n\n4.4.5 Methods for Contextual Cues Imputation\nThe following attribution methods are evaluated for detecting contextual cues in the CCI step.\nContrastive Gradient Norm (Yin and Neubig, 2022) estimates the input tokens’ contributions towards predicting a target token, rather than a contrastive alternative. We use this method to explain the generation of context-sensitive tokens in the presence and absence of context.\n\\[A_{t\\,\\text{ctx}} = \\{\\,\\| \\nabla_c \\big(f_\\text{tgt}(P_\\text{ctx}^{i}, \\dots) \\big)\\|\\,|\\, \\forall c \\in C\\}\\]\nFor the choice of \\(f_\\text{tgt}\\), we evaluate both probability difference \\(P_\\text{ctx}(\\hat y_i) - P_\\text{no-ctx}(\\hat y_i)\\), conceptually similar to the original formulation, and the KL-Divergence of contextual and non-contextual distributions \\(D_\\text{KL}(P_\\text{ctx}^{i} \\| P_\\text{no-ctx}^{i})\\). We use \\(\\nabla_\\text{diff}\\) and \\(\\nabla_\\text{KL}\\) to identify gradient norm attribution in the two settings. \\(\\nabla_\\text{KL}\\) scores can be seen as the contribution of input tokens towards the shift in probability distribution caused by the presence of input context.7\nAttention Weights Following previous work, we use the mean attention weight across all heads and layers (Attention Mean, Kim et al. (2019)) and the weight for the head obtaining the highest plausibility per-dataset (Attention Best, Yin et al. (2021)) as importance measures for CCI. Attention Best can be seen as a best-case estimate of attention performance but is not a viable metric in real settings, provided that the best attention head to capture a phenomenon of interest is unknown beforehand. Since attention weights are model byproducts unaffected by predicted outputs, we use only attention scores for the contextual setting \\(P_\\text{ctx}^{i}\\) and ignore the contextless alternative when using these metrics.\n\n\n4.4.6 CCI Plausibility Results\n\n\n\n\n\n\nFigure 4.4: Macro F1 of CCI methods over full datasets using Opus Large models trained with only source context (left) or with source+target context (right). Boxes and red median lines show CCI results based on gold context-sensitive tokens. Dotted bars show median CCI scores obtained from context-sensitive tokens identified by KL-Divergence during CTI (E2E settings).\n\n\n\nWe conduct a controlled CCI evaluation using gold context-sensitive tokens as the starting point to attribute contextual cues. Provided that gold context-sensitive tokens are only available in annotated reference translations, a simple option when applying CCI to those would involve using references as model generations. However, this was shown to be problematic by previous research, as it would induce a distributional discrepancy in model predictions (Vamvas and Sennrich, 2021b). For this reason, we let the model generate a natural translation and instead try to align tags to this new sentence using the awesome aligner (Dou and Neubig, 2021) with labse multilingual embeddings (Feng et al., 2022). While this process is not guaranteed to always result in accurate tags, it provides a good approximation of gold CTI annotations for model generation, which is suitable for our assessment. This corresponds to the baseline plausibility evaluation described in Section 2.2.2, allowing us to evaluate attribution methods in isolation, assuming perfect identification of context-sensitive tokens. Figure 4.4 presents our results. Scores in the right plot are relative to the context-aware Opus Large model of Section 4.4.4 using both source and target context. Instead, the left plot presents results for an alternative version of the same model that was fine-tuned using only the source context (i.e., translating \\(C_x, x \\rightarrow y\\) without producing the target context \\(C_y\\)). Source-only context was used in previous context-aware MT studies (Fernandes et al., 2022), and we include it in our analysis to assess how the presence of target context impacts model plausibility. We finally validate the end-to-end plausibility of PECoRe-detected pairs using context-sensitive tokens identified by the best CTI metric from Section 4.4.4 (KL-Divergence) as the starting point for CCI, and using a simple statistical selector equivalent to the one used for CTI evaluation.\nFirst, contextual cues are more easily detected for the source-only model using all evaluated methods. This finding corroborates previous evidence highlighting how context usage issues might emerge when lengthy context is provided (Fernandes et al., 2021; Shi et al., 2023). When moving from gold CTI tags to the end-to-end setting (E2E) we observe a larger drop in plausibility for the SCAT+ and DiscEval-MT ana datasets that more closely match the fine-tuning data of analyzed MT models. This suggests that standard evaluation practices may overestimate model plausibility for in-domain settings and that our proposed framework can effectively mitigate this issue. Interestingly, the Attention Best method suffers the most from end-to-end CCI application, while other approaches are more mildly affected. This can result from attention heads failing to generalize to other discourse-level phenomena at test time, providing further evidence of the limitations of attention as an explanatory metric (Jain and Wallace, 2019; Bastings and Filippova, 2020). While \\(\\nabla_\\text{diff}\\) and \\(\\nabla_\\text{KL}\\) appear as the most robust choices across the two datasets, per-example variability remains high across the board, leaving space for improvement for more plausible attribution methods in future work.",
    "crumbs": [
      "Attributing Context Usage in Multilingual NLP",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Quantifying Context Usage in Neural Machine Translation</span>"
    ]
  },
  {
    "objectID": "chapters/chap-4-pecore.html#sec-chap4-analysis",
    "href": "chapters/chap-4-pecore.html#sec-chap4-analysis",
    "title": "4  Quantifying Context Usage in Neural Machine Translation",
    "section": "4.5 Detecting Context Reliance in the Wild",
    "text": "4.5 Detecting Context Reliance in the Wild\nWe continue our analysis by applying the PECoRe method to the popular Flores-101 MT benchmark (Goyal et al., 2022), containing groups of 3-5 contiguous sentences from English Wikipedia. While previous sections used labeled examples to evaluate the effectiveness of PECoRe components, here we apply our framework end-to-end to unannotated MT outputs and inspect the resulting cue-target pairs to identify the successes and failures of context-aware MT models.\nSpecifically, we apply PECoRe to the context-aware Opus Large and mBART-50 models of Section 4.4.1, using KL-Divergence as CTI metric and \\(\\nabla_\\text{KL}\\) as CCI attribution method. We set \\(s_\\text{cti}\\) and \\(s_\\text{cci}\\) to two standard deviations above the per-example average score to focus our analysis on very salient tokens.\n\n\n\n\n\n\n1. Acronym Translation (English → French, correct but more generic)\n\n\n\\(C_x\\): Across the United States of America, there are approximately 400,000 known cases of Multiple Sclerosis (MS) […]\n\n\n\\(C_y\\): Aux États-Unis, il y a environ 400 000 cas connus de sclérose en plaques […]\n\n\n\\(x\\): MS affects the central nervous system, which is made up of the brain, the spinal cord and the optic nerve.\n\n\n\\(\\tilde y\\): La SEP affecte le système nerveux central, composé du cerveau, de la moelle épinière et du nerf optique.\n\n\n\\(\\hat y\\): La maladie affecte le système nerveux central, composé du cerveau, de la moelle épinière et du nerf optique.\n\n\n2. Anaphora Resolution (English → French, incorrect)\n\n\n\\(C_x\\): The terrified King and Madam Elizabeth were forced back to Paris by a mob of market women.\n\n\n\\(C_y\\): Le roi et Madame Elizabeth ont été forcés à revenir à Paris par une foule de femmes du marché.\n\n\n\\(x\\): In a carriage, they traveled back to Paris surrounded by a mob of people screaming and shouting threats […]\n\n\n\\(\\tilde y\\): Dans une carriole, ils sont retournés à Paris entourés d’une foule de gens hurlant et criant des menaces […]\n\n\n\\(\\hat y\\): Dans une carriole, elles sont retournées à Paris entourées d’une foule de gens hurlant et criant des menaces […]\n\n\n\n\n\nTable 4.4: Flores-101 examples with cue-target pairs identified by PECoRe in Opus Large contextual translations. Context-sensitive tokens generated instead of their non-contextual counterparts are identified by CTI, and contextual cues justifying their predictions are retrieved by CCI. Other changes in \\(\\hat y\\) are not considered context-sensitive by PECoRe.\n\n\n\n\n\n\n\n\n\n3. Numeric format cohesion (English → French, incorrect)\n\n\n\\(C_x\\): The game kicked off at 10:00am with great weather apart from mid morning drizzle […]\n\n\n\\(C_y\\): Le match a commencé à 10:00 du matin avec un beau temps à part la nuée du matin […]\n\n\n\\(x\\): South Africa started on the right note when they had a comfortable 26-00 win against Zambia.\n\n\n\\(\\tilde y\\): L’Afrique du Sud a commencé sur la bonne note quand ils ont eu une confortable victoire de 26 contre le Zambia.\n\n\n\\(\\hat y\\): L’Afrique du Sud a commencé sur la bonne note quand ils ont eu une confortable victoire de 26:00 contre le Zambia.\n\n\n4. Lexical cohesion (English → Turkish, correct)\n\n\n\\(C_x\\): The activity of all stars in the system was found to be driven by their luminosity, their rotation, and nothing else.\n\n\n\\(C_y\\): Sistemdeki bütün ulduzların faaliyetlerinin, parlaklıkları, rotasyonları ve başka hiçbir şeyin etkisi altında olduğunu ortaya çıkardılar.\n\n\n\\(x\\): The luminosity and rotation are used together to determine a star’s Rossby number, which is related to plasma flow.\n\n\n\\(\\tilde y\\): Parlaklık ve döngü, bir akışıyla ilgili Rossby sayısını belirlemek için birlikte kullanılıyor.\n\n\n\\(\\hat y\\): Parlaklık ve rotasyon, bir akışıyla ilgili Rossby sayısını belirlemek için birlikte kullanılıyor.\n\n\n\n\n\nTable 4.5: Flores-101 examples with cue-target pairs identified by PECoRe in mBART-50 contextual translations. Context-sensitive tokens generated instead of their non-contextual counterparts are identified by CTI, and contextual cues justifying their predictions are retrieved by CCI. Other changes in \\(\\hat y\\) are not considered context-sensitive by PECoRe.\n\n\n\nTable 4.4 and Table 4.5 show some examples annotated with PECoRe outputs. In the first example, the acronym MS, standing for Multiple Sclerosis, is translated generically as la maladie (the illness) in the contextual output, but as SEP (the French acronym for MS, i.e. sclérose en plaques) when context is not provided. PECoRe shows how this choice is mostly driven by the MS mention in source context \\(C_x\\) while the term sclérose en plaques in target context \\(C_y\\) is not identified as influential, possibly motivating the choice for the more generic option.\nIn the second example, the prediction of pronoun elles (they, feminine) depends on the context noun phrase mob of market women (foule de femmes du marché in French). However, the correct pronoun referent is Le roi et Madame Elizabeth (the king and Madam Elizabeth), so the pronoun should be the masculine default ils, commonly used for mixed-gender groups in French. PECoRe identifies this as a context-dependent failure due to an issue with the MT model’s anaphora resolution.\nThe third example presents an interesting case of erroneous numeric format cohesion that would typically go undetected when relying on pre-defined linguistic hypotheses. In this sentence, the score 26-00 is translated as 26 in the contextless output and as 26:00 in the context-aware translation. The 10:00 time indications found by PECoRe in the contexts suggest this is a case of problematic lexical cohesion.\nFinally, we include an example of context usage for English\\(\\rightarrow\\)Turkish translation to test the contextual capabilities of the default mBART-50 model without context-aware fine-tuning. Again, PECoRe shows how the word rotasyon (rotation) is selected over döngü (loop) as the correct translation in the contextual case due to the presence of the lexically similar word rotasyonları in the previous context.",
    "crumbs": [
      "Attributing Context Usage in Multilingual NLP",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Quantifying Context Usage in Neural Machine Translation</span>"
    ]
  },
  {
    "objectID": "chapters/chap-4-pecore.html#sec-chap4-inseq-integration",
    "href": "chapters/chap-4-pecore.html#sec-chap4-inseq-integration",
    "title": "4  Quantifying Context Usage in Neural Machine Translation",
    "section": "4.6 Integrating PECoRe in Inseq",
    "text": "4.6 Integrating PECoRe in Inseq\nTo facilitate the use of PECoRe in future research, a flexible implementation of the framework was incorporated into the Inseq toolkit presented in Chapter 3. Since its v0.6.0 Inseq offers the CLI command attribute-context, supporting all contrastive step functions and attribution methods in the library, and compatible with any decoder-only and encoder-decoder generative language model. Figure 4.5 provides an example employing the Inseq API to attribute a language model answer to input context paragraphs, similarly to the retrieval-augmented generation task we discuss in Chapter 5.8 In the example, the StableLM 2 Zephyr 1.6B language model9 is prompted with contexts retrieved from Wikipedia to provide a long-form answer to a query about population in the Hawaiian islands. When referring to “the information provided” in ⓵, PECoRe identifies the indices of the two documents containing relevant information as salient. The name of Ni’ihau, a small island with barely any population, is also found important when the model produces an additional remark on their population in ⓶. However, we observe that the answer in the context is not identified as salient by PECoRe during generation, suggesting that the model might be relying on memorization. We test the hypothesis by prompting the model in a closed-book setting without context paragraphs, finding that the model can indeed respond correctly without context. Moreover, as expected, the island of Ni’ihau is never mentioned in the contextless response. Additional examples of PECoRe usage for other generation tasks are provided in Section A.2.3.\n\n\n\n\n\n\nFigure 4.5: Example of context attribution for open-book QA using the Inseq-powered PECoRe demo. Context-sensitive tokens and contextual cues are highlighted.",
    "crumbs": [
      "Attributing Context Usage in Multilingual NLP",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Quantifying Context Usage in Neural Machine Translation</span>"
    ]
  },
  {
    "objectID": "chapters/chap-4-pecore.html#conclusion",
    "href": "chapters/chap-4-pecore.html#conclusion",
    "title": "4  Quantifying Context Usage in Neural Machine Translation",
    "section": "4.7 Conclusion",
    "text": "4.7 Conclusion\nWe introduced PECoRe, a novel interpretability framework for detecting and attributing context usage in language models’ generations. PECoRe extends the standard plausibility evaluation procedure adopted in interpretability research by proposing a two-step procedure to identify context-sensitive generated tokens and match them to contextual cues contributing to their prediction. We applied PECoRe to context-aware MT, finding that context-sensitive tokens and their disambiguating rationales can be detected consistently and with reasonable accuracy across several datasets, models and discourse phenomena. Moreover, an end-to-end application of our framework without human annotations revealed incorrect context usage, leading to problematic MT model outputs.\nWhile our evaluation is mainly focused on the machine translation domain, thanks to its generality and its integration in the Inseq framework PECoRe can easily be applied to other context-dependent language generation tasks such as question answering and summarization, as also demonstrated in the previous section. Future applications of our methodology could investigate the usage of in-context demonstrations and chain-of-thought reasoning in large language models (Brown et al., 2020; Wei et al., 2022), and explore PECoRe usage for different model architectures and input modalities. In the next chapter, we extend PECoRe for attributing context usage in retrieval-augmented generation tasks, where the model is expected to rely on external knowledge sources to produce answers to user queries.\n\n\n\n\nSamira Abnar and Willem Zuidema. 2020. Quantifying attention flow in transformers. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault, editors, Proceedings of the 58th annual meeting of the association for computational linguistics, pages 4190–4197, Online. Association for Computational Linguistics.\n\n\nPepa Atanasova, Jakob Grue Simonsen, Christina Lioma, and Isabelle Augenstein. 2020. A diagnostic study of explainability techniques for text classification. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, Proceedings of the 2020 conference on empirical methods in natural language processing (EMNLP), pages 3256–3274, Online. Association for Computational Linguistics.\n\n\nGiuseppe Attanasio, Eliana Pastor, Chiara Di Bonaventura, and Debora Nozza. 2023. Ferret: A framework for benchmarking explainers on transformers. In Danilo Croce and Luca Soldaini, editors, Proceedings of the 17th conference of the european chapter of the association for computational linguistics: System demonstrations, pages 256–266, Dubrovnik, Croatia. Association for Computational Linguistics.\n\n\nJasmijn Bastings, Sebastian Ebert, Polina Zablotskaia, Anders Sandholm, and Katja Filippova. 2022. “Will you find these shortcuts?” A protocol for evaluating the faithfulness of input salience methods for text classification. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Proceedings of the 2022 conference on empirical methods in natural language processing, pages 976–991, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\n\n\nJasmijn Bastings and Katja Filippova. 2020. The elephant in the interpretability room: Why use attention as explanation when we have saliency methods? In Afra Alishahi, Yonatan Belinkov, Grzegorz Chrupała, Dieuwke Hupkes, Yuval Pinter, and Hassan Sajjad, editors, Proceedings of the third BlackboxNLP workshop on analyzing and interpreting neural networks for NLP, pages 149–155, Online. Association for Computational Linguistics.\n\n\nRachel Bawden, Rico Sennrich, Alexandra Birch, and Barry Haddow. 2018. Evaluating discourse phenomena in neural machine translation. In Marilyn Walker, Heng Ji, and Amanda Stent, editors, Proceedings of the 2018 conference of the north American chapter of the association for computational linguistics: Human language technologies, volume 1 (long papers), pages 1304–1313, New Orleans, Louisiana. Association for Computational Linguistics.\n\n\nNora Belrose, Zach Furman, Logan Smith, Danny Halawi, Igor Ostrovsky, Lev McKinney, Stella Biderman, and Jacob Steinhardt. 2023. Eliciting latent predictions from transformers with the tuned lens. ArXiv, abs/2303.08112.\n\n\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, et al. 2020. Language models are few-shot learners. In Proceedings of the 34th international conference on neural information processing systems, Red Hook, NY, USA. Curran Associates Inc.\n\n\nMauro Cettolo, Marcello Federico, Luisa Bentivogli, Jan Niehues, Sebastian Stüker, Katsuhito Sudoh, Koichiro Yoshino, and Christian Federmann. 2017. Overview of the IWSLT 2017 evaluation campaign. In Sakriani Sakti and Masao Utiyama, editors, Proceedings of the 14th international conference on spoken language translation, pages 2–14, Tokyo, Japan. International Workshop on Spoken Language Translation.\n\n\nDavid Dale, Elena Voita, Loic Barrault, and Marta R. Costa-jussà. 2023a. Detecting and mitigating hallucinations in machine translation: Model internal workings alone do well, sentence similarity Even better. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st annual meeting of the association for computational linguistics (volume 1: Long papers), pages 36–50, Toronto, Canada. Association for Computational Linguistics.\n\n\nDavid Dale, Elena Voita, Janice Lam, Prangthip Hansanti, Christophe Ropers, Elahe Kalbassi, Cynthia Gao, Loic Barrault, and Marta Costa-jussà. 2023b. HalOmi: A manually annotated benchmark for multilingual hallucination and omission detection in machine translation. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 conference on empirical methods in natural language processing, pages 638–653, Singapore. Association for Computational Linguistics.\n\n\nJay DeYoung, Sarthak Jain, Nazneen Fatema Rajani, Eric Lehman, Caiming Xiong, Richard Socher, and Byron C. Wallace. 2020. ERASER: A benchmark to evaluate rationalized NLP models. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault, editors, Proceedings of the 58th annual meeting of the association for computational linguistics, pages 4443–4458, Online. Association for Computational Linguistics.\n\n\nZi-Yi Dou and Graham Neubig. 2021. Word alignment by fine-tuning embeddings on parallel corpora. In Paola Merlo, Jorg Tiedemann, and Reut Tsarfaty, editors, Proceedings of the 16th conference of the european chapter of the association for computational linguistics: Main volume, pages 2112–2128, Online. Association for Computational Linguistics.\n\n\nEsin Durmus, He He, and Mona Diab. 2020. FEQA: A question answering evaluation framework for faithfulness assessment in abstractive summarization. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault, editors, Proceedings of the 58th annual meeting of the association for computational linguistics, pages 5055–5070, Online. Association for Computational Linguistics.\n\n\nLukas Edman, Gabriele Sarti, Antonio Toral, Gertjan van Noord, and Arianna Bisazza. 2024. Are character-level translations worth the wait? Comparing ByT5 and mT5 for machine translation. Transactions of the Association for Computational Linguistics, 12:392–410.\n\n\nFangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Arivazhagan, and Wei Wang. 2022. Language-agnostic BERT sentence embedding. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Proceedings of the 60th annual meeting of the association for computational linguistics (volume 1: Long papers), pages 878–891, Dublin, Ireland. Association for Computational Linguistics.\n\n\nPatrick Fernandes, António Farinhas, Ricardo Rei, José G. C. de Souza, Perez Ogayo, Graham Neubig, and Andre Martins. 2022. Quality-aware decoding for neural machine translation. In Marine Carpuat, Marie-Catherine de Marneffe, and Ivan Vladimir Meza Ruiz, editors, Proceedings of the 2022 conference of the north american chapter of the association for computational linguistics: Human language technologies, pages 1396–1412, Seattle, United States. Association for Computational Linguistics.\n\n\nPatrick Fernandes, Kayo Yin, Emmy Liu, André Martins, and Graham Neubig. 2023. When does translation require context? A data-driven, multilingual exploration. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st annual meeting of the association for computational linguistics (volume 1: Long papers), pages 606–626, Toronto, Canada. Association for Computational Linguistics.\n\n\nPatrick Fernandes, Kayo Yin, Graham Neubig, and André F. T. Martins. 2021. Measuring and increasing context usage in context-aware machine translation. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, Proceedings of the 59th annual meeting of the association for computational linguistics and the 11th international joint conference on natural language processing (volume 1: Long papers), pages 6467–6478, Online. Association for Computational Linguistics.\n\n\nJavier Ferrando, Gerard I. Gállego, Belen Alastruey, Carlos Escolano, and Marta R. Costa-jussà. 2022a. Towards opening the black box of neural machine translation: Source and target interpretations of the transformer. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Proceedings of the 2022 conference on empirical methods in natural language processing, pages 8756–8769, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\n\n\nJavier Ferrando, Gerard I. Gállego, and Marta R. Costa-jussà. 2022b. Measuring the mixing of contextual information in the transformer. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Proceedings of the 2022 conference on empirical methods in natural language processing, pages 8698–8714, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\n\n\nJavier Ferrando, Gerard I. Gállego, Ioannis Tsiamas, and Marta R. Costa-jussà. 2023. Explaining how transformers use context to build predictions. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st annual meeting of the association for computational linguistics (volume 1: Long papers), pages 5486–5513, Toronto, Canada. Association for Computational Linguistics.\n\n\nMor Geva, Avi Caciularu, Kevin Wang, and Yoav Goldberg. 2022. Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Proceedings of the 2022 conference on empirical methods in natural language processing, pages 30–45, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\n\n\nNaman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-Jen Chen, Guillaume Wenzek, Da Ju, Sanjana Krishnan, Marc’Aurelio Ranzato, Francisco Guzmán, and Angela Fan. 2022. The Flores-101 evaluation benchmark for low-resource and multilingual machine translation. Transactions of the Association for Computational Linguistics, 10:522–538.\n\n\nTanya Goyal and Greg Durrett. 2021. Annotating and modeling fine-grained factuality in summarization. In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou, editors, Proceedings of the 2021 conference of the north american chapter of the association for computational linguistics: Human language technologies, pages 1449–1462, Online. Association for Computational Linguistics.\n\n\nAri Holtzman, Peter West, Vered Shwartz, Yejin Choi, and Luke Zettlemoyer. 2021. Surface form competition: Why the highest probability answer isn‘t always right. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Proceedings of the 2021 conference on empirical methods in natural language processing, pages 7038–7051, Online; Punta Cana, Dominican Republic. Association for Computational Linguistics.\n\n\nAlon Jacovi and Yoav Goldberg. 2020. Towards faithfully interpretable NLP systems: How should we define and evaluate faithfulness? In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault, editors, Proceedings of the 58th annual meeting of the association for computational linguistics, pages 4198–4205, Online. Association for Computational Linguistics.\n\n\nSarthak Jain and Byron C. Wallace. 2019. Attention is not Explanation. In Jill Burstein, Christy Doran, and Thamar Solorio, editors, Proceedings of the 2019 conference of the north American chapter of the association for computational linguistics: Human language technologies, volume 1 (long and short papers), pages 3543–3556, Minneapolis, Minnesota. Association for Computational Linguistics.\n\n\nLinghao Jin, Jacqueline He, Jonathan May, and Xuezhe Ma. 2023. Challenges in context-aware neural machine translation. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 conference on empirical methods in natural language processing, pages 15246–15263, Singapore. Association for Computational Linguistics.\n\n\nYunsu Kim, Duc Thanh Tran, and Hermann Ney. 2019. When and why is document-level context useful in neural machine translation? In Andrei Popescu-Belis, Sharid Loáiciga, Christian Hardmeier, and Deyi Xiong, editors, Proceedings of the fourth workshop on discourse in machine translation (DiscoMT 2019), pages 24–34, Hong Kong, China. Association for Computational Linguistics.\n\n\nGoro Kobayashi, Tatsuki Kuribayashi, Sho Yokoi, and Kentaro Inui. 2020. Attention is not only a weight: Analyzing transformers with vector norms. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, Proceedings of the 2020 conference on empirical methods in natural language processing (EMNLP), pages 7057–7075, Online. Association for Computational Linguistics.\n\n\nWojciech Kryscinski, Bryan McCann, Caiming Xiong, and Richard Socher. 2020. Evaluating the factual consistency of abstractive text summarization. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, Proceedings of the 2020 conference on empirical methods in natural language processing (EMNLP), pages 9332–9346, Online. Association for Computational Linguistics.\n\n\nSolomon Kullback and Richard A Leibler. 1951. On information and sufficiency. The annals of mathematical statistics, 22(1):79–86.\n\n\nPierre Lison, Jörg Tiedemann, and Milen Kouylekov. 2018. OpenSubtitles2018: Statistical rescoring of sentence alignments in large, noisy parallel corpora. In Nicoletta Calzolari, Khalid Choukri, Christopher Cieri, Thierry Declerck, Sara Goggi, Koiti Hasida, Hitoshi Isahara, Bente Maegaard, Joseph Mariani, Hélène Mazo, Asuncion Moreno, Jan Odijk, Stelios Piperidis, and Takenobu Tokunaga, editors, Proceedings of the eleventh international conference on language resources and evaluation (LREC 2018), Miyazaki, Japan. European Language Resources Association (ELRA).\n\n\nAntónio Lopes, M. Amin Farajian, Rachel Bawden, Michael Zhang, and André F. T. Martins. 2020. Document-level neural MT: A systematic comparison. In André Martins, Helena Moniz, Sara Fumega, Bruno Martins, Fernando Batista, Luisa Coheur, Carla Parra, Isabel Trancoso, Marco Turchi, Arianna Bisazza, Joss Moorkens, Ana Guerberof, Mary Nurminen, Lena Marg, and Mikel L. Forcada, editors, Proceedings of the 22nd annual conference of the european association for machine translation, pages 225–234, Lisboa, Portugal. European Association for Machine Translation.\n\n\nAndreas Madsen, Siva Reddy, and Sarath Chandar. 2022. Post-hoc interpretability for neural NLP: A survey. ACM Comput. Surv., 55(8).\n\n\nSuvodeep Majumder, Stanislas Lauly, Maria Nadejde, Marcello Federico, and Georgiana Dinu. 2022. A baseline revisited: Pushing the limits of multi-segment models for context-aware translation. ArXiv, abs/2210.10906.\n\n\nSameen Maruf, Fahimeh Saleh, and Gholamreza Haffari. 2021. A survey on document-level neural machine translation: Methods and evaluation. ACM Comput. Surv., 54(2).\n\n\nJoshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. 2020. On faithfulness and factuality in abstractive summarization. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault, editors, Proceedings of the 58th annual meeting of the association for computational linguistics, pages 1906–1919, Online. Association for Computational Linguistics.\n\n\nR. Thomas McCoy, Ellie Pavlick, and Tal Linzen. 2019. Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference. In Anna Korhonen, David Traum, and Lluís Màrquez, editors, Proceedings of the 57th annual meeting of the association for computational linguistics, pages 3428–3448, Florence, Italy. Association for Computational Linguistics.\n\n\nHosein Mohebbi, Willem Zuidema, Grzegorz Chrupała, and Afra Alishahi. 2023. Quantifying context mixing in transformers. In Andreas Vlachos and Isabelle Augenstein, editors, Proceedings of the 17th conference of the european chapter of the association for computational linguistics, pages 3378–3400, Dubrovnik, Croatia. Association for Computational Linguistics.\n\n\nGabriele Sarti, Grzegorz Chrupała, Malvina Nissim, and Arianna Bisazza. 2024a. Quantifying the plausibility of context reliance in neural machine translation. In The twelfth international conference on learning representations (ICLR 2024), Vienna, Austria. OpenReview.\n\n\nGabriele Sarti, Nils Feldhus, Jirui Qi, Malvina Nissim, and Arianna Bisazza. 2024b. Democratizing advanced attribution analyses of generative language models with the inseq toolkit. In xAI-2024 late-breaking work, demos and doctoral consortium joint proceedings, pages 289–296, Valletta, Malta. CEUR.org.\n\n\nGabriele Sarti, Nils Feldhus, Ludwig Sickert, Oskar van der Wal, Malvina Nissim, and Arianna Bisazza. 2023. Inseq: An interpretability toolkit for sequence generation models. In Danushka Bollegala, Ruihong Huang, and Alan Ritter, editors, Proceedings of the 61st annual meeting of the association for computational linguistics (volume 3: System demonstrations), pages 421–435, Toronto, Canada. Association for Computational Linguistics.\n\n\nFreda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed Chi, Nathanael Schärli, and Denny Zhou. 2023. Large language models can be easily distracted by irrelevant context. In Proceedings of the 40th international conference on machine learning, Honolulu, Hawaii, USA. JMLR.org.\n\n\nKaren Simonyan, Andrea Vedaldi, and Andrew Zisserman. 2014. Deep inside convolutional networks: Visualising image classification models and saliency maps. In Yoshua Bengio and Yann LeCun, editors, 2nd international conference on learning representations, (ICLR), Banff, AB, Canada.\n\n\nGabriel Stanovsky, Noah A. Smith, and Luke Zettlemoyer. 2019. Evaluating gender bias in machine translation. In Anna Korhonen, David Traum, and Lluís Màrquez, editors, Proceedings of the 57th annual meeting of the association for computational linguistics, pages 1679–1684, Florence, Italy. Association for Computational Linguistics.\n\n\nMukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017. Axiomatic attribution for deep networks. In Proceedings of the 34th international conference on machine learning (ICML), volume 70, pages 3319–3328, Sydney, Australia. Journal of Machine Learning Research (JMLR).\n\n\nJoel Tang, Marina Fomicheva, and Lucia Specia. 2022. Reducing hallucinations in neural machine translation with feature attribution. ArXiv.\n\n\nYuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Naman Goyal, Vishrav Chaudhary, Jiatao Gu, and Angela Fan. 2021. Multilingual translation from denoising pre-training. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, Findings of the association for computational linguistics: ACL-IJCNLP 2021, pages 3450–3466, Online. Association for Computational Linguistics.\n\n\nJörg Tiedemann and Yves Scherrer. 2017. Neural machine translation with extended context. In Bonnie Webber, Andrei Popescu-Belis, and Jörg Tiedemann, editors, Proceedings of the third workshop on discourse in machine translation, pages 82–92, Copenhagen, Denmark. Association for Computational Linguistics.\n\n\nJörg Tiedemann and Santhosh Thottingal. 2020. OPUS-MT – building open translation services for the world. In André Martins, Helena Moniz, Sara Fumega, Bruno Martins, Fernando Batista, Luisa Coheur, Carla Parra, Isabel Trancoso, Marco Turchi, Arianna Bisazza, Joss Moorkens, Ana Guerberof, Mary Nurminen, Lena Marg, and Mikel L. Forcada, editors, Proceedings of the 22nd annual conference of the european association for machine translation, pages 479–480, Lisboa, Portugal. European Association for Machine Translation.\n\n\nKeyon Vafa, Yuntian Deng, David Blei, and Alexander Rush. 2021. Rationales for sequential predictions. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Proceedings of the 2021 conference on empirical methods in natural language processing, pages 10314–10332, Online; Punta Cana, Dominican Republic. Association for Computational Linguistics.\n\n\nJannis Vamvas and Rico Sennrich. 2021a. Contrastive conditioning for assessing disambiguation in MT: A case study of distilled bias. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Proceedings of the 2021 conference on empirical methods in natural language processing, pages 10246–10265, Online; Punta Cana, Dominican Republic. Association for Computational Linguistics.\n\n\nJannis Vamvas and Rico Sennrich. 2021b. On the limits of minimal pairs in contrastive evaluation. In Jasmijn Bastings, Yonatan Belinkov, Emmanuel Dupoux, Mario Giulianelli, Dieuwke Hupkes, Yuval Pinter, and Hassan Sajjad, editors, Proceedings of the fourth BlackboxNLP workshop on analyzing and interpreting neural networks for NLP, pages 58–68, Punta Cana, Dominican Republic. Association for Computational Linguistics.\n\n\nJannis Vamvas and Rico Sennrich. 2022. As little as possible, as much as necessary: Detecting over- and undertranslations with contrastive conditioning. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Proceedings of the 60th annual meeting of the association for computational linguistics (volume 2: Short papers), pages 490–500, Dublin, Ireland. Association for Computational Linguistics.\n\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in neural information processing systems, volume 30. Curran Associates, Inc.\n\n\nElena Voita, Rico Sennrich, and Ivan Titov. 2019. Context-aware monolingual repair for neural machine translation. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors, Proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing (EMNLP-IJCNLP), pages 877–886, Hong Kong, China. Association for Computational Linguistics.\n\n\nElena Voita, Rico Sennrich, and Ivan Titov. 2021. Analyzing the source and target contributions to predictions in neural machine translation. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, Proceedings of the 59th annual meeting of the association for computational linguistics and the 11th international joint conference on natural language processing (volume 1: Long papers), pages 1126–1140, Online. Association for Computational Linguistics.\n\n\nElena Voita, Pavel Serdyukov, Rico Sennrich, and Ivan Titov. 2018. Context-aware neural machine translation learns anaphora resolution. In Iryna Gurevych and Yusuke Miyao, editors, Proceedings of the 56th annual meeting of the association for computational linguistics (volume 1: Long papers), pages 1264–1274, Melbourne, Australia. Association for Computational Linguistics.\n\n\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc V. Le, and Denny Zhou. 2022. Chain-of-thought prompting elicits reasoning in large language models. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in neural information processing systems, volume 35, pages 24824–24837. Curran Associates, Inc.\n\n\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, et al. 2020. Transformers: State-of-the-art natural language processing. In Qun Liu and David Schlangen, editors, Proceedings of the 2020 conference on empirical methods in natural language processing: System demonstrations, pages 38–45, Online. Association for Computational Linguistics.\n\n\nKayo Yin, Patrick Fernandes, Danish Pruthi, Aditi Chaudhary, André F. T. Martins, and Graham Neubig. 2021. Do context-aware translation models pay the right attention? In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, Proceedings of the 59th annual meeting of the association for computational linguistics and the 11th international joint conference on natural language processing (volume 1: Long papers), pages 788–801, Online. Association for Computational Linguistics.\n\n\nKayo Yin and Graham Neubig. 2022. Interpreting language models with contrastive explanations. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Proceedings of the 2022 conference on empirical methods in natural language processing, pages 184–198, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\n\n\nMatthew D. Zeiler and Rob Fergus. 2014. Visualizing and understanding convolutional networks. In David Fleet, Tomas Pajdla, Bernt Schiele, and Tinne Tuytelaars, editors, 13th european conference on computer vision (ECCV), pages 818–833, Switzerland. Springer International Publishing.",
    "crumbs": [
      "Attributing Context Usage in Multilingual NLP",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Quantifying Context Usage in Neural Machine Translation</span>"
    ]
  },
  {
    "objectID": "chapters/chap-4-pecore.html#footnotes",
    "href": "chapters/chap-4-pecore.html#footnotes",
    "title": "4  Quantifying Context Usage in Neural Machine Translation",
    "section": "",
    "text": "Code: https://github.com/gsarti/pecore↩︎\nWe avoid using the term faithfulness due to its ambiguous usage in interpretability research.↩︎\nIn the contextual MT example of Figure 4.2, \\(C\\) includes source context \\(C_x\\) and target context \\(C_y\\).↩︎\nWe use \\(m^i\\) to denote the result of \\(m(P_\\text{ctx}^{i}, P_\\text{no-ctx}^{i})\\). Several metrics are presented in Section 4.4.2.↩︎\nSCAT+ is available on the Hugging Face Hub: inseq/scat↩︎\nOur modified version of DiscEval-MT is available on the Hugging Face Hub: inseq/disc_eval_mt.↩︎\nProvided that \\(P_\\text{no-ctx}(\\hat y_i)\\) does not depend on context, the \\(\\nabla_\\text{KL}\\) gradient is functionally equivalent to the gradient for the cross-entropy function \\(H(P_\\text{ctx}, P_\\text{no-ctx}) = - \\sum_{\\hat{y}_i \\in \\mathcal{V}} P_\\text{ctx}(\\hat{y}_i) \\log P_\\text{no-ctx}(\\hat{y}_i)\\)).↩︎\nThe interface is available at: https://huggingface.co/spaces/gsarti/pecore.↩︎\nstabilityai/stablelm-2-zephyr-1_6b↩︎",
    "crumbs": [
      "Attributing Context Usage in Multilingual NLP",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Quantifying Context Usage in Neural Machine Translation</span>"
    ]
  },
  {
    "objectID": "chapters/chap-5-mirage.html",
    "href": "chapters/chap-5-mirage.html",
    "title": "5  Answer Attribution for Trustworthy Retrieval-Augmented Generation",
    "section": "",
    "text": "5.1 Introduction\nRetrieval-augmented generation with large language models has become the de facto standard methodology for question answering in both academic (Lewis et al., 2020; Izacard et al., 2023) and industrial settings (Dao and Le, 2023; Ma et al., 2024). This approach is effective in mitigating hallucinations and producing factually accurate answers (Petroni et al., 2020; Lewis et al., 2020; Borgeaud et al., 2022; Ren et al., 2025). However, verifying whether the model answer is faithfully supported by the retrieved sources is often non-trivial due to the large context size and the variety of potentially correct answers (Krishna et al., 2021; Xu et al., 2023).\nIn light of this issue, several answer attribution1 approaches were recently proposed to ensure the trustworthiness of RAG outputs (Rashkin et al., 2023; Bohnet et al., 2022; Muller et al., 2023). Initial efforts in this area employed models trained on Natural Language Inference (NLI) to automate the identification of supporting documents (Bohnet et al., 2022; Yue et al., 2023). However, being based on an external validator, this approach does not faithfully explain the answer generation process but simply identifies plausible sources supporting model answers in a post-hoc fashion. Following recent progress in the instruction-following abilities of LLMs, self-citation (i.e. prompting LLMs to generate inline citations alongside their answers) has been proposed to mitigate the training and inference costs of external validator modules (Gao et al., 2023a). However, self-citation performance is hindered by the imperfect instruction-following capacity of modern LLMs (Mu et al., 2023; Liu et al., 2023), and resulting attributions are still predicted in an unintelligible, post-hoc fashion. This is an important limitation for these approaches, since the primary goal of answer attribution should be to ensure that the LLM is not ‘right for the wrong reasons’ (McCoy et al., 2019).\nIn light of these considerations, we introduce Mirage, an extension of the context-reliance evaluation PECoRe framework from the previous chapter for efficient and faithful answer attributions. Mirage first identifies context-sensitive tokens in a generated sentence by measuring the shift in LM predictive distribution caused by the added input context. Then, it attributes this shift to specific influential tokens in the context using gradient-based saliency or other input attribution techniques (Madsen et al., 2022). Finally, attributions can be aggregated at the document level to match context-dependent generated sentences with retrieved documents that contribute to their prediction. The resulting pairs can then be converted to citations using the standard answer attribution (AA) format.\nWe begin our assessment of Mirage on the short-form XOR-AttriQA dataset (Muller et al., 2023), showing high agreement between Mirage results and human annotations across several languages. We then test our method on the open-ended ELI5 dataset (Fan et al., 2019), achieving AA quality comparable to or better than self-citation while ensuring a higher degree of control over attribution parameters. In summary, we make the following contributions:2",
    "crumbs": [
      "Attributing Context Usage in Multilingual NLP",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Answer Attribution for Trustworthy Retrieval-Augmented Generation</span>"
    ]
  },
  {
    "objectID": "chapters/chap-5-mirage.html#sec-chap5-intro",
    "href": "chapters/chap-5-mirage.html#sec-chap5-intro",
    "title": "5  Answer Attribution for Trustworthy Retrieval-Augmented Generation",
    "section": "",
    "text": "Figure 5.1: Mirage is a model internals-based framework for answer attribution in RAG settings. Context-sensitive answer spans (in color) are detected and matched with contextual cues in retrieved sources to evaluate the trustworthiness of models’ answers.\n\n\n\n\n\nWe introduce Mirage, a model internals-based answer attribution framework optimized for RAG applications.\nWe quantify the plausibility of Mirage attributions on two datasets, showing improvements over NLI and self-citation methods while ensuring better controllability and efficiency.\nWe analyze challenging attribution settings, highlighting Mirage‘s faithfulness to LLMs’ reasoning process.",
    "crumbs": [
      "Attributing Context Usage in Multilingual NLP",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Answer Attribution for Trustworthy Retrieval-Augmented Generation</span>"
    ]
  },
  {
    "objectID": "chapters/chap-5-mirage.html#background-and-related-work",
    "href": "chapters/chap-5-mirage.html#background-and-related-work",
    "title": "5  Answer Attribution for Trustworthy Retrieval-Augmented Generation",
    "section": "5.2 Background and Related Work",
    "text": "5.2 Background and Related Work\nIn RAG settings, a set of documents relevant to a user query is retrieved from an external dataset and infilled into an LLM prompt to improve the generation process (Petroni et al., 2020; Lewis et al., 2020). Answer attribution (Rashkin et al., 2023; Bohnet et al., 2022; Muller et al., 2023) aims to identify which retrieved documents support the generated answer (answer faithfulness, Gao et al., 2023b), e.g., by exploiting the similarity between model outputs and references.3 Simplifying access to relevant sources via answer attribution is a fundamental step towards ensuring RAG trustworthiness in customer-facing scenarios (Liu et al., 2023).\n\n5.2.1 Answer Attribution Methods\nEntailment-based Answer Attribution Bohnet et al. (2022) and Muller et al. (2023) propose to approximate human AA annotations with NLI systems such as TRUE (Honovich et al., 2022), using a source document as premise and an LLM-generated sentence as entailment hypothesis. AAs produced by these systems were shown to correlate strongly with human annotations, prompting their adoption in AA studies (Muller et al., 2023; Gao et al., 2023a). Despite their effectiveness, entailment-based methods can be computationally expensive when several answer sentence-document pairs are present. Moreover, this setup assumes the NLI model’s ability to robustly detect entailment relations across all domains and languages for which the LLM generator is used. In practice, however, NLI systems were shown to be brittle in challenging scenarios, exploiting shallow heuristics (McCoy et al., 2019; Nie et al., 2020; Sinha et al., 2021; Luo et al., 2022), and require dedicated efforts for less-resourced settings (Conneau et al., 2018). For example, NLI may fail to correctly attribute answers in multi-hop QA settings when considering individual documents as premises (Yang et al., 2018; Welbl et al., 2018).\nSelf-citation (Gao et al., 2023a) is a recent AA approach exploiting the ability of recent LLMs to follow instructions in natural language (Raffel et al., 2020; Chung et al., 2024; Sanh et al., 2022; OpenAI, 2023), thereby avoiding the need for an external validator. Nakano et al. (2021) and Menick et al. (2022) propose citation fine-tuning for LLMs, while Gao et al. (2023a) instruct general-purpose LLMs to produce inline citations in a few-shot setting. Self-citation answers are generally more relevant to the provided sources’ contents, but can still contain unsupported statements and inaccurate citations (Liu et al., 2023). In our preliminary analysis, we find that self-citation often misses relevant citations, uses wrong formats, or refers to non-existing documents (Figure 5.2). For the ELI5 dataset (Fan et al., 2019), we find that LLaMA 2 7B Chat (Touvron et al., 2023) and Zephyr \\(\\beta\\) 7B (Tunstall et al., 2024) fail to produce AAs matching the prompt instructions for the majority of generated sentences, with almost all answers having at least one unattributed sentence when the Gao et al. (2023a) self-citation setup is used (Table 5.1).\n\n\n\n\n\n\n\n\n\nFigure 5.2: Instruction-following errors in self-citation, using the setup of Gao et al. (2023a).\n\n\n\n\n\n \n\n\n\n\n\n\n\n\nModel\nMissing citation (%)\n\n\nSentence\nAnswer\n\n\n\n\nZephyr 7B \\(\\beta\\)\n54.5\n95.7\n\n\nLLaMA 2 7B Chat\n62.4\n99.3\n\n\n\n\n\nTable 5.1: Percentage of unattributed sentences and answers with \\(\\geq 1\\) unattributed sentences on ELI5.\n\n\n\n\n\n\nAnswer Attribution can be Unfaithful The aforementioned approaches do not account for attributions’ faithfulness, i.e. whether the selected documents influence the LLM during the generation. Indeed, the presence of an entailment relation or high semantic similarity does not imply that the retrieved document influenced the answer generation process. This can be true in cases where LLMs may rely on memorized knowledge while ignoring relevant, albeit unnecessary, contextual information.\nEven in the case of self-citation, recent work showed that, while the justifications of self-explaining LLMs appear plausible, they generally do not align with their internal reasoning process (Atanasova et al., 2023; Madsen et al., 2024; Agarwal et al., 2024; Randl et al., 2025), with little to no predictive efficacy (Huang et al., 2023). By contrast, approaches based on model internals are designed to faithfully reflect input importance in motivating model predictions. For instance, Alghisi et al. (2024) explores the use of gradient-based attribution to locate salient history segments for various dialogical tasks.\nConcurrent to our work, Phukan et al. (2024) and Cohen-Wang et al. (2024) have proposed other internals-based methods for granular AA of LLM generations. While the two-step approaches proposed in both works are similar to Mirage, they also differ in substantial ways. Notably, Phukan et al. (2024) derive attributions from embedding similarity, which does not capture the functional influence of context usage during the generation process. ContextCite (Cohen-Wang et al., 2024) instead fits a linear surrogate model to estimate the impact of ablating context segments on downstream answer probabilities. While this procedure approximates causal context influence, it still requires a sufficiently large context and many LLM forward passes to learn the surrogate model4, ultimately providing a coarser attribution for the full generated output. On the contrary, Mirage efficiently estimates generated tokens requiring attribution via contrastive metrics to produce granular attributions at the token level, limiting computations to estimate how context impacts LLM predictions. A maximally faithful AA approach would ablate all possible combinations of context elements to counterfactually estimate their importance in relation to model predictions. Given the long-form answers and contexts in RAG settings, this is practically unfeasible. Even if based on approximations, internals-based approaches such as Mirage are intrinsically more faithful than external validators like NLI models, since they aim to exploit information functional to the predictive process rather than relying solely on the generated output.",
    "crumbs": [
      "Attributing Context Usage in Multilingual NLP",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Answer Attribution for Trustworthy Retrieval-Augmented Generation</span>"
    ]
  },
  {
    "objectID": "chapters/chap-5-mirage.html#sec-chap5-framework",
    "href": "chapters/chap-5-mirage.html#sec-chap5-framework",
    "title": "5  Answer Attribution for Trustworthy Retrieval-Augmented Generation",
    "section": "5.3 Method",
    "text": "5.3 Method\nIdentifying which generated spans were most influenced by preceding information is a key challenge for LM attribution. The Model Internals-based RAG Explanations (Mirage) method we propose is an extension of the Plausibility Evaluation for Context Reliance (PECoRe) framework (Sarti et al., 2024) for context-aware machine translation. Importantly, this framework requires open-weights access to the LLM generator, which is a strict but necessary requirement to provide an accurate overview of the actual context usage during generation (Casper et al., 2024). This section frames the PECoRe’s two-step procedure in the context of RAG, as illustrated in Figure 5.3, and clarifies how Mirage adapts it for RAG answer attribution.\n\n\n\n\n\n\nFigure 5.3: Illustration of Mirage’s two-step approach adapted from PECoRe for RAG answer attribution. Step 1: CTI detects context-sensitive tokens in the generation (e.g. smaller). Step 2: CCI attributes the generation of detected tokens back to context tokens (e.g. few in Doc[1] promotes the generation of smaller instead of PC) using contrastive input attribution. Token pairs are then aggregated into sentence-document citations for practical usage.\n\n\n\nStep 1: Context-sensitive Token Identification (CTI) For every token in an answer sentence \\(\\mathbf{y} = \\langle y_1, \\dots, y_{n} \\rangle\\) generated by an LM prompted with a query \\(\\mathbf{q}\\) and a context \\(\\mathbf{c} = \\langle c_1, \\dots, c_{|\\mathbf{c}|} \\rangle\\), a contrastive metric \\(m\\) such as KL divergence (Kullback and Leibler, 1951) is used to quantify the shift in the LM predictive distribution at the \\(i\\)-th generation step when the context is present or absent (\\(P^i_\\text{ctx}\\) or \\(P^i_\\text{no-ctx}\\)). Resulting scores \\(\\mathbf{m} = \\langle m_1, \\dots, m_{n} \\rangle\\) reflect the context sensitivity of every generated token and can be converted into binary labels using a selector function \\(s_\\text{cti}\\):\n\\[\n\\begin{split}\n    \\text{CTI}(\\mathbf{q}, \\mathbf{c}, \\mathbf{y}) = \\{\\,y_i\\,|\\,s_\\text{cti\\,}(m_i) = 1\\,\\forall y_i \\in \\mathbf{y}\\}\n\\\\\n    \\text{with}\\;m_i = D_\\text{KL}(P^i_\\text{ctx} \\| P^i_\\text{no-ctx})\n\\end{split}\n\\]\nStep 2: Contextual Cues Imputation (CCI) For every context-sensitive token \\(y_i\\) identified by CTI, a contrastive alternative \\(y^{\\setminus \\mathbf{c}}_i\\) is produced by excluding \\(\\mathbf{c}\\) from the prompt, but using the original generated prefix \\(\\mathbf{y}_{&lt;i}\\). Then, contrastive input attribution (Yin and Neubig, 2022) is used to obtain attribution scores \\(\\mathbf{a}^i = \\langle a^i_1, \\dots, a^i_{|\\mathbf{c}|} \\rangle\\) for every context token \\(c_j \\in \\mathbf{c}\\):\n\\[\n\\begin{split}\n    \\mathbf{a}^i =\n    \\big\\{\\,\\nabla_j\\big(p(y_i) - p(y^{\\setminus \\mathbf{c}}_i)\\,\\big),\\;\\forall c_j \\in \\mathbf{c}\\,\\}\n\\end{split}\n\\]\nwhere \\(\\nabla_j\\) is the L2 norm of the gradient vector over the input embedding of context token \\(c_j\\), and both probabilities are computed from the same contextual inputs \\((\\mathbf{q}, \\mathbf{c}, \\mathbf{y}_{&lt;i})\\). Intuitively, this procedure identifies which tokens in \\(\\mathbf{c}\\) influence the increment of the probability for token \\(y_i\\) and the decrement of that for the non-contextual option \\(y^{\\setminus \\mathbf{c}}_i\\), as shown in Step 2 in Figure 5.3. Resulting scores are once again binarized with a selector \\(s_\\text{CCI}\\):\n\\[\n\\text{CCI}(y_i) = \\{\\,c_j\\,|\\,s_\\text{cci\\,}(a^i_j) = 1,\\;\\forall c_j \\in \\mathbf{c}\\}\n\\]\nThis results in pairs of context-sensitive generated tokens and the respective input-context tokens influencing their prediction:\n\\[\n\\mathcal{P} = \\big\\{ \\langle\\,y_i, c_j\\,\\rangle,\\; \\forall y_i \\in \\text{CTI}, \\forall c_j \\in \\text{CCI}(y_i)\\big\\}\n\\]\n\n5.3.1 From Granular Attributions to Document-level Citations\nCTI Filtering To obtain discrete labels from the CTI step, we set \\(s_\\text{cti}(m_i) = m_i \\geq m^*\\), where \\(m^*\\) is a threshold value for selecting context-sensitive generated tokens. We experiment with two variants of \\(m^*\\): a calibrated threshold \\(m_{\\text{cal}}^*\\) obtained by maximizing agreement between the contrastive metric and human annotations on a calibration set with human AA annotations, and an example-level threshold \\(m_\\text{ex}^*\\) using only within-example scores to avoid the need for calibration data. Following Sarti et al. (2024), we set \\(m_\\text{ex}^* = \\overline{\\mathbf{m}} + \\sigma_\\mathbf{m}\\), where \\(\\overline{\\mathbf{m}}\\) and \\(\\sigma_\\mathbf{m}\\) are the average and standard deviation of \\(\\mathbf{m}\\) scores for generated tokens.\nCCI Filtering To extract granular document citations (i.e., colored spans with document indices in Figure 5.1), we set \\(s_\\text{cci} = a^i_j \\geq a^{i*}\\), where \\(a^{i*}\\) is either the Top-K or Top-% highest attribution value in \\(\\mathbf{a}^i\\), to filter attributed context tokens \\(c_j \\in \\text{CCI}(y_i)\\). Then, we use the identifier \\(\\text{docid}(c_j)\\) of the documents they belong to as citation indices for context-sensitive token \\(y_i\\). Highlights for consecutive tokens citing the same documents are collated into a single span and mapped from subword to word level to facilitate interpretation.\nSentence-level Aggregation Following standard sentence-level AA practices, we aggregate token-level citations as the union over all cited documents \\(\\text{docid}(\\cdot)\\) across context-sensitive tokens in \\(\\mathbf{y}\\):\n\\[\n\\begin{gathered}\n    \\text{Mirage}(\\mathbf{y}) = \\bigcup_{y_i \\in \\text{CTI}(\\mathbf{y})} \\text{docid}(c_j)\\;\\forall c_j \\in \\text{CCI}(y_i) \\\\\n    \\text{with}\\;s_\\text{cti} = m_i \\geq m^*, s_\\text{cci} = a^i_j \\geq a^{i*}\n\\end{gathered}\n\\]\nIn the following sections, we use Mirage\\(_{\\text{cal}}\\) and Mirage\\(_{\\text{ex}}\\) to refer to sentence-level answer attribution using \\(m_{\\text{cal}}^*\\) and \\(m_{\\text{ex}}^*\\) thresholds, respectively.",
    "crumbs": [
      "Attributing Context Usage in Multilingual NLP",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Answer Attribution for Trustworthy Retrieval-Augmented Generation</span>"
    ]
  },
  {
    "objectID": "chapters/chap-5-mirage.html#sec-chap5-agreement",
    "href": "chapters/chap-5-mirage.html#sec-chap5-agreement",
    "title": "5  Answer Attribution for Trustworthy Retrieval-Augmented Generation",
    "section": "5.4 Agreement with Human Answer Attribution Annotations",
    "text": "5.4 Agreement with Human Answer Attribution Annotations\nWe begin our evaluation by comparing Mirage predictions to human-produced answer attributions. Importantly, our aim is not to compare several AA approaches to claim optimal faithfulness, but rather to evaluate how our proposed framework fares against existing approaches at the task of producing answer attributions from model internals. We employ the XOR-AttriQA dataset (Muller et al., 2023), which, to our knowledge, is the only open dataset with human annotations over RAG outputs produced by a publicly accessible LM.5\nWe limit our assessment to open-weights LLMs to ensure that Mirage answer attribution can faithfully reflect the model’s inner processing towards the natural production of the annotated answer used for evaluation. While these answers could be force-decoded from an open-source model to enable usage, such a procedure would likely impact the validity of AA, as the selected model would not naturally generate the forced answers. Moreover, while cross-linguality is not the focus of our work, XOR-AttriQA allows us to assess the robustness of Mirage across several languages and its agreement with human annotations compared to an entailment-based system.\n\n5.4.1 Experimental Setup\nXOR-AttriQA consists of 500/4720 validation/test tuples, each containing a concise factual query \\(\\mathbf{q}\\), a set of retrieved documents that we use as context \\(\\mathbf{c} = \\langle \\text{doc}_1, \\dots, \\text{doc}_k \\rangle\\), and a single-sentence answer \\(\\mathbf{y}\\) produced by an mT5-base model (Xue et al., 2021) fine-tuned on cross-lingual QA in a RAG setup (CORA; Asai et al. (2021)). Queries and documents span five languages —Bengali (BN), Finnish (FI), Japanese (JA), Russian (RU), and Telugu (TE)—and cross-lingual retrieval is allowed.\nAlthough the RAG generator employs a set of retrieved documents during generation, human annotators were asked to label tuples \\((\\mathbf{q}, \\text{doc}_i, \\mathbf{y})\\) to indicate whether the information in \\(\\text{doc}_i\\) supports the generation of \\(\\mathbf{y}\\).\n\n\n\n\n\n\nDataset\nBN\nFI\nJA\nRU\nTE\nTotal\n\n\n\n\nOrig.\n1407\n659\n1066\n954\n634\n4720\n\n\nMatch\n274\n214\n232\n254\n170\n1144\n\n\n\n\n\nTable 5.2: Statistic for test sets of the original XOR-AttriQA and XOR-AttriQA\\(_{\\text{match}}\\).\n\n\n\nNotably, Mirage requires extracting model internals in the naturalistic setting that leads to the generation of the desired answer, i.e., the one assessed by human annotators. Hence, we perform a selection procedure to identify XOR-AttriQA examples where the answer produced by filling in the concatenated documents \\(\\mathbf{c}\\) in the LM prompt matches the one provided. The resulting subset, which we dub XOR-AttriQA\\(_{\\text{match}}\\), contains 142/1144 calibration/test examples and is used for our evaluation. Replicating the original answer generation process is challenging since the original ordering of the documents \\(\\text{doc}_i\\) in \\(\\mathbf{c}\\) is unavailable.6 To maximize the chances of replication, we attempt to restore the original document sequence by randomly shuffling the order of \\(\\text{doc}_i\\)s until LLM can naturally predict the answer \\(\\mathbf{y}\\). The procedure adopted is described in Algorithm 1. The statistics of the original XOR-AttriQA and XOR-AttriQA\\(_{\\text{match}}\\) are shown in Table 5.2.\n\n\n\\begin{algorithm} \\caption{Restore document sequence producing the original annotated answer in XOR-AttriQA} \\begin{algorithmic} \\Require $\\{Doc_1, ..., Doc_n\\}$, $query$, $answer$, $\\mathbb{M}$ \\Procedure{RestoreSequence}{$\\{Doc_1, ..., Doc_n\\}, query, answer, \\mathbb{M}$} \\State $iter = 0, \\, found=False$ \\While {$iter &lt; 200$} \\State $pred = \\mathbb{M}(\\{Doc_1, ..., Doc_n\\}, query)$ \\If{$pred == answer$} \\State $found = True$, \\textbf{break} \\Else \\State ${\\rm Shuffle}(\\{Doc_1, ..., Doc_n\\})$ \\EndIf \\State iter += 1 \\EndWhile \\If{$found$} \\State \\textbf{return} $\\{Doc_1, ..., Doc_n\\}$ \\EndIf \\EndProcedure \\end{algorithmic} \\end{algorithm}\n\n\n\n\n5.4.2 Entailment-based Baselines\nMuller et al. (2023) use an mT5 XXL model fine-tuned on NLI for performing answer attribution on XOR-AttriQA. Since neither the tuned model nor the tuning data are released, we opt to use TRUE (Honovich et al., 2022), a fine-tuned T5 11B model (Raffel et al., 2020), which was shown to highly overlap with human annotation on English answer attribution tasks (Muller et al., 2023; Gao et al., 2023a). We evaluate TRUE agreement with human annotation in two setups. In NLI\\(_\\text{orig}\\), we evaluate the model directly on all examples, including non-English data. While this leads the English-centric TRUE model out of distribution, it accounts for real-world scenarios with noisy data, and can be used to assess the robustness of the method in less-resourced settings. Instead, in NLI\\(_\\text{mt}\\), all queries and documents are machine translated to English using the Google Translate API. While this simplifies the task by ensuring all TRUE inputs are in English, it can lead to information loss due to imprecise translation.\n\n\n5.4.3 Results and Analysis\nMirage agrees with human answer attribution Table 5.3 presents our results. Mirage is found to largely agree with human annotations on XOR-AttriQA\\(_{\\text{match}}\\), with scores on par or slightly better than those of the ad-hoc NLI\\(_\\text{mt}\\) system augmented with automatic translation. Although calibration appears to generally improve Mirage’s agreement with human annotators, we note that the uncalibrated Mirage\\(_{\\text{ex}}\\) achieves strong performances despite having no access to external modules or tuning data. These findings confirm that the inner workings of LMs can be used to perform answer attribution, yielding performances on par with supervised answer attribution approaches even in the absence of annotations for calibration.\n\n\n\n\n\n\nMethod\nExtra Req.\nCCI Filter\nBN\nFI\nJA\nRU\nTE\nAvg. / Std\n\n\n\n\nNLI\\(_{\\text{orig}}\\)\n11B NLI model\n--\n33.8\n83.7\n86.5\n85.8\n50.0\n68.0 / 21.9\n\n\nNLI\\(_{\\text{mt}}\\)\n11B NLI model + MT\n82.6\n83.7\n90.5\n81.7\n82.5\n84.2 / 3.2\n\n\nMirage\\(_{\\text{cal}}\\) (Ours)\n142 annotated AA ex.\nTop 3\n81.7\n84.2\n87.8\n83.3\n87.0\n84.8 / 2.3\n\n\nTop 5%\n84.4\n83.0\n91.4\n85.8\n88.9\n86.7 / 3.1\n\n\nMirage\\(_{\\text{ex}}\\) (Ours)\n--\nTop 3\n80.2\n78.5\n83.8\n77.2\n75.2\n79.0 / 2.9\n\n\nTop 5%\n81.7\n80.1\n89.2\n84.4\n81.8\n83.4 / 3.2\n\n\n\n\n\nTable 5.3: Agreement % of Mirage and entailment-based baselines with human AA on XOR-AttriQA\\(_{\\text{match}}\\) using CORA for RAG. Extra Req.: data/models needed for AA in addition to the RAG model and the current example. Filter: \\(s_\\text{cci}\\) filtering for saliency scores. Best overall and best uncalibrated scores are highlighted.\n\n\n\nMirage is robust across languages and filtering procedures Table 5.3 shows that NLI\\(_\\text{orig}\\) answer attribution performances are largely language-dependent due to the unbalanced multilingual abilities of the TRUE NLI model. This highlights the brittleness of entailment-based approaches in OOD settings, as discussed in Section 5.2.1. Instead, Mirage variants perform similarly across all languages by exploiting the internals of the multilingual RAG model. Mirage’s performance across languages is comparable to that of NLI\\(_\\text{mt}\\), which requires an extra translation step to operate on English inputs.\nWe further validate the robustness of the CCI filtering process by testing percentile values between Top 3-100% for the Mirage\\(_{\\text{ex}}\\) setting. Figure 5.4 shows that Top % values between 3 and 20% lead to a comparably high agreement with human annotation, suggesting this filtering threshold can be selected without ad-hoc parameter tuning.\n\n\n\n\n\n\nFigure 5.4: Robustness of Mirage\\(_{\\text{ex}}\\) agreement with human annotations across Top-% CCI filtering thresholds.",
    "crumbs": [
      "Attributing Context Usage in Multilingual NLP",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Answer Attribution for Trustworthy Retrieval-Augmented Generation</span>"
    ]
  },
  {
    "objectID": "chapters/chap-5-mirage.html#sec-chap5-eli5-evaluation",
    "href": "chapters/chap-5-mirage.html#sec-chap5-eli5-evaluation",
    "title": "5  Answer Attribution for Trustworthy Retrieval-Augmented Generation",
    "section": "5.5 Answer Attribution for Long-form QA",
    "text": "5.5 Answer Attribution for Long-form QA\nXOR-AttriQA can only provide limited insights for real-world answer attribution evaluation, as its examples are sourced from Wikipedia articles and its answers are very concise. In this section, we extend our evaluation to ELI5 (Fan et al., 2019), a challenging long-form QA dataset that was recently employed to evaluate LLM self-citation capabilities (Gao et al., 2023a). Different from XOR-AttriQA, ELI5 answers are expected to contain multiple sentences of variable length, making it especially fitting to assess Mirage context-sensitive token identification capabilities before document attribution. Alongside our quantitative assessment of Mirage in relation to self-citation baselines, we conduct a qualitative evaluation of the disagreement between the two methods.\n\n5.5.1 Experimental Setup\nDataset The ELI5 dataset contains open-ended why/how/what queries \\(\\mathbf{q}\\) from the “Explain Like I’m Five” subreddit eliciting long-form multi-sentence answers. For our evaluation, we use the RAG-adapted ELI5 version by Gao et al. (2023a), containing top-5 matching documents \\(\\mathbf{c} = \\langle \\text{doc}_1, \\dots, \\text{doc}_5 \\rangle\\) retrieved from a filtered version of the Common Crawl (Sphere; Piktus et al. (2021)) for every query. The answer attribution task is performed by generating a multi-sentence answer \\(\\mathbf{ans} = \\langle \\mathbf{y}_1, \\dots, \\mathbf{y}_m \\rangle\\) with an LLM using \\((\\mathbf{q}, \\mathbf{c})\\) as inputs, and identifying documents in \\(\\mathbf{c}\\) supporting the generation of answer sentence \\(\\mathbf{y}_i,\\,\\forall \\mathbf{y}_i \\in \\mathbf{ans}\\).\nModels and Answer Attribution Procedure We select LLaMA 2 7B Chat (Touvron et al., 2023) and Zephyr \\(\\beta\\) 7B (Tunstall et al., 2024) for our experiments since they are high-quality open-source LLMs of manageable size. To enable a fair comparison between the tested attribution methods, we first generate answers with inline citations using the self-citation prompt by Gao et al. (2023b). Then, we remove citation tags and use Mirage to attribute the resulting answers to retrieved documents. This process ensures that citation quality is compared over the same set of answers, controlling for the variability that a different prompt could produce. For more robust results, we perform generation three times using different sampling seeds, and report the averaged scores. Since human-annotated data is not available, we only assess the calibration-free Mirage\\(_{\\text{ex}}\\).\nEntailment-based Evaluation Differently from the XOR-AttriQA dataset used in Section 5.4, ELI5 does not contain human annotations of AA. For this reason, and to ensure consistency with Gao et al. (2023a) self-citation assessment, we adopt the TRUE model as a high-quality approximation of expected annotation behavior. Despite the potential OOD issues of entailment-based AA highlighted in Section 5.4, we expect TRUE to perform well on ELI5 since it closely matches the general/scientific knowledge queries in TRUE’s fine-tuning corpora and contains only English sentences. To overcome the multi-hop issue when using single documents for entailment-based answer attribution, we follow the ALCE evaluation (Gao et al., 2023a) to measure citation quality as NLI precision and recall (summarized by F1 scores) over the concatenation of retrieved documents. The ALCE framework for RAG QA evaluation assesses the LLMs’ responses from three viewpoints: citation quality, correctness, and fluency. Citation quality evaluates the answer attribution performance with recall and precision scores. The recall score calculates if the concatenation of the cited documents entails the generated sentence. The precision measures whether each document is cited precisely by verifying if the concatenated text still entails the generation whenever one of the documents is removed. We further calculate F1 scores to summarize the overall performance.\n\n\n5.5.2 Results\nResults in Table 5.4 show that Mirage provides a significant boost in answer attribution precision and recall for the Zephyr \\(\\beta\\) model. At the same time, it greatly improves citation recall at the expense of precision for LLaMA 2, resulting in an overall higher F1 score for the Mirage\\(_{\\text{ex}}\\) Top 5% setting. These results confirm that Mirage can produce effective answer attributions in longer and more complex settings while employing no external resources like the self-citation approach.\n\n\n\n\n\n\nModel\nAnswer Attrib.\nCitation\n\n\nPrec.\nRec.\nF1\n\n\n\n\nZephyr \\(\\beta\\)\nSelf-citation\n41.4\n24.3\n30.6\n\n\nMirage\\(_{\\text{ex}}\\) Top 3\n38.3\n46.2\n41.9\n\n\nMirage\\(_{\\text{ex}}\\) Top 5%\n44.7\n46.5\n45.6\n\n\nLLaMA 2\nSelf-citation\n37.9\n19.8\n26.0\n\n\nMirage\\(_{\\text{ex}}\\) Top 3\n21.8\n29.6\n25.1\n\n\nMirage\\(_{\\text{ex}}\\) Top 5%\n26.2\n29.1\n27.6\n\n\n\n\n\nTable 5.4: Answer attribution quality estimated by TRUE for self-citation and Mirage on ELI5.\n\n\n\nFrom the comparison between Top 3 and Top 5% CCI filtering strategies, we note that the latter generally results in better performance. This intuitively supports the idea that an adaptive selection strategy is more suitable for accommodating the wide variability of attribution scores across different examples. Figure 5.5 visualizes the distributions of attribution scores \\(a^i_j\\) for an answer produced by Zephyr \\(\\beta\\), showing that most context tokens in retrieved documents receive low attribution scores, with only a handful of them contributing to the prediction of the context-sensitive token ‘9’ in the generation. This example also provides an intuitive explanation of the robustness of Top-% selection thresholds discussed in Section 5.4.3. Ultimately, the Top 5% threshold is sufficient to select the document containing the direct mention of the generated token.\n\n\n\n\n\n\nFigure 5.5: Attribution scores over retrieved documents’ tokens for the prediction of context-sensitive token ‘9’.\n\n\n\nSince the \\(m^*_\\text{ex}\\) threshold used to select context-sensitive tokens by Mirage\\(_{\\text{ex}}\\) depends on the mean and standard deviation of generated answer’s scores, we expect that the length of the generated answer might play a role in citation quality. As shown in Figure 5.6, Mirage citation quality is indeed lower for shorter answer sentences. However, a similar trend is observed for self-citation, which is outperformed by Mirage for all but the shortest length bin (\\(\\leq 10\\) tokens). The proportion of non-attributed sentences (red line) suggests that the lower quality could be a byproduct of the ALCE evaluation protocol, where non-attributed sentences receive zero precision/recall. Future availability of human-annotated RAG datasets may shed more light on this effect.\n\n\n\n\n\n\nFigure 5.6: Mirage\\(_\\text{ex}\\) (top) and self-citation (bottom) average performance on ELI5 answer sentences binned by length. Red: Percentage of sentences with \\(\\geq 1\\) citations.\n\n\n\n\n\n5.5.3 Qualitative Analysis of Disagreements\nTo better understand Mirage’s performance, we examine some ELI5 examples where Mirage disagrees with self-citation on Zephyr \\(\\beta\\)’s generations. Table 5.5 and Table 5.6 illustrate two cases in which the entailment-based TRUE model results agree with either Mirage or self-citation.\nIn Table 5.5, the answer provided by the model is directly supported by Document [1], as also identified by TRUE. However, self-citation fails to cite the related document at the end of the two sentences. By contrast, Mirage attributes several spans to Document [1], resulting in the correct answer attribution for both sentences.\n\n\n\n\n\n\nInput: Prompt + Retrieved docs (N=5) + Query\n\n\nDocument [1] […] Q. What does it mean for books to be Sensitized or Desensitized? A security strip is embedded into each library book. When a book is checked out, it must be ‘desensitized’ so that it will not set off the alarm when it leaves or enters the library. When the book is returned, it is ‘sensitized’ so that the alarm will be set off should someone attempt to take the book from the library without going through the proper borrowing procedure.\n\n\nDocument [2] […]\n\n\nQuery: How does a small paper library bar code set off the security alarm when you haven’t checked a book out?\n\n\nAnswer attribution results\n\n\nSelf-citation: […] When a book is checked out, it is ‘desensitized’ to prevent the alarm from going off. [\\(\\varnothing\\)] When the book is returned, it is ‘sensitized’ so the alarm will sound if the item is taken without authorization. [\\(\\varnothing\\)] […]\n\n\nMirage: […] When a book is checked\\(^{(1)}\\) out, it\\(^{(1)}\\) is ‘desensitized’\\(^{(1)}\\) to prevent the alarm from going off. [1] When the book\\(^{(1)}\\) is returned, it is ‘sensitized’\\(^{(1)}\\) so the alarm will sound if the item is taken without authorization. [1] […]\n\n\nNLI (True model): [1] entails both answer sentences.\n\n\n\n\n\nTable 5.5: Example of self-citation failure using Zephyr \\(\\beta\\) on ELI5. NLI and Mirage produce the correct citation, while self-citation does not cite any document ([\\(\\varnothing\\)]).\n\n\n\nWhile TRUE achieves high consistency with human judgment (e.g., for the example in Table 5.5), NLI-based AA can still prove unreliable in cases of high lexical overlap between the answer and supporting documents. Table 5.6 illustrates one such case, where both self-citation and TRUE attribute the answer to Document [3], whereas Mirage does not label any context document as salient for the answer. Here, the answer wrongly states that the bar code can be used to prevent the alarm, while Document [3] mentions that the code can be used to cancel the alarm after an accidental activation. Thus, despite the high lexical and semantic relatedness, the answer is not supported by Document [3]. The failure of TRUE in this setting highlights the sensitivity of entailment-based systems to surface-level similarity, making them brittle in cases where the model’s context usage is not straightforward. Using another sampling seed for the same query produces the answer “[…] the individual can cancel the alarm by providing their password at the keypad”, which Mirage correctly attributes to Document [3].\n\n\n\n\n\n\nInput: Prompt + Retrieved docs (N=5) + Query\n\n\nDocument [2] […]\n\n\nDocument [3] […] What will happen if you accidentally set off your security system? The siren will sound and it will be loud, but you should be able to stop the siren by entering your code into your keypad. […]\n\n\nDocument [4] […]\n\n\nQuery: How does a small paper library bar code set off the security alarm when you haven’t checked a book out?\n\n\nAnswer attribution results\n\n\nSelf-citation: […] False alarms can be prevented by entering the code on the keypad, as documented in [3]. […]\n\n\nMirage: […] False alarms can be prevented by entering the code on the keypad [\\(\\varnothing\\)] […]\n\n\nNLI (True model): [3] entails the answer sentence.\n\n\n\n\n\nTable 5.6: Example showcasing the brittleness of entailment-based AA. Mirage correctly finds that the answer cannot be attributed ([\\(\\varnothing\\)]), while NLI and self-citation attribute the lexically similar Document [3].",
    "crumbs": [
      "Attributing Context Usage in Multilingual NLP",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Answer Attribution for Trustworthy Retrieval-Augmented Generation</span>"
    ]
  },
  {
    "objectID": "chapters/chap-5-mirage.html#limitations",
    "href": "chapters/chap-5-mirage.html#limitations",
    "title": "5  Answer Attribution for Trustworthy Retrieval-Augmented Generation",
    "section": "5.6 Limitations",
    "text": "5.6 Limitations\nWe now highlight some limitations of the Mirage method and our experimental evaluation, which should be addressed in future work.\nLLMs Optimized for Self-citation Our analysis focuses specifically on models that are not explicitly trained to perform self-citation and can provide citations only when prompted to do so. While recent systems incorporate self-citation into their optimization scheme for RAG applications, incorporating model internals into the attribution process will remain a valuable and inexpensive method to ensure faithful answer attributions.\nBrittleness of NLI-based Evaluation Following Gao et al. (2023a), the evaluation of Section 5.5 employs the NLI-based system TRUE due to the lack of AA-annotated answers produced by open-source LLMs. However, using the predictions of NLI models as AA references is far from ideal, given their brittleness in challenging scenarios and their tendency to exploit shallow heuristics. While the ELI5 dataset is reasonably in-domain for the TRUE model, this factor might still undermine the reliability of some of our quantitative evaluation results. Future work should produce a wider variety of annotated datasets for reproducible answer attribution using open-source LLMs, enabling us to extend our analysis to a broader set of languages and model sizes and ultimately enhance the robustness of our findings.\nApplicability to Other Domains and Models Our evaluation is conducted on relatively homogeneous QA datasets and does not include language models with &gt;7B parameters. This limits the generalizability of our findings to other domains and larger models. Future work should extend our analysis to a broader range of domains and model sizes to further validate the robustness and applicability of Mirage. This said, we expect Mirage to be less vulnerable to language and quality shifts compared to existing AA methods that depend on external validators or on the model’s instruction-following abilities.\nScalability on Longer Context The computational cost for the simple gradient-based version of Mirage we propose is \\(2O(F)+|\\text{CTI}(\\mathbf{y})| \\cdot O(B)\\), where \\(O(F), O(B)\\) are respectively the costs of a forward and a backward pass with the LLM, and \\(|\\text{CTI}(\\mathbf{y})|\\) is the number of tokens selected by the CTI step. While CTI effectively limits the expensive backward component in the Mirage computation, its cost is bound to increase significantly for larger models and context sizes. When applying Mirage to LLMs with &lt;10B parameters, we note that its cost can be comparable or lower to supervised models like TRUE, requiring several forward passes using a large 11B LLM. Importantly, Mirage is a flexible framework that can be implemented using different input attribution methods in the CCI step, including lightweight techniques that require only forward passes, such as Attention Rollout (Abnar and Zuidema, 2020), Value Zeroing (Mohebbi et al., 2023), or ALTI-Logit (Ferrando et al., 2023). Finally, a promising perspective for scaling to larger LLMs could be to assess whether Mirage-produced AAs remain accurate when force-decoding the original model’s answer from a different LLM with fewer parameters.\nParametrization and Choice of Attribution Method While Section 5.4 highlights the robustness of Mirage to various CCI filtering thresholds, the method still requires non-trivial parametrization. In particular, we emphasize that the choice of the attribution method employed to generate attribution scores in the CCI step can significantly impact the faithfulness of the resulting answer attributions. Although we used a relatively simple gradient-based approach, our proposed framework is method-agnostic. We leave the evaluation of modern input attribution techniques, such as the ones mentioned in the previous paragraph, to future work to further improve Mirage applicability in real-world settings.",
    "crumbs": [
      "Attributing Context Usage in Multilingual NLP",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Answer Attribution for Trustworthy Retrieval-Augmented Generation</span>"
    ]
  },
  {
    "objectID": "chapters/chap-5-mirage.html#conclusion",
    "href": "chapters/chap-5-mirage.html#conclusion",
    "title": "5  Answer Attribution for Trustworthy Retrieval-Augmented Generation",
    "section": "5.7 Conclusion",
    "text": "5.7 Conclusion\nIn this chapter, we introduced Mirage, a novel approach to enhance the faithfulness of answer attribution in RAG systems. By leveraging model internals, Mirage effectively addresses the limitations of previous methods based on prompting or external NLI validators. Our experiments demonstrate that Mirage produces outputs that strongly agree with human annotations while being more efficient and controllable than its counterparts. Our qualitative analysis shows that Mirage can produce faithful attributions that reflect actual context usage during generation, reducing the risk of false positives motivated by surface-level similarity. Overall, Mirage represents a promising first step in exploiting interpretability insights to develop faithful answer attribution methods, paving the way for the usage of LLM-powered question-answering systems in real-world, user-facing applications.\nIn the next part of this thesis, we will move beyond analysis-driven methods to study how prompting (Chapter 6) and interpretability-based methods (Chapter 7) can be used to effectively condition the machine translation generation process.\n\n\n\n\nSamira Abnar and Willem Zuidema. 2020. Quantifying attention flow in transformers. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault, editors, Proceedings of the 58th annual meeting of the association for computational linguistics, pages 4190–4197, Online. Association for Computational Linguistics.\n\n\nChirag Agarwal, Sree Harsha Tanneru, and Himabindu Lakkaraju. 2024. Faithfulness vs. Plausibility: On the (un)reliability of explanations from large language models. Arxiv.\n\n\nSimone Alghisi, Massimo Rizzoli, Gabriel Roccabruna, Seyed Mahed Mousavi, and Giuseppe Riccardi. 2024. Should we fine-tune or RAG? Evaluating different techniques to adapt LLMs for dialogue. In Saad Mahamood, Nguyen Le Minh, and Daphne Ippolito, editors, Proceedings of the 17th international natural language generation conference, pages 180–197, Tokyo, Japan. Association for Computational Linguistics.\n\n\nRohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. 2023. Palm 2 technical report. Arxiv.\n\n\nAkari Asai, Xinyan Yu, Jungo Kasai, and Hanna Hajishirzi. 2021. One question answering model for many languages with cross-lingual dense passage retrieval. Advances in Neural Information Processing Systems, 34:7547–7560.\n\n\nPepa Atanasova, Oana-Maria Camburu, Christina Lioma, Thomas Lukasiewicz, Jakob Grue Simonsen, and Isabelle Augenstein. 2023. Faithfulness tests for natural language explanations. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st annual meeting of the association for computational linguistics (volume 2: Short papers), pages 283–294, Toronto, Canada. Association for Computational Linguistics.\n\n\nBernd Bohnet, Vinh Q. Tran, Pat Verga, Roee Aharoni, Daniel Andor, Livio Baldini Soares, Jacob Eisenstein, Kuzman Ganchev, Jonathan Herzig, Kai Hui, Tom Kwiatkowski, Ji Ma, Jianmo Ni, Tal Schuster, William W. Cohen, Michael Collins, Dipanjan Das, Donald Metzler, Slav Petrov, et al. 2022. Attributed question answering: Evaluation and modeling for attributed large language models. ArXiv.\n\n\nSebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego De Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, et al. 2022. Improving language models by retrieving from trillions of tokens. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th international conference on machine learning, volume 162, pages 2206–2240. PMLR.\n\n\nStephen Casper, Carson Ezell, Charlotte Siegmann, Noam Kolt, Taylor Lynn Curtis, Benjamin Bucknall, Andreas Haupt, Kevin Wei, Jérémy Scheurer, Marius Hobbhahn, Lee Sharkey, Satyapriya Krishna, Marvin Von Hagen, Silas Alberti, Alan Chan, Qinyi Sun, Michael Gerovitch, David Bau, Max Tegmark, et al. 2024. Black-box access is insufficient for rigorous AI audits. In Proceedings of the 2024 ACM conference on fairness, accountability, and transparency, pages 2254–2272, New York, NY, USA. Association for Computing Machinery.\n\n\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, et al. 2024. Scaling instruction-finetuned language models. Journal of Machine Learning Research, 25(70):1–53.\n\n\nBenjamin Cohen-Wang, Harshay Shah, Kristian Georgiev, and Aleksander Mądry. 2024. ContextCite: Attributing model generation to context. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, Advances in neural information processing systems, volume 37, pages 95764–95807. Curran Associates, Inc.\n\n\nAlexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams, Samuel Bowman, Holger Schwenk, and Veselin Stoyanov. 2018. XNLI: Evaluating cross-lingual sentence representations. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun’ichi Tsujii, editors, Proceedings of the 2018 conference on empirical methods in natural language processing, pages 2475–2485, Brussels, Belgium. Association for Computational Linguistics.\n\n\nXuan-Quy Dao and Ngoc-Bich Le. 2023. Chatgpt is good but bing chat is better for vietnamese students. Arxiv.\n\n\nAngela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. 2019. ELI5: Long form question answering. In Anna Korhonen, David Traum, and Lluís Màrquez, editors, Proceedings of the 57th annual meeting of the association for computational linguistics, pages 3558–3567, Florence, Italy. Association for Computational Linguistics.\n\n\nJavier Ferrando, Gerard I. Gállego, Ioannis Tsiamas, and Marta R. Costa-jussà. 2023. Explaining how transformers use context to build predictions. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st annual meeting of the association for computational linguistics (volume 1: Long papers), pages 5486–5513, Toronto, Canada. Association for Computational Linguistics.\n\n\nTianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen. 2023a. Enabling large language models to generate text with citations. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 conference on empirical methods in natural language processing, pages 6465–6488, Singapore. Association for Computational Linguistics.\n\n\nYunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen Wang. 2023b. Retrieval-augmented generation for large language models: A survey. ArXiv.\n\n\nOr Honovich, Roee Aharoni, Jonathan Herzig, Hagai Taitelbaum, Doron Kukliansy, Vered Cohen, Thomas Scialom, Idan Szpektor, Avinatan Hassidim, and Yossi Matias. 2022. TRUE: Re-evaluating factual consistency evaluation. In Marine Carpuat, Marie-Catherine de Marneffe, and Ivan Vladimir Meza Ruiz, editors, Proceedings of the 2022 conference of the north american chapter of the association for computational linguistics: Human language technologies, pages 3905–3920, Seattle, United States. Association for Computational Linguistics.\n\n\nJing Huang, Atticus Geiger, Karel D’Oosterlinck, Zhengxuan Wu, and Christopher Potts. 2023. Rigorously assessing natural language explanations of neurons. In Yonatan Belinkov, Sophie Hao, Jaap Jumelet, Najoung Kim, Arya McCarthy, and Hosein Mohebbi, editors, Proceedings of the 6th BlackboxNLP workshop: Analyzing and interpreting neural networks for NLP, pages 317–331, Singapore. Association for Computational Linguistics.\n\n\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. 2023. Atlas: Few-shot learning with retrieval augmented language models. Journal of Machine Learning Research, 24(251):1–43.\n\n\nKalpesh Krishna, Aurko Roy, and Mohit Iyyer. 2021. Hurdles to progress in long-form question answering. In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou, editors, Proceedings of the 2021 conference of the north american chapter of the association for computational linguistics: Human language technologies, pages 4940–4957, Online. Association for Computational Linguistics.\n\n\nSolomon Kullback and Richard A Leibler. 1951. On information and sufficiency. The annals of mathematical statistics, 22(1):79–86.\n\n\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-augmented generation for knowledge-intensive NLP tasks. In Proceedings of the 34th international conference on neural information processing systems, Red Hook, NY, USA. Curran Associates Inc.\n\n\nNelson Liu, Tianyi Zhang, and Percy Liang. 2023. Evaluating verifiability in generative search engines. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Findings of the association for computational linguistics: EMNLP 2023, pages 7001–7025, Singapore. Association for Computational Linguistics.\n\n\nCheng Luo, Wei Liu, Jieyu Lin, Jiajie Zou, Ming Xiang, and Nai Ding. 2022. Simple but challenging: Natural language inference models fail on simple sentences. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Findings of the association for computational linguistics: EMNLP 2022, pages 3449–3462, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\n\n\nLijia Ma, Xingchen Xu, and Yong Tan. 2024. Crafting knowledge: Exploring the creative mechanisms of chat-based search engines. Arxiv.\n\n\nAndreas Madsen, Sarath Chandar, and Siva Reddy. 2024. Are self-explanations from large language models faithful? In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Findings of the association for computational linguistics: ACL 2024, pages 295–337, Bangkok, Thailand. Association for Computational Linguistics.\n\n\nAndreas Madsen, Nicholas Meade, Vaibhav Adlakha, and Siva Reddy. 2022. Evaluating the faithfulness of importance measures in NLP by recursively masking allegedly important tokens and retraining. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Findings of the association for computational linguistics: EMNLP 2022, pages 1731–1751, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\n\n\nR. Thomas McCoy, Ellie Pavlick, and Tal Linzen. 2019. Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference. In Anna Korhonen, David Traum, and Lluís Màrquez, editors, Proceedings of the 57th annual meeting of the association for computational linguistics, pages 3428–3448, Florence, Italy. Association for Computational Linguistics.\n\n\nJacob Menick, Maja Trebacz, Vladimir Mikulik, John Aslanides, Francis Song, Martin Chadwick, Mia Glaese, Susannah Young, Lucy Campbell-Gillingham, Geoffrey Irving, et al. 2022. Teaching language models to support answers with verified quotes. Arxiv.\n\n\nHosein Mohebbi, Willem Zuidema, Grzegorz Chrupała, and Afra Alishahi. 2023. Quantifying context mixing in transformers. In Andreas Vlachos and Isabelle Augenstein, editors, Proceedings of the 17th conference of the european chapter of the association for computational linguistics, pages 3378–3400, Dubrovnik, Croatia. Association for Computational Linguistics.\n\n\nNorman Mu, Sarah Chen, Zifan Wang, Sizhe Chen, David Karamardian, Lulwa Aljeraisy, Dan Hendrycks, and David Wagner. 2023. Can LLMs follow simple rules? Arxiv.\n\n\nBenjamin Muller, John Wieting, Jonathan Clark, Tom Kwiatkowski, Sebastian Ruder, Livio Soares, Roee Aharoni, Jonathan Herzig, and Xinyi Wang. 2023. Evaluating and modeling attribution for cross-lingual question answering. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 conference on empirical methods in natural language processing, pages 144–157, Singapore. Association for Computational Linguistics.\n\n\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. 2021. Webgpt: Browser-assisted question-answering with human feedback. Arxiv.\n\n\nYixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. 2020. Adversarial NLI: A new benchmark for natural language understanding. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault, editors, Proceedings of the 58th annual meeting of the association for computational linguistics, pages 4885–4901, Online. Association for Computational Linguistics.\n\n\nOpenAI. 2023. Gpt-4 technical report. Arxiv.\n\n\nFabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim Rocktäschel, Yuxiang Wu, Alexander H. Miller, and Sebastian Riedel. 2020. How context affects language models’ factual predictions. In Automated knowledge base construction.\n\n\nAnirudh Phukan, Shwetha Somasundaram, Apoorv Saxena, Koustava Goswami, and Balaji Vasan Srinivasan. 2024. Peering into the mind of language models: An approach for attribution in contextual question answering. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Findings of the association for computational linguistics: ACL 2024, pages 11481–11495, Bangkok, Thailand. Association for Computational Linguistics.\n\n\nAleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Dmytro Okhonko, Samuel Broscheit, Gautier Izacard, Patrick Lewis, Barlas Oğuz, Edouard Grave, Wen-tau Yih, et al. 2021. The web is your oyster-knowledge-intensive NLP against a very large web corpus. Arxiv.\n\n\nJirui Qi^*, Gabriele Sarti^*, Raquel Fernández, and Arianna Bisazza. 2024. Model internals-based answer attribution for trustworthy retrieval-augmented generation. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Proceedings of the 2024 conference on empirical methods in natural language processing, pages 6037–6053, Miami, Florida, USA. Association for Computational Linguistics.\n\n\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of machine learning research, 21(140):1–67.\n\n\nKorbinian Randl, John Pavlopoulos, Aron Henriksson, and Tony Lindgren. 2025. Evaluating the reliability of self-explanations in large language models. In Discovery science: 27th international conference, pages 36–51, Berlin, Heidelberg. Springer-Verlag.\n\n\nHannah Rashkin, Vitaly Nikolaev, Matthew Lamm, Lora Aroyo, Michael Collins, Dipanjan Das, Slav Petrov, Gaurav Singh Tomar, Iulia Turc, and David Reitter. 2023. Measuring attribution in natural language generation models. Computational Linguistics, 49(4):777–840.\n\n\nRuiyang Ren, Yuhao Wang, Yingqi Qu, Wayne Xin Zhao, Jing Liu, Hua Wu, Ji-Rong Wen, and Haifeng Wang. 2025. Investigating the factual knowledge boundary of large language models with retrieval augmentation. In Owen Rambow, Leo Wanner, Marianna Apidianaki, Hend Al-Khalifa, Barbara Di Eugenio, and Steven Schockaert, editors, Proceedings of the 31st international conference on computational linguistics, pages 3697–3715, Abu Dhabi, UAE. Association for Computational Linguistics.\n\n\nVictor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, et al. 2022. Multitask prompted training enables zero-shot task generalization. In Proceedings of the tenth international conference on learning representations (ICLR).\n\n\nGabriele Sarti, Grzegorz Chrupała, Malvina Nissim, and Arianna Bisazza. 2024. Quantifying the plausibility of context reliance in neural machine translation. In The twelfth international conference on learning representations (ICLR 2024), Vienna, Austria. OpenReview.\n\n\nKoustuv Sinha, Prasanna Parthasarathi, Joelle Pineau, and Adina Williams. 2021. UnNatural Language Inference. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, Proceedings of the 59th annual meeting of the association for computational linguistics and the 11th international joint conference on natural language processing (volume 1: Long papers), pages 7329–7346, Online. Association for Computational Linguistics.\n\n\nHugo Touvron, Louis Martin, Kevin R. Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Daniel M. Bikel, Lukas Blecher, Cristian Cantòn Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. ArXiv.\n\n\nLewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Clémentine Fourrier, Nathan Habib, Nathan Sarrazin, Omar Sanseviero, Alexander M. Rush, and Thomas Wolf. 2024. Zephyr: Direct distillation of LM alignment. In Proceedings of the 1st conference on language modeling (COLM).\n\n\nJohannes Welbl, Pontus Stenetorp, and Sebastian Riedel. 2018. Constructing datasets for multi-hop reading comprehension across documents. Transactions of the Association for Computational Linguistics, 6:287–302.\n\n\nFangyuan Xu, Yixiao Song, Mohit Iyyer, and Eunsol Choi. 2023. A critical evaluation of evaluations for long-form question answering. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st annual meeting of the association for computational linguistics (volume 1: Long papers), pages 3225–3245, Toronto, Canada. Association for Computational Linguistics.\n\n\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2021. MT5: A massively multilingual pre-trained text-to-text transformer. In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou, editors, Proceedings of the 2021 conference of the north american chapter of the association for computational linguistics: Human language technologies, pages 483–498, Online. Association for Computational Linguistics.\n\n\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun’ichi Tsujii, editors, Proceedings of the 2018 conference on empirical methods in natural language processing, pages 2369–2380, Brussels, Belgium. Association for Computational Linguistics.\n\n\nKayo Yin and Graham Neubig. 2022. Interpreting language models with contrastive explanations. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Proceedings of the 2022 conference on empirical methods in natural language processing, pages 184–198, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\n\n\nXiang Yue, Boshi Wang, Ziru Chen, Kai Zhang, Yu Su, and Huan Sun. 2023. Automatic evaluation of attribution by large language models. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Findings of the association for computational linguistics: EMNLP 2023, pages 4615–4635, Singapore. Association for Computational Linguistics.",
    "crumbs": [
      "Attributing Context Usage in Multilingual NLP",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Answer Attribution for Trustworthy Retrieval-Augmented Generation</span>"
    ]
  },
  {
    "objectID": "chapters/chap-5-mirage.html#footnotes",
    "href": "chapters/chap-5-mirage.html#footnotes",
    "title": "5  Answer Attribution for Trustworthy Retrieval-Augmented Generation",
    "section": "",
    "text": "We use the term answer attribution (AA) when referring to the task of citing relevant sources to distinguish it from the input attribution methods used in Mirage.↩︎\nCode and data released at https://github.com/Betswish/MIRAGE. A demo for Mirage using the Inseq attribute-context API is available at https://hf.co/spaces/gsarti/mirage.↩︎\nPopular frameworks such as LangChain and LlamaIndex support similarity-based citations using vector databases.↩︎\nAuthors suggest a minimum of 32 different ablations.↩︎\nE.g., the human-annotated answers in Bohnet et al. (2022) were generated by PALM 540B (Anil et al., 2023), whose internals are inaccessible. See Section A.3.1 for a comparison.↩︎\nMuller et al. (2023) only provide the split documents without the original ordering.↩︎",
    "crumbs": [
      "Attributing Context Usage in Multilingual NLP",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Answer Attribution for Trustworthy Retrieval-Augmented Generation</span>"
    ]
  },
  {
    "objectID": "chapters/chap-6-ramp.html",
    "href": "chapters/chap-6-ramp.html",
    "title": "6  Retrieval and Marking for Attribute-Controlled Translation",
    "section": "",
    "text": "6.1 Introduction\nText style transfer (TST) is a task that aims to control stylistic attributes of an input text without affecting its semantic content (Jin et al., 2022). Research in TST has focused mainly on English, thanks to the availability of large monolingual English datasets that cover stylistic attributes such as formality and simplicity (Rao and Tetreault, 2018; Zhu et al., 2010). In recent years, however, multilingual and cross-lingual applications of TST have steadily gained popularity (Briakou et al., 2021; Garcia et al., 2021; Krishna et al., 2022). A notable instance of cross-lingual TST is attribute-controlled translation (ACT), in which attribute1 conditioning is performed alongside machine translation to ensure that translations are not only correct but match user-specified preferences, such as formality/honorifics (Sennrich et al., 2016; Niu et al., 2017; Michel and Neubig, 2018; Niu and Carpuat, 2020; Nadejde et al., 2022; Wang et al., 2023), gender (Rabinovich et al., 2017; Vanmassenhove et al., 2018; Saunders and Byrne, 2020), and length (Lakew et al., 2019; Schioppa et al., 2021). ACT is crucial for sectors such as customer service and business communication, where stylistic differences can significantly impact user perception (e.g., misgendering customers or speaking to them in an inappropriately informal tone can be perceived as offensive or disconcerting). Table 6.1 shows examples of ACT for formality and gender attributes.\nMost prior work on ACT relies on a supervised adaptation component that conditions the generative model on the selective attribute. However, few annotated ACT datasets are available, and they generally cover only a limited set of languages and attributes. Thus, enabling few-shot or zero-shot ACT would facilitate applying attribute control to less-resourced attributes and languages.\nAs a first step into our investigation of conditioning machine translation generation, this chapter introduces a new approach for ACT: Retrieval and Attribute-Marking enhanced Prompting (Ramp). Recent studies have shown that large language models can perform MT out of the box using the prompting paradigm (Brown et al., 2020; Lin et al., 2022; Chowdhery et al., 2023). We build on this, prompting LLMs to perform attribute-controlled MT through two innovations: (1) retrieval of similar examples and (2) explicit attribute marking.\nRecent works adopting the prompting paradigm for text style transfer have mainly focused on the generalization capabilities of large English-centric LMs for zero-shot style transfer using previously unseen style descriptions (Suzgun et al., 2022; Reif et al., 2022). However, prior work on other NLP tasks has shown that cross-lingual prompting of multilingual LLMs can be effective (Zhao and Schütze, 2021; Zhou et al., 2023; Huang et al., 2022). As such, we leverage multilingual LLMs and extend their ACT capabilities cross-lingually to languages not covered by the in-context examples, thus enabling zero-shot ACT.",
    "crumbs": [
      "Conditioning Generation for Personalized Machine Translation",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Retrieval and Marking for Attribute-Controlled Translation</span>"
    ]
  },
  {
    "objectID": "chapters/chap-6-ramp.html#sec-chap6-introduction",
    "href": "chapters/chap-6-ramp.html#sec-chap6-introduction",
    "title": "6  Retrieval and Marking for Attribute-Controlled Translation",
    "section": "",
    "text": "Formality-Controlled Translation (CoCoA-MT)\n\n\n\n\nNeutral Src (EN)\nOK, then please follow me to your table.\n\n\nFormal Ref (JA)\nではテーブルまで私について来てください。\n\n\nInformal Ref (JA)\nではテーブルまで私について来て。\n\n\n\n\n\n\n\n\n\nGender-Controlled Translation (MT-GenEval)\n\n\n\n\nNeutral Src (EN)\nAfter retiring from teaching, Cook became a novelist.\n\n\nFeminine Ref (NL)\nNadat ze stopte met lesgeven, werd Cook schrijfster.\n\n\nMasculine Ref (NL)\nNadat hij stopte met lesgeven, werd Cook schrijver.\n\n\n\n\n\nTable 6.1: Examples of attribute triplets from CoCoA-MT and MT-GenEval. Attribute markers in the attribute-controlled translations are underlined.\n\n\n\n\n\n\n\n\n\n\n\nFigure 6.1: An example of Ramp using 2 in-context examples. Top: A sentence similarity model embeds the input sentence, and the top-\\(k\\) most similar labeled examples are retrieved from a pool of training data to build the prompt context. Bottom: Labeled cross-lingual examples are used to fill in the English prompt template, which is then provided to the LLM to generate the output.",
    "crumbs": [
      "Conditioning Generation for Personalized Machine Translation",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Retrieval and Marking for Attribute-Controlled Translation</span>"
    ]
  },
  {
    "objectID": "chapters/chap-6-ramp.html#method",
    "href": "chapters/chap-6-ramp.html#method",
    "title": "6  Retrieval and Marking for Attribute-Controlled Translation",
    "section": "6.2 Method",
    "text": "6.2 Method\nAttribute-Controlled Translation ACT takes two inputs, a sentence \\(\\mathbf{x}\\) and a desired target attribute \\(a \\in A\\) (with \\(A\\) being the space of attributes), and outputs a translation \\(\\mathbf{y}\\) that complies with the specified attribute. It can be formulated as a function \\(f: (\\mathbf{x},a)\\rightarrow\\mathbf{y}\\). In our experiments, we use attribute values provided by the CoCoA-MT formality translation dataset and the MT-GenEval gender translation dataset, i.e., \\(A=\\) {formal, informal} or {female, male}.2\nPrompting In the prompting paradigm for decoder-only LLMs, inputs are given as decoding prefixes to the model, usually combined with natural language instructions for output generation. In style-controlled translation, we formulate the prompt for target language \\(l\\) and attribute \\(a\\) using the text “Here is a sentence: {\\(\\underline{\\mathbf{x}}\\)} Here is its \\(\\underline{l}\\) translation written in a \\(\\underline{a}\\) style:” to produce the output \\(\\mathbf{y}\\).3 In the few-shot setting, we provide a sequence of \\(k\\) labeled in-context examples before the unlabeled input, which can be formulated as a function \\(f: \\{(\\mathbf{x}_1, l_1, a, \\mathbf{y}_1),\\dots, (\\mathbf{x}_{k+1}, l_{k+1}, a)\\}\\rightarrow\\mathbf{y}_{k+1}\\).\n\n6.2.1 Our Approach: Ramp\nRamp builds on the success of the prompting paradigm on few-shot generation tasks such as monolingual text style transfer (Reif et al., 2022) and MT (Garcia and Firat, 2022; Agrawal et al., 2023) by creating more informative prompts through similarity retrieval and attribute marking. See Figure 6.1 for an illustration of Ramp.\nSimilarity Retrieval In standard prompting, in-context examples are sampled randomly from the pool of labeled examples \\(\\mathcal{D}_A\\). In Ramp, we select examples based on their similarity with the input text. We first embed both the input text and the source texts of \\(\\mathcal{D}_A\\) using all-MiniLM-L6-v2 (Wang et al., 2020). Then, the top-\\(k\\) most similar examples are retrieved for the input text based on cosine similarity. These are then used in a descending order based on their cosine similarity as in-context examples in the inference prompt. As demonstrated in Figure 6.1, the in-context example “You will always be welcome here.” has the highest similarity to the test example “You’re welcome.”, so it is prompted first.\nAttribute Marking In standard prompting, in-context examples are provided without explicit information on why they satisfy the prompting objective. Inspired by recent studies that have shown that decomposition of complex tasks can improve prompting quality (Nye et al., 2022; Wei et al., 2022), we include for every in-context example an additional sentence directly after the target sentence that specifies which text spans convey the desired attribute (e.g., “The translated sentence conveys a formal style by using words such as ‘Vous’.”). In our experiments, we use the gold attribute spans included in the CoCoA-MT and MT-GenEval datasets. In Section 6.5 we suggest possibilities for automatically deriving attribute spans when gold training labels are not available.\n\n\n6.2.2 Cross-Lingual Prompting\nThe similarity retrieval component of Ramp requires a large pool \\(\\mathcal{D}_A\\) from which to find appropriate in-context examples for prompting. Low-resource attributes or language pairs may have insufficient or no annotated data from which to retrieve such examples. To mitigate this issue, we introduce cross-lingual prompting, in which the target side of the in-context examples differs from the desired target language of the translation task. As demonstrated in Figure 6.1, we investigate whether the system can leverage examples in one language (e.g., attribute indicators in Spanish) to produce the same attribute in another (e.g., French). Two main features of our Ramp model allow us to perform cross-lingual prompting: (1) the use of multilingual LLMs, and (2) the example retrieval step, which is done on the source language only.",
    "crumbs": [
      "Conditioning Generation for Personalized Machine Translation",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Retrieval and Marking for Attribute-Controlled Translation</span>"
    ]
  },
  {
    "objectID": "chapters/chap-6-ramp.html#experiments",
    "href": "chapters/chap-6-ramp.html#experiments",
    "title": "6  Retrieval and Marking for Attribute-Controlled Translation",
    "section": "6.3 Experiments",
    "text": "6.3 Experiments\nIn this section, we describe the datasets, LLMs, and baselines used in our experiments, as well as the evaluation metrics. We then present the results of Ramp in both same-language and cross-lingual prompting settings.\n\n6.3.1 Datasets\nWe experiment on two multilingual ACT datasets:\n\nCoCoA-MT (Nadejde et al., 2022) covers formality-controlled translation in the conversation domain. Source sentences are underspecified for formality, and references require formality markings (formal or informal).\nMT-GenEval (Currey et al., 2022) covers gendered translation in the Wikipedia domain. We use the contextual subset, in which sentences are gender ambiguous in the source while the reference requires gender marking. We do not use the disambiguating sentences; instead, we explicitly control the target gender.\n\nBoth datasets have gold annotations for attribute-marked target spans, and both cover translation from English into multiple diverse target languages. We list their target languages in Table 6.2.\n\n\n\n\n\n\nAR\nArabic\nDE\nGerman\nEN\nEnglish\n\n\nES\nSpanish\nFR\nFrench\nHI\nHindi\n\n\nIT\nItalian\nJA\nJapanese\nNL\nDutch\n\n\nRU\nRussian\n\n\n\n\n\n\n\n\n\n\n\n\n\nAR\nES\nFR\nHI\nPT\nDE\nIT\nJA\nRU\nNL\n\n\n\n\nCoCoA-MT\n\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n\n✓\n\n\nMT-GenEval\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n\n✓\n✓\n\n\nXGLM\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n\n\n\nBLOOM\n✓\n✓\n✓\n✓\n✓\n\n\n\n\n\n\n\n\n\n\nTable 6.2: Top: Language codes used in the Ramp experiments. Bottom: Target languages in the test sets and languages seen by LLMs in pre-training. We report results on languages seen by both LLMs.\n\n\n\n\n\n6.3.2 Large Language Models\nWe select three massively multilingual decoder-only LLMs for the prompting experiments:\n\nXGLM (Lin et al., 2022) is a 7.5B-parameter model trained on a balanced corpus containing 30 languages. It was shown to outperform much larger models such as GPT-3 on tasks related to machine translation and cross-lingual language understanding. We select it due to its broad linguistic coverage and its manageable size.\nBloom (BigScience Workshop et al., 2022) is a model available in multiple sizes, trained on a curated corpus spanning 46 natural languages (and 13 programming languages). However, many of the test languages are not part of its pre-training corpus (see Table 6.2). We evaluate two variants of the model (7.1B and 175B parameters) to assess how it is affected by a massive scaling in model parameters. The larger variant has a parameter count comparable to that of GPT-3, making it the largest publicly available multilingual LLM at present.\nGPT-NeoX (Black et al., 2022) is a 20B-parameter model trained on The Pile (Gao et al., 2021), a large English-centric corpus covering a broad range of domains. While the model was primarily trained on English data and is therefore not intended for multilingual usage, it exhibits interesting generalization performance for many of our target languages.\n\nThe selected models span three orders of magnitude in terms of number of parameters and differ in the languages that they cover (see Table 6.2).\n\n\n6.3.3 Baseline\nAttribute tagging is a standard method for ACT, so we include a baseline following the approach and configuration used by Nadejde et al. (2022), i.e. an encoder-decoder transformer MT model (Vaswani et al., 2017) pre-trained on public parallel data and further finetuned on contrastive training pairs with attribute tags (from either CoCoA-MT or MT-GenEval) such as &lt;formal&gt;, &lt;informal&gt;, &lt;masculine&gt; and &lt;feminine&gt;. We refer to these models as adapted MT in our evaluation.\n\n\n\n\n\n\nDataset\nAttribute\n# Train\n# Test\nAcc.\n\n\n\n\nCoCoA-MT\nFormality\n7,600\n1,596\n0.990\n\n\nMT-GenEval\nGender\n4,900\n9,854\n0.970\n\n\n\n\n\nTable 6.3: Dataset statistics. We report # of triplets in the train/test split aggregated across all languages and the classification accuracy on the test split of the classifiers.\n\n\n\n\n\n6.3.4 Evaluation Metrics\nWe measure translation quality with BLEU (Papineni et al., 2002) and comet (Rei et al., 2020). For attribute accuracy, we use the lexical matching metrics provided with CoCoA-MT and MT-GenEval (Lexical-Accuracy) and sentence encoders trained on contrastive examples (Sentential-Accuracy). For the latter, we train multilingual classifiers on top of the mDeBERTa-v3 encoder (He et al., 2023). High-performance pre-trained classifiers have been shown to produce attribute accuracy estimates closer to human judgments for style transfer (Lai et al., 2022). Table 6.3 presents the accuracy of the classification models on the test sets of their respective datasets, averaged across all languages.\nWe use the original train/test split provided by the CoCoA-MT dataset. Each split contains telephony and topical_chat domains. We use the topical_chat domain in our experiments. MT-GenEval contains a dev and test split, and we use the dev split as training data for the classification model and prompting experiments.\nWe finetune mDeBERTa-v3-base model4 on the contrastive examples in the respective training sets to obtain the attribute classifiers. We fine-tune the classifier for two epochs with a batch size of 8, a learning rate of 2e-5, 500 warm-up steps, a max sequence length of 256, and save checkpoints every 500 steps. We do not do hyperparameter tuning, and thus, a validation set is not used.\nUnlike lexical accuracy, the multilingual attribute classifier does not penalize text generated in incorrect languages. Thus, in cross-lingual prompting experiments, we include a step of language detection5 so that generated sentences not in the requested target language are considered incorrect.\n\n\n6.3.5 Same-Language Prompting\nWe first evaluate the effectiveness of Ramp for formality- and gender-controlled translation where the language pair used for in-context examples is the same as the one used in the prompt candidate (e.g., English\\(\\to\\)Spanish formality-controlled translation using English\\(\\to\\)Spanish in-context examples).\nWe begin by conducting a preliminary evaluation of 3 LLMs across different ranges of in-context examples to reduce the number of experimental settings for our main assessment. We perform formality-controlled translation using CoCoA-MT, and evaluate LLMs by varying the number of in-context examples (i.e., 4-8-16-32, selected based on the feasible context length6). Figure 6.2 presents results averaged across all four languages seen by Bloom during its pre-training.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6.2: BLEU and sentential formality accuracy of prompt outputs on CoCoA-MT test set for different amounts of in-context examples in the base and Ramp settings. Confidence intervals are obtained for the base setting by sampling in-context examples using 3 seeds. Detailed scores are included in Table B.1.\n\n\n\nWe observe that Ramp generally outperforms base prompting (i.e., random in-context examples and no attribute marking) across most LLMs and example settings for both BLEU and formality accuracy. Moreover, BLEU and formality accuracy improve with increased model size and with the number of examples, until this number reaches 16. Based on these results, we move forward with the main evaluation using XGLM 7.5B and Bloom 175B models and 16 in-context examples for both datasets.\nTable 6.4 presents our main results alongside the adapted MT baseline. The base model uses in-context examples that are randomly sampled from the pool of labeled examples. We also include an ablation that adds only attribute marking on top of base prompting, without similarity retrieval (+mark).\n\n\n\n\n\n\n\n\n\nCoCoA-MT\nMT-GenEval\n\n\nBLEU\nCOMET\nL-Acc\nS-Acc\nBLEU\nCOMET\nL-Acc\nS-Acc\n\n\n\n\nSame-Language\nXGLM 7.5B\nbase\n28.6\n0.463\n0.835\n0.846\n23.7\n0.445\n0.790\n0.727\n\n\n+mark\n28.7\n0.423\n0.920\n0.902\n23.7\n0.444\n0.789\n0.732\n\n\nRamp\n30.0\n0.451\n0.938\n0.923\n24.8\n0.473\n0.836\n0.820\n\n\nBLOOM 175B\nbase\n39.9\n0.691\n0.930\n0.940\n33.3\n0.679\n0.748\n0.704\n\n\n+mark\n40.3\n0.688\n0.970\n0.970\n33.1\n0.674\n0.759\n0.725\n\n\nRamp\n41.9\n0.711\n0.973\n0.970\n34.3\n0.699\n0.817\n0.818\n\n\nAdapted MT\n38.5\n0.454\n0.691\n0.693\n39.6\n0.750\n0.842\n0.864\n\n\nCross-Lingual\nBLOOM 175B\nbase\n32.1\n0.644\n0.567\n0.596\n28.5\n0.469\n0.777\n0.633\n\n\nRamp\n31.8\n0.646\n0.625\n0.622\n29.4\n0.502\n0.788\n0.673\n\n\n\n\n\nTable 6.4: BLEU, COMET, Lexical- and Sentential-Accuracy of selected LLMs using 16 same-language in-context examples on two tasks, alongside adapted MT models. Scores are aggregated across seen languages (w.r.t. BLOOM pre-training) and both attributes for each task. (Full results in Table B.2–Table B.5.)\n\n\n\nWe observe that in the +mark setting, simple attribute marking consistently improves attribute accuracy of the generated text, but leads to degradation of comet on CoCoA-MT. The complete Ramp with similarity retrieval not only compensates for the comet degradation but also improves quality and attribute metrics across the board, especially for the high-capacity Bloom 175B model.\nAdapted MT outperforms Bloom 175B on MT-GenEval in all metrics, but underperforms it on CoCoA-MT. This suggests that it is challenging to conduct a fine-grained comparison between LLMs and standard MT systems, as they may have different domain coverage. Bloom 175B consistently outperforms XGLM 7.5B in both generic translation quality and attribute control accuracy, so we focus on Bloom 175B for our cross-lingual prompting analysis.\n\n\n6.3.6 Cross-Lingual Prompting\nWe have demonstrated the effectiveness of selecting similar same-language examples to build the prompt, echoing related work (Liu et al., 2022; Agrawal et al., 2023). In this section, we evaluate the cross-lingual prompting option, i.e., retrieving in-context examples from other target languages besides the desired language of translation. We test this zero-shot setting using the leave-one-out strategy, i.e. we retrieve in-context examples from every language except the desired language of translation. We ensure that we retrieve an equal number of examples from all languages: the number of examples retrieved from each language is the total desired number of in-context examples divided by the number of training languages. In CoCoA-MT, we retrieve 14 in-context examples from 7 languages. In MT-GenEval, we retrieve 8 in-context examples from 8 languages.7 Finally, results are averaged across tested language pairs. Languages that are not seen during the LLM pre-training are included among in-context examples, but not as the target language of the translation task.\nTable 6.4 (bottom) presents our results using Bloom 175B. On both test sets, compared to the baseline, we observe improved attribute accuracy and comparable or better generic translation quality when using Ramp with cross-lingual prompting.\nWe observe translation quality degradation with Ramp on some target languages of CoCoA-MT, such as Spanish. Manual analysis shows that repeatedly inaccurate retrieval results could lead to hallucinations.8 For example, Ramp retrieves multiple sentences containing “million” for the input If you got it why not? He is worth over 20 billion dollars after all. This results in mistranslation of billion to million (millionario): Si lo tienes, ¿por qué no? Es millonario después de todo. We give detailed examples in Section B.1.3. This is a known issue with retrieval-based prompting (Liu et al., 2022; Agrawal et al., 2023), which can be mitigated by using more diverse in-context examples or a larger pool of training data for retrieval.",
    "crumbs": [
      "Conditioning Generation for Personalized Machine Translation",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Retrieval and Marking for Attribute-Controlled Translation</span>"
    ]
  },
  {
    "objectID": "chapters/chap-6-ramp.html#sec-chap6-limitations",
    "href": "chapters/chap-6-ramp.html#sec-chap6-limitations",
    "title": "6  Retrieval and Marking for Attribute-Controlled Translation",
    "section": "6.4 Limitations",
    "text": "6.4 Limitations\nWe delineate some limitations of our approach and discuss future work directions.\nExample Availability and Prompt Sensitivity The proposed formulation of the Ramp method relies on gold annotations for attribute marking, which are not always available depending on the dataset. However, Ramp could be easily extended to unsupervised settings through LLM input attribution methods, such as those we presented in the previous part of this thesis. This approach builds upon recent techniques in unsupervised language generation metrics (Fomicheva et al., 2021; Fomicheva et al., 2022; Leiter et al., 2024). Apart from the choice of in-context examples, prompting is also sensitive to their ordering (Lu et al., 2022) and the design of the template (Jiang et al., 2020). We refrain from tuning example orders and templates to avoid introducing too many variables, but we acknowledge that this could lead to suboptimal results.\nUnseen Languages, Computational Resources and Diversity Multilingual LLMs perform competitively on machine translation for languages seen during their pre-training. However, we noticed that Bloom 175B produces better English\\(\\to\\)Italian translations than XGLM 7.5B even though Italian is not listed among Bloom’s training languages. This could be due to typological similarity between Italian and the Romance languages included in Bloom training. Multilingual LLMs such as Bloom also require significantly more GPU resources for inference than standard bilingual MT systems do, making them less practical for production deployment. Finally, the MT-GenEval test set is limited in providing only two gender labels (female and male) as minimal pairs, while neutral rewriting is not represented.",
    "crumbs": [
      "Conditioning Generation for Personalized Machine Translation",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Retrieval and Marking for Attribute-Controlled Translation</span>"
    ]
  },
  {
    "objectID": "chapters/chap-6-ramp.html#sec-chap6-conclusion",
    "href": "chapters/chap-6-ramp.html#sec-chap6-conclusion",
    "title": "6  Retrieval and Marking for Attribute-Controlled Translation",
    "section": "6.5 Conclusion",
    "text": "6.5 Conclusion\nAs a first step in our exploration of conditioning machine translation generation, we introduced the Ramp in-context learning for better conditioning performance through the use of attribute annotations and similar retrieved examples. We demonstrated its effectiveness with multilingual LLMs for both formality-controlled and gender-controlled translation, showing that it improves attribute accuracy and translation quality over standard prompting and adapted MT baselines, including in cross-lingual settings using relevant in-context examples from other languages. In the next chapter, we expand our investigation to steering methods that intervene directly in the inner processing of LLMs and study their effectiveness for personalization in the challenging domain of literary machine translation.\n\n\n\n\nSweta Agrawal, Chunting Zhou, Mike Lewis, Luke Zettlemoyer, and Marjan Ghazvininejad. 2023. In-context examples selection for machine translation. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Findings of the association for computational linguistics: ACL 2023, pages 8857–8873, Toronto, Canada. Association for Computational Linguistics.\n\n\nBigScience Workshop, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Luccioni, François Yvon, et al. 2022. BLOOM: A 176B-parameter open-access multilingual language model. Arxiv.\n\n\nSidney Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, Usvsn Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach. 2022. GPT-NeoX-20B: An open-source autoregressive language model. In Angela Fan, Suzana Ilic, Thomas Wolf, and Matthias Gallé, editors, Proceedings of BigScience episode #5 – workshop on challenges & perspectives in creating large language models, pages 95–136, virtual+Dublin. Association for Computational Linguistics.\n\n\nEleftheria Briakou, Di Lu, Ke Zhang, and Joel Tetreault. 2021. Olá, bonjour, salve! XFORMAL: A benchmark for multilingual formality style transfer. In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou, editors, Proceedings of the 2021 conference of the north american chapter of the association for computational linguistics: Human language technologies, pages 3199–3216, Online. Association for Computational Linguistics.\n\n\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, et al. 2020. Language models are few-shot learners. In Proceedings of the 34th international conference on neural information processing systems, Red Hook, NY, USA. Curran Associates Inc.\n\n\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, et al. 2023. PaLM: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1–113.\n\n\nAnna Currey, Maria Nadejde, Raghavendra Reddy Pappagari, Mia Mayer, Stanislas Lauly, Xing Niu, Benjamin Hsu, and Georgiana Dinu. 2022. MT-GenEval: A counterfactual and contextual dataset for evaluating gender accuracy in machine translation. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Proceedings of the 2022 conference on empirical methods in natural language processing, pages 4287–4299, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\n\n\nMarina Fomicheva, Piyawat Lertvittayakumjorn, Wei Zhao, Steffen Eger, and Yang Gao. 2021. The Eval4NLP shared task on explainable quality estimation: Overview and results. In Yang Gao, Steffen Eger, Wei Zhao, Piyawat Lertvittayakumjorn, and Marina Fomicheva, editors, Proceedings of the 2nd workshop on evaluation and comparison of NLP systems, pages 165–178, Punta Cana, Dominican Republic. Association for Computational Linguistics.\n\n\nMarina Fomicheva, Lucia Specia, and Nikolaos Aletras. 2022. Translation error detection as rationale extraction. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Findings of the association for computational linguistics: ACL 2022, pages 4148–4159, Dublin, Ireland. Association for Computational Linguistics.\n\n\nLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. 2021. The pile: An 800GB dataset of diverse text for language modeling. Arxiv.\n\n\nXavier Garcia, Noah Constant, Mandy Guo, and Orhan Firat. 2021. Towards universality in multilingual text rewriting. Arxiv.\n\n\nXavier Garcia and Orhan Firat. 2022. Using natural language prompts for machine translation. Arxiv.\n\n\nPengcheng He, Jianfeng Gao, and Weizhu Chen. 2023. DeBERTaV3: Improving DeBERTa using ELECTRA-style pre-training with gradient-disentangled embedding sharing. In Proceedings of the 11th international conference on learning representations.\n\n\nLianzhe Huang, Shuming Ma, Dongdong Zhang, Furu Wei, and Houfeng Wang. 2022. Zero-shot cross-lingual transfer of prompt-based tuning with a unified multilingual prompt. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Proceedings of the 2022 conference on empirical methods in natural language processing, pages 11488–11497, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\n\n\nZhengbao Jiang, Frank F. Xu, Jun Araki, and Graham Neubig. 2020. How can we know what language models know? Transactions of the Association for Computational Linguistics, 8:423–438.\n\n\nDi Jin, Zhijing Jin, Zhiting Hu, Olga Vechtomova, and Rada Mihalcea. 2022. Deep learning for text style transfer: A survey. Computational Linguistics, 48(1):155–205.\n\n\nKalpesh Krishna, Deepak Nathani, Xavier Garcia, Bidisha Samanta, and Partha Talukdar. 2022. Few-shot controllable style transfer for low-resource multilingual settings. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Proceedings of the 60th annual meeting of the association for computational linguistics (volume 1: Long papers), pages 7439–7468, Dublin, Ireland. Association for Computational Linguistics.\n\n\nHuiyuan Lai, Jiali Mao, Antonio Toral, and Malvina Nissim. 2022. Human judgement as a compass to navigate automatic metrics for formality transfer. In Anya Belz, Maja Popović, Ehud Reiter, and Anastasia Shimorina, editors, Proceedings of the 2nd workshop on human evaluation of NLP systems (HumEval), pages 102–115, Dublin, Ireland. Association for Computational Linguistics.\n\n\nSurafel Melaku Lakew, Mattia Di Gangi, and Marcello Federico. 2019. Controlling the output length of neural machine translation. In Jan Niehues, Rolando Cattoni, Sebastian Stüker, Matteo Negri, Marco Turchi, Thanh-Le Ha, Elizabeth Salesky, Ramon Sanabria, Loic Barrault, Lucia Specia, and Marcello Federico, editors, Proceedings of the 16th international conference on spoken language translation, Hong Kong. Association for Computational Linguistics.\n\n\nChristoph Leiter, Piyawat Lertvittayakumjorn, Marina Fomicheva, Wei Zhao, Yang Gao, and Steffen Eger. 2024. Towards explainable evaluation metrics for machine translation. Journal of Machine Learning Research, 25(75):1–49.\n\n\nXi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian O’Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona Diab, et al. 2022. Few-shot learning with multilingual generative language models. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Proceedings of the 2022 conference on empirical methods in natural language processing, pages 9019–9052, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\n\n\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2022. What makes good in-context examples for GPT-3? In Eneko Agirre, Marianna Apidianaki, and Ivan Vulić, editors, Proceedings of deep learning inside out (DeeLIO 2022): The 3rd workshop on knowledge extraction and integration for deep learning architectures, pages 100–114, Dublin, Ireland; Online. Association for Computational Linguistics.\n\n\nYao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2022. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Proceedings of the 60th annual meeting of the association for computational linguistics (volume 1: Long papers), pages 8086–8098, Dublin, Ireland. Association for Computational Linguistics.\n\n\nPaul Michel and Graham Neubig. 2018. Extreme adaptation for personalized neural machine translation. In Iryna Gurevych and Yusuke Miyao, editors, Proceedings of the 56th annual meeting of the association for computational linguistics (volume 2: Short papers), pages 312–318, Melbourne, Australia. Association for Computational Linguistics.\n\n\nMaria Nadejde, Anna Currey, Benjamin Hsu, Xing Niu, Marcello Federico, and Georgiana Dinu. 2022. CoCoA-MT: A dataset and benchmark for contrastive controlled MT with application to formality. In Marine Carpuat, Marie-Catherine de Marneffe, and Ivan Vladimir Meza Ruiz, editors, Findings of the association for computational linguistics: NAACL 2022, pages 616–632, Seattle, United States. Association for Computational Linguistics.\n\n\nXing Niu and Marine Carpuat. 2020. Controlling neural machine translation formality with synthetic supervision. Proceedings of the AAAI Conference on Artificial Intelligence, 34(05):8568–8575.\n\n\nXing Niu, Marianna Martindale, and Marine Carpuat. 2017. A study of style in machine translation: Controlling the formality of machine translation output. In Martha Palmer, Rebecca Hwa, and Sebastian Riedel, editors, Proceedings of the 2017 conference on empirical methods in natural language processing, pages 2814–2819, Copenhagen, Denmark. Association for Computational Linguistics.\n\n\nMaxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus Odena. 2022. Show your work: Scratchpads for intermediate computation with language models. In Deep learning for code workshop.\n\n\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: A method for automatic evaluation of machine translation. In Pierre Isabelle, Eugene Charniak, and Dekang Lin, editors, Proceedings of the 40th annual meeting of the association for computational linguistics, pages 311–318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.\n\n\nElla Rabinovich, Raj Nath Patel, Shachar Mirkin, Lucia Specia, and Shuly Wintner. 2017. Personalized machine translation: Preserving original author traits. In Mirella Lapata, Phil Blunsom, and Alexander Koller, editors, Proceedings of the 15th conference of the European chapter of the association for computational linguistics: Volume 1, long papers, pages 1074–1084, Valencia, Spain. Association for Computational Linguistics.\n\n\nSudha Rao and Joel Tetreault. 2018. Dear sir or madam, may I introduce the GYAFC dataset: Corpus, benchmarks and metrics for formality style transfer. In Marilyn Walker, Heng Ji, and Amanda Stent, editors, Proceedings of the 2018 conference of the north American chapter of the association for computational linguistics: Human language technologies, volume 1 (long papers), pages 129–140, New Orleans, Louisiana. Association for Computational Linguistics.\n\n\nRicardo Rei, Craig Stewart, Ana C Farinha, and Alon Lavie. 2020. COMET: A neural framework for MT evaluation. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, Proceedings of the 2020 conference on empirical methods in natural language processing (EMNLP), pages 2685–2702, Online. Association for Computational Linguistics.\n\n\nEmily Reif, Daphne Ippolito, Ann Yuan, Andy Coenen, Chris Callison-Burch, and Jason Wei. 2022. A recipe for arbitrary text style transfer with large language models. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Proceedings of the 60th annual meeting of the association for computational linguistics (volume 2: Short papers), pages 837–848, Dublin, Ireland. Association for Computational Linguistics.\n\n\nGabriele Sarti, Phu Mon Htut, Xing Niu, Benjamin Hsu, Anna Currey, Georgiana Dinu, and Maria Nadejde. 2023. RAMP: Retrieval and attribute-marking enhanced prompting for attribute-controlled translation. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st annual meeting of the association for computational linguistics (volume 2: Short papers), pages 1476–1490, Toronto, Canada. Association for Computational Linguistics.\n\n\nDanielle Saunders and Bill Byrne. 2020. Reducing gender bias in neural machine translation as a domain adaptation problem. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault, editors, Proceedings of the 58th annual meeting of the association for computational linguistics, pages 7724–7736, Online. Association for Computational Linguistics.\n\n\nAndrea Schioppa, David Vilar, Artem Sokolov, and Katja Filippova. 2021. Controlling machine translation for multiple attributes with additive interventions. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Proceedings of the 2021 conference on empirical methods in natural language processing, pages 6676–6696, Online; Punta Cana, Dominican Republic. Association for Computational Linguistics.\n\n\nRico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Controlling politeness in neural machine translation via side constraints. In Kevin Knight, Ani Nenkova, and Owen Rambow, editors, Proceedings of the 2016 conference of the north American chapter of the association for computational linguistics: Human language technologies, pages 35–40, San Diego, California. Association for Computational Linguistics.\n\n\nMirac Suzgun, Luke Melas-Kyriazi, and Dan Jurafsky. 2022. Prompt-and-rerank: A method for zero-shot and few-shot arbitrary textual style transfer with small language models. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Proceedings of the 2022 conference on empirical methods in natural language processing, pages 2195–2222, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\n\n\nEva Vanmassenhove, Christian Hardmeier, and Andy Way. 2018. Getting gender right in neural machine translation. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun’ichi Tsujii, editors, Proceedings of the 2018 conference on empirical methods in natural language processing, pages 3003–3008, Brussels, Belgium. Association for Computational Linguistics.\n\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in neural information processing systems, volume 30. Curran Associates, Inc.\n\n\nDavid Vilar, Markus Freitag, Colin Cherry, Jiaming Luo, Viresh Ratnakar, and George Foster. 2023. Prompting PaLM for translation: Assessing strategies and performance. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st annual meeting of the association for computational linguistics (volume 1: Long papers), pages 15406–15427, Toronto, Canada. Association for Computational Linguistics.\n\n\nWenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. 2020. MINILM: Deep self-attention distillation for task-agnostic compression of pre-trained transformers. In Proceedings of the 34th international conference on neural information processing systems, Red Hook, NY, USA. Curran Associates Inc.\n\n\nYifan Wang, Zewei Sun, Shanbo Cheng, Weiguo Zheng, and Mingxuan Wang. 2023. Controlling styles in neural machine translation with activation prompt. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Findings of the association for computational linguistics: ACL 2023, pages 2606–2620, Toronto, Canada. Association for Computational Linguistics.\n\n\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc V. Le, and Denny Zhou. 2022. Chain-of-thought prompting elicits reasoning in large language models. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in neural information processing systems, volume 35, pages 24824–24837. Curran Associates, Inc.\n\n\nMengjie Zhao and Hinrich Schütze. 2021. Discrete and soft prompting for multilingual models. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Proceedings of the 2021 conference on empirical methods in natural language processing, pages 8547–8555, Online; Punta Cana, Dominican Republic. Association for Computational Linguistics.\n\n\nMeng Zhou, Xin Li, Yue Jiang, and Lidong Bing. 2023. Enhancing cross-lingual prompting with dual prompt augmentation. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Findings of the association for computational linguistics: ACL 2023, pages 11008–11020, Toronto, Canada. Association for Computational Linguistics.\n\n\nZhemin Zhu, Delphine Bernhard, and Iryna Gurevych. 2010. A monolingual tree-based translation model for sentence simplification. In Chu-Ren Huang and Dan Jurafsky, editors, Proceedings of the 23rd international conference on computational linguistics (coling 2010), pages 1353–1361, Beijing, China. Coling 2010 Organizing Committee.",
    "crumbs": [
      "Conditioning Generation for Personalized Machine Translation",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Retrieval and Marking for Attribute-Controlled Translation</span>"
    ]
  },
  {
    "objectID": "chapters/chap-6-ramp.html#footnotes",
    "href": "chapters/chap-6-ramp.html#footnotes",
    "title": "6  Retrieval and Marking for Attribute-Controlled Translation",
    "section": "",
    "text": "We employ the term attribute rather than style, since not all the attributes addressed here (e.g., gender) can be considered styles.↩︎\nSee Section 6.4 for ethical considerations.↩︎\nWe adopt prompt templates similar to the one used by Reif et al. (2022), and we write the prompt template in English. Complete templates are provided in Section B.1.1.↩︎\nmicrosoft/mdeberta-v3-base↩︎\nhttps://pypi.org/project/langdetect/↩︎\nBloom 175B encountered out-of-memory errors with 32 in-context examples on 8 A100 40GB GPUs.↩︎\nWe reduced the number of in-context examples in this setting to avoid out-of-memory errors with Bloom 175B.↩︎\nVilar et al. (2023) also observe hallucinations when the retrieved examples have bad translations (i.e., non-parallel sentences).↩︎",
    "crumbs": [
      "Conditioning Generation for Personalized Machine Translation",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Retrieval and Marking for Attribute-Controlled Translation</span>"
    ]
  },
  {
    "objectID": "chapters/chap-7-sae-litmt.html",
    "href": "chapters/chap-7-sae-litmt.html",
    "title": "7  Steering Language Models for Personalized Machine Translation",
    "section": "",
    "text": "7.1 Introduction\nWhen we read a translated book, we do not simply read the story in a new language; we also experience the translator’s personal voice through their stylistic choices. Past efforts in the automatic translation of literary works have historically been constrained by the limited capabilities and flexibility of machine translation systems. The recent popularization of MT systems based on large language models has significantly improved their capacity to handle the long contexts typical of literary translations, but mimicking the creative and rich language that characterizes the translators’ own style remains an open issue. In this context, several works have explored the use of prompting and tuning-based strategies to ensure that translations are stylistically appropriate (Michel and Neubig, 2018; Wang et al., 2021). However, their influence on model internal representations is rarely explored, making their impact less controllable and often unpredictable. Building upon the prompting techniques demonstrated in Chapter 6, this chapter tackles the more complex challenge of personalizing machine translation to match individual translator styles. While Ramp focused on explicit attributes like formality and gender, literary translation requires capturing the subtle, implicit stylistic preferences that characterize individual translators’ voices. For this purpose, we compare prompting approaches with steering methods proposed in interpretability literature. These techniques can be used to surgically intervene on LLMs’ intermediate representation to generate personalized translations when few examples are available, using the Par3 dataset (Thai et al., 2022) with multiple human translations for novels translated into English from 7 typologically diverse languages.\nWe begin with preliminary assessments by verifying whether translators’ styles are discernible by automatic systems, finding that trained classifiers can distinguish writing styles with high accuracy, while the task is notoriously challenging for human annotators (Youyou et al., 2015; Flekova et al., 2016). We also find a simple prompting setting with in-context personalization examples to improve the style accuracy of LLM translation, suggesting personalized translation styles are reproducible. We connect the conditioning induced by prompting to the inner workings of the model, identifying activations with high discriminative capacity for style differences in intermediate model layers. We then propose a contrastive steering approach based on sparse autoencoders (SAEs, Huben et al. (2024)) to condition model generations by upweighting sparse, interpretable latents at inference time. We validate the effectiveness of our method across three LLMs of various sizes on Par3 novels, comparing our results with established prompting and steering methods.\nOur results show that contrastive SAE steering is a promising approach for MT personalization, resulting in translations that not only align more closely with general human translation features but also with the desired personalized style compared to other methods. Importantly, these results are achieved with no degradation in translation quality, according to established MT quality metrics. We conclude by comparing the impact of our method on model representations with the outcome of multi-shot prompting, finding that probes trained on prompt-conditioned activations can predict the effectiveness of SAE steering with high precision. These results confirm that tested prompting and steering techniques converge to similar solutions for conditioning model behavior, enabling future investigations into the mechanistic impact of prompting through the study of learned SAE latents and other interpretable components.",
    "crumbs": [
      "Conditioning Generation for Personalized Machine Translation",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Steering Language Models for Personalized Machine Translation</span>"
    ]
  },
  {
    "objectID": "chapters/chap-7-sae-litmt.html#introduction",
    "href": "chapters/chap-7-sae-litmt.html#introduction",
    "title": "7  Steering Language Models for Personalized Machine Translation",
    "section": "",
    "text": "Figure 7.1: We compare prompt-based approaches with steering techniques intervening on model internals for personalizing MT outputs in literary machine translation, employing MT quality metrics and style classifiers to disentangle the effect of steering on outputs fluency and personalization adequacy.",
    "crumbs": [
      "Conditioning Generation for Personalized Machine Translation",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Steering Language Models for Personalized Machine Translation</span>"
    ]
  },
  {
    "objectID": "chapters/chap-7-sae-litmt.html#sec-chap7-sota",
    "href": "chapters/chap-7-sae-litmt.html#sec-chap7-sota",
    "title": "7  Steering Language Models for Personalized Machine Translation",
    "section": "7.2 Related Work",
    "text": "7.2 Related Work\nMachine Translation of Literary Texts The literary domain has historically been challenging for automatic MT systems due to their limited ability in handling rich linguistic and cultural contexts (Matusov, 2019) and their propensity to produce overly literal outputs (Guerberof-Arenas and Toral, 2022). Automatic literary translation has a long history dating back to pre-neural MT approaches (Voigt and Jurafsky, 2012; Toral and Way, 2015; Toral and Way, 2018; Moorkens et al., 2018) with two recent dedicated evaluation campaigns (Wang et al., 2023b; Wang et al., 2024a). The advent of LLMs has brought new opportunities in processing longer contexts for document-level translation (Wang et al., 2023a; Briakou et al., 2024; Wu et al., 2025), but critical errors requiring human translators’ intervention nonetheless persist (Karpinska and Iyyer, 2023). Here, we use the Par3 dataset (Thai et al., 2022) containing multiple human translations of novels to evaluate MT personalization in the literary domain.\nPersonalization for Machine Translation Advances in MT quality recently led to a growing interest in personalization approaches to ensure a consistent format and appropriate stylistic choices in model generations (Rabinovich et al., 2017; Lin et al., 2021). Previous approaches for controlling attributes such as formality (Sennrich et al., 2016; Niu et al., 2017; Nadejde et al., 2022) or gender (Vanmassenhove et al., 2018; Saunders and Byrne, 2020) typically required tuning existing models on pre-defined properties of interest, with few works attempting a real data-driven adaptation from unlabeled demonstrations (Michel and Neubig, 2018; Wang et al., 2021; Zhang et al., 2022). More recently, several studies employed prompting (Garcia and Firat, 2022; Sarti et al., 2023) or preference optimization from post-editing behavior (Lee et al., 2023; Berger et al., 2024) to render MT personalization more effective and data-efficient. We complement established prompt methodologies with steering approaches to personalize MT outputs using a few user-provided examples.",
    "crumbs": [
      "Conditioning Generation for Personalized Machine Translation",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Steering Language Models for Personalized Machine Translation</span>"
    ]
  },
  {
    "objectID": "chapters/chap-7-sae-litmt.html#sec-chap7-preliminary",
    "href": "chapters/chap-7-sae-litmt.html#sec-chap7-preliminary",
    "title": "7  Steering Language Models for Personalized Machine Translation",
    "section": "7.3 Preliminaries",
    "text": "7.3 Preliminaries\n\n\n\n\n\n\nZH → EN || 道人道：「既如此，便隨你去來。」\n\n\n\n\nH\\(_1\\)\n\"Such being the case,\" the Taoist acquiesced, \"I am ready to follow you, whenever you please to go.\"\n\n\nH\\(_2\\)\n\"Very good, I will go with you then,\" said the Taoist.\n\n\nGemma 2 2B\n\"If that's the case, then go ahead\" said the Taoist.\n\n\nGemma 2 9B\n\"If so, then I will go with you\" the Taoist said\n\n\nLlama 3.1 8B\nThe Taoist said: \"If you insist on going, then go ahead.\"\n\n\n\n\n\n\n\n\n\nIT → EN || Sarà l’effetto dell’acqua del mare. Il mare ne fa di questi scherzi\n\n\n\n\nH\\(_1\\)\n\"The salt water must have done it. The sea plays funny tricks.\"\n\n\nH\\(_2\\)\n\"It must have been the effect of sea-water. The sea makes extraordinary changes.\"\n\n\nGemma 2 2B\n\"It will be the effect of the sea water. The sea makes of these jokes.\"\n\n\nGemma 2 9B\nIt will be the effect of the sea water. The sea plays these tricks.\n\n\nLlama 3.1 8B\nIt will be the effect of the sea water. The sea does things like this.\n\n\n\n\n\nTable 7.1: ZH\\(\\rightarrow\\)EN and IT\\(\\rightarrow\\)EN examples for Par3 segments translated by humans (H\\(_1\\),H\\(_2\\)) and LLMs with zero-shot prompting (MT). More examples in Table B.8, Table B.7.\n\n\n\nBefore testing the effectiveness of personalization strategies, we validate some key assumptions: i) Whether the personalized translation style is discernible, i.e., if it is possible to tell apart human- and machine-generated translations; ii) Whether different translation styles are automatically reproducible, i.e., if LLMs can mimic a specific translator’s style when provided with some examples; and iii) Whether style distinctions are reflected in the model’s internal representations, to motivate the interest in steering approaches for personalization.\nWe use the Par3 dataset by Thai et al. (2022), which contains multiple non-English novels, as a benchmark to evaluate personalization. Novels are segmented into paragraphs with translations into English by two professional literary translators. To ensure a diverse and representative evaluation, we select novels spanning a variety of linguistic families and cultural backgrounds. Our dataset includes Romance languages such as Italian (Pinocchio) and French (Around the World in Eighty Days), as well as Germanic languages like Dutch (The Diary of a Young Girl) and German (Beware of Pity). To evaluate our setup on non-Latin scripts and distinct linguistic structures, we also include Russian (Crime and Punishment), Japanese (No Longer Human), and Chinese (Dream of the Red Chamber). Table 7.2 summarizes the number of paragraphs employed in the evaluation of each language.\n\n\n\n\n\n\nLang\nNovel name\nTrain\nVal\nTest\nICL Ex.\n\n\n\n\nItalian (IT)\nPinocchio\n745\n82\n107\n20\n\n\nFrench (FR)\nAround the World in Eighty Days\n829\n92\n120\n20\n\n\nDutch (NL)\nThe Diary of a Young Girl\n769\n85\n110\n20\n\n\nGerman (DE)\nBeware of Pity\n606\n67\n96\n20\n\n\nRussian (RU)\nCrime and Punishment\n1517\n168\n224\n20\n\n\nJapanese (JA)\nNo Longer Human\n652\n40\n81\n20\n\n\nChinese (ZH)\nDream of the Red Chamber\n694\n76\n92\n20\n\n\n\n\n\nTable 7.2: Languages and novels used in our evaluation. Numbers corresponds to a single paragraph in the dataset. In training, each paragraph is associated with three distinct translations: H\\(_1\\),H\\(_2\\), and MT. 20 additional in-context examples are left aside for each language for prompting and steering methods.\n\n\n\nExamples for a subset of languages are shown in Table 7.1. We name the two available human translations H\\(_1\\) and H\\(_2\\), and compare them with MT outputs produced by LLMs, which we denote as \\(\\text{MT}_{\\text{model}}\\). We use three LLMs, namely Llama 3.1 8B Instruct (Team, 2024b) and Gemma 2 (Team, 2024a) in its 2B and 9B instruction-tuned variants. Our model selection is motivated by our steering requirements, discussed in Section 7.5.\n\n7.3.1 Are Personalized Translations Discernible?\nFollowing prior work on personalization (Wang et al., 2024c; Liu et al., 2023), we train a series of classifiers based on multilingual XLM transformer encoders (Conneau et al., 2020) to distinguish between H\\(_1\\), H\\(_2\\), and MT translations. If those systems can reliably separate these three classes, it suggests the presence of reasonably distinct stylistic signals differentiating them. In particular, the ability to distinguish between H\\(_1\\) and H\\(_2\\) would denote not only the possibility to discern a human-like style from human-made and automatic translations, but also a personalized style from different human translators.\nWe train a classifier for each language and each model in our evaluation suite. All classifiers are fine-tuned from the xlm-roberta-large model1, using a linear classification head. Training is conducted for 6 epochs with a learning rate of 2e-5 and a batch size of 32, selecting the best model checkpoint based on validation accuracy. Training data only includes generations from models and the translator without any source text. It is also perfectly balanced, as each paragraph provides one instance for all three labels: H\\(_1\\), H\\(_2\\), and MT. The total size of the training set varies depending on the number of paragraphs in the chosen novel. On average, we obtain approximately 830 instances, resulting in a total of around 2,490 labeled examples for training (see Table 7.2). Validation and test sets are strictly held out and never seen during training. Additionally, they do not include the small 20-example subsets used for prompting or steering. Results in Table 7.3 indicate that translation styles are discernible with high accuracy. On average, across all models and languages, the classifiers achieve an accuracy ranging from 77% (Japanese) to 99% (Chinese), with an overall average of 86%. These results suggest that personalization information is abundant in the literary setting and can plausibly be exploited for modeling. These findings corroborate previous results showing the high learnability of this task by machines while remaining intrinsically difficult for human annotators (Youyou et al., 2015; Flekova et al., 2016; Wang et al., 2024b).2\n\n\n\n\n\n\nLang\nGemma 2 2B\nGemma 2 9B\nLlama 3.1 8B\n\n\n\n\nDE\n0.89\n0.90\n0.84\n\n\nRU\n0.92\n0.90\n0.91\n\n\nZH\n0.99\n0.98\n0.98\n\n\nIT\n0.78\n0.85\n0.80\n\n\nNL\n0.79\n0.78\n0.82\n\n\nFR\n0.88\n0.87\n0.90\n\n\nJA\n0.76\n0.79\n0.76\n\n\n\n\n\nTable 7.3: Accuracy of model- and language-specific 3-way (H\\(_1\\),H\\(_2\\), MT) classifiers on balanced held-out sets for every language. Random baseline: 0.33.\n\n\n\n\n\n7.3.2 Can LLMs Reproduce Human Translation Styles?\nTo confirm whether MT personalization can be achieved, we test the LLM’s ability to mimic the stylistic choices of a particular translator in a multi-shot (MS) prompting setup. For each translator available across tested novels, we provide the model with 20 in-context examples selected from the original pool of translated paragraphs by that translator, asking it to generate a consistent translation. We compare MS results with the default zero-shot (ZS) prompting, which uses no examples from the translator, to quantify the effect of in-context examples. Table 7.4 presents results for personalization accuracy, automatically evaluated using our high-scoring classifiers from the previous section; and translation quality, estimated via the widely used comet MT metric (Rei et al., 2020). The proportion of outputs categorized as matching the translator’s style is increased two- to four-fold following MS prompting, suggesting that LLMs can employ implicit clues in small sets of user examples to produce personalized translations. Stable scores for comet also confirm that translation quality is maintained during style adaptations.\n\n\n\n\n\n\n\nGemma 2 2B\nGemma 2 9B\nLlama 3.1 8B\n\n\n\n\n\n📊\n☄️\n📊\n☄️\n📊\n☄️\n\n\nZS\n0.10\n0.69\n0.08\n0.71\n0.08\n0.70\n\n\nMS\n0.24\n0.69\n0.31\n0.73\n0.32\n0.73\n\n\n\n\n\nTable 7.4: Classifier-based personalization accuracy (📊) and Comet-based translation quality (☄️) for zero-shot (ZS) and multi-shot (MS) prompting with 20 in-context examples averaged across all translators and languages.\n\n\n\n\n\n7.3.3 Finding Personalization Information in LLM Representations\nIn light of these results, we set out to test how the model encodes information reflecting a stylistic shift when style-appropriate examples are provided. To this purpose, we train linear probes (Belinkov, 2022) using model activations as input features to predict the style label (MT, H\\(_1\\), or H\\(_2\\)) that the style classifier (from Section 7.3.1) would assign to the eventual translation, based purely on the prompt’s internal representation. Probing accuracy is measured by testing the model’s ability to predict the classified outcome before generation, using only the prompt representation formed by the model. Given a test set of human-translated paragraphs, we train our probes on a set of examples using an MS prompt with 20 in-context examples. The set is balanced between prompts that showcase personalization with gold in-context examples from a human translator and non-personalized prompts with MT-generated examples previously produced by the same tested model in a ZS setup. Test examples are selected from the respective novels to ensure for the classifier prediction shifts from MT in the ZS setting to the style of in-context examples when MS is used, signaling a causal influence of demonstrations on output personalization.3 This balanced setup prevents the leakage of task information, such as the number of in-context examples, to learned probes, ensuring that stylistic differences between human- and machine-generated in-context examples are the sole factor determining differences in model activations. We focus specifically on Gemma models, extracting activations after the attention block at each model layer for the last token of the prompt, which was previously shown to encode key task-relevant information (Hendel et al., 2023; Todd et al., 2024; Scalena et al., 2024). Figure 7.2 reports probe accuracies across all layers of Gemma 2 2B and 9B. We find a peak in probe accuracy of \\(\\sim95\\%\\) around intermediate model layers, suggesting that these layers encode stylistic information with near-perfect precision.4 These results confirm that personalization is discernible from LLMs’ internal representation, motivating our experiments towards the design of inference-time interventions to steer models towards personalized MT outputs.\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\nFigure 7.2: Probing classifier performance on the human translation detection task across Gemma 2 2B (left) and 9B (right) layers. Activations in intermediate layers are found to capture translation style information with high precision.",
    "crumbs": [
      "Conditioning Generation for Personalized Machine Translation",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Steering Language Models for Personalized Machine Translation</span>"
    ]
  },
  {
    "objectID": "chapters/chap-7-sae-litmt.html#methods",
    "href": "chapters/chap-7-sae-litmt.html#methods",
    "title": "7  Steering Language Models for Personalized Machine Translation",
    "section": "7.4 Methods",
    "text": "7.4 Methods\nWe begin by introducing the prompting and steering methods that we use as baselines and outline our own proposed SAE-based steering approach for personalized translation.\n\n7.4.1 Prompting Baselines\nZero-Shot (ZS) The ZS setup used in our main experiment corresponds to the one from Section 7.3.1, in which the model is simply asked to produce a translation with no conditioning from examples or explanations towards the target translation style. We use this setting to establish a baseline style and translation quality performance for the models.\nZero-Shot Explain (ZS-Exp) Building upon the ZS setting, we experiment with a prompting strategy where LLMs are provided with detailed explanations of the most salient elements that characterize the desired translation style. We obtain such descriptions by prompting a capable proprietary model, GPT-4o (OpenAI, 2023), with 20 translations matching the desired style, asking it to synthesize a set of guidelines to match the examples. We evaluate two contrastive variants of this approach, providing GPT-4o with either MT examples (ZS-Exp\\(_\\text{HT}\\)) or alternative human translations (ZS-Exp\\(_\\text{PT}\\)) alongside examples matching the desired style, and asking to describe what characterizes the latter compared to the former. To avoid data leakage, all generated explanations are manually reviewed to ensure they do not contain any verbatim content or direct excerpts from the input examples.5 Tested models are then prompted with GPT-4o explanations in a ZS setting, to verify whether interpretable directives synthesized from a set of examples matching the desired behavior can produce reliable personalization results.\nMulti-Shot (MS) Following Section 7.3.2’s findings, we adopt the same MS setup using 20 in-context translation examples matching the style of a target human translator (H\\(_1\\) or H\\(_2\\)).\n\n\n7.4.2 Steering Baselines\nWe employ the Activation Addition (ActAdd) and Representation Fine-tuning (ReFT) methods introduced in Section 2.3.2 as baselines for comparing the effectiveness of our proposed method. For ActAdd, we employ the standard contrastive formulation by Rimsky et al. (2024) and Scalena et al. (2024) to extract two sets of style-relevant (\\(\\{z\\}^+\\)) and default (\\(\\{z\\}^-\\)) activations from a given model layer using 20 in-context examples demonstrating default behavior (MT) and the desired behavior (H\\(_1\\) or H\\(_2\\) translations), respectively. We then compute the average \\(\\Delta\\) steering vector between the two sets of activations, scale it by a factor of \\(\\alpha\\) = 2, which was found to be effective by previous research (Scalena et al., 2024), and apply it additively to the same model layer during inference. For ReFT, we apply learned interventions to the same personalization-relevant layers identified in Section 7.3.3 and limit confounding factors by tuning ReFT interventions with the set of 20 examples used for MS prompting.\n\n\n7.4.3 Contrastive SAE Steering\nGiven the SAE formulation we present in Section 2.3.2, our primary interest lies in the sparse latents \\(h(z_l) \\in \\mathbb{R}^m\\) learned by the SAE encoder, which were empirically found to capture monosemantic and interpretable properties of model inputs.\nContrastive prompt setup Given a set of paragraphs \\(\\mathcal{D}\\) for a novel in the Par3 dataset, each instance in it is a tuple:\n\\[\n\\mathcal{D} = \\left\\{ \\left&lt; s, \\text{H}_1, \\text{H}_2, \\text{MT}_{\\text{model}} \\right&gt; \\right\\}\n\\]\nwith s being the non-English source sentence, \\(\\text{H}_1\\) and \\(\\text{H}_2\\) translations from two distinct human translators and \\(\\text{MT}_{\\text{model}}\\) the machine translation from the model under evaluation. Similar to previous methods, we employ a contrastive approach to extract SAE latents that are most active in the presence of the desired personalization style, while simultaneously controlling for more generic features that capture the generic properties of the task. We define two sets of contrastive prompts:\n\\[\n\\mathcal{D}^+ = \\left\\{ \\left&lt; s, e^{+} \\right&gt; \\right\\}\\;\\;\\text{and}\\;\\;\\mathcal{D}^- = \\left\\{ \\left&lt; s, e^{-} \\right&gt; \\right\\}\n\\]\ncapturing the personalized style of interest and baseline properties of the task, respectively. Similarly to the ZS-Exp setup from Section 7.4.1, we explore two \\(\\mathcal{D}^-\\) configurations using either \\(e^- =\\) MT (SAE Cont.\\(_\\text{HT}\\)) or \\(e^- =\\) H\\(_2\\) (or H\\(_1\\), if H\\(_2\\) is the personalization target; SAE Cont.\\(_\\text{PT}\\)) to assess the effect of baseline choice on steering effectiveness.\nFeature extraction First, we gather activations \\(z^+_l\\) and \\(z^-_l\\) by prompting the model with inputs from the two contrastive sets \\(\\mathcal{D}^+\\) and \\(\\mathcal{D}^-\\). Activations are extracted at the last prompt token position from its most informative layer, as identified in Section 7.3.3. Activations are then converted into sparse latent representations \\(x^+ = h(z^+)\\) and \\(x^- = h(z^-)\\), with \\(x^+, x^- \\in \\mathbb{R}^m\\) by the SAE encoder. This procedure is repeated across 20 contrastive examples, resulting in two collections of SAE latent vectors for positive/negative examples:\n\\[\n\\begin{aligned}\n\\mathcal{X}^+ &= \\left\\{x^{+}_1, x^{+}_2, \\dots, x^{+}_{20} \\right\\} \\\\\n\\mathcal{X}^- &= \\left\\{x^{-}_1, x^{-}_2, \\dots, x^{-}_{20} \\right\\}\n\\end{aligned}\n\\]\nRelevance-based Feature Selection To identify discriminative features for personalization in the large set of latents, we employ an information-theoretic approach adapted from Zhao et al. (2025). For each of the inputs, we identify the subset of size \\(n &lt; m\\) that includes only the SAE active features, i.e., the latent dimensions for which the logit is greater than 0. We consider logit values in this subset as instances of a random variable \\(X_i \\in x\\), and calculate the mutual information \\(I(X_i, Y)\\) between each feature \\(X_i\\) and the target binary variable \\(Y = \\{+, -\\}\\) corresponding to the style of the provided examples (personalized or non-personalized). A higher \\(I(X_i, Y)\\) indicates that the \\(i\\)-th feature is more informative for discriminating between personalized and default inputs, and can hence be used for steering. A representative sample of 40 latents showing the highest mutual information scores for both personalized (\\(\\{X_i\\}^+\\)) and non-personalized (\\(\\{X_i\\}^-\\)) examples is selected using this procedure.6 For every selected latent, we compute its expected logit when personalization is present or absent in provided examples, i.e. \\(\\mathbb{E}^+[X_i]\\) and \\(\\mathbb{E}^-[X_i]\\).\nInference-time intervention Finally, activations are steered by setting selected latents to their expected value whenever their observed score is below (for the promoted personalized case) or above (for the demoted non-personalized case) the pre-computed average. Hence, in the SAE Cont.\\(_\\text{HT}\\) setting we enhance the features relevant to a target personalized style, e.g. \\(\\{X_i\\}^{H_1}\\) for H\\(_1\\), and suppress the features \\(\\{X_i\\}^{MT}\\), corresponding to the model’s default MT. In SAE Cont.\\(_\\text{PT}\\), instead, we promote the same H\\(_1\\)-related latents while suppressing \\(\\{X_i\\}^{H_2}\\) to steer the model towards H\\(_1\\) personal style. Additionally, we modulate the magnitude of the resulting vector with an \\(\\alpha\\) coefficient, which was found to play an essential role in steering effectiveness in previous research (Scalena et al., 2024; Ferrando et al., 2025). Algorithm 1 outlines the procedure for our proposed latent-based steering. It enhances features identified as relevant to personalization while simultaneously suppressing those negatively correlated with the task.\n\n\n\\begin{algorithm} \\caption{Contrastive SAE Steering} \\begin{algorithmic} \\Require Input activation $z$, SAE model, target latents expected value $\\mathbb{E}^+[X_i]$, contrast latents expected value $\\mathbb{E}^-[X_i]$, steering coefficient $\\alpha$ \\Ensure Steered activation $z_{\\text{new}}$ \\Procedure{ContrastiveSteering}{$z, \\text{SAE}, \\mathbb{E}^+[X_i], \\mathbb{E}^-[X_i], \\alpha$} \\State $x = \\text{SAE.encode}(z)$ \\State $m = \\text{length}(x)$ \\For{$i \\gets 1$ to $m$} \\If{$\\mathbb{E}^+[X_i] &gt; x[i]$} \\State $x[i] = \\mathbb{E}^+[X_i]$ \\EndIf \\If{$\\mathbb{E}^-[X_i] &lt; x[i]$} \\State $x[i] =\\mathbb{E}^-[X_i]$ \\EndIf \\EndFor \\State $z_{\\text{new}} = \\alpha \\cdot \\text{SAE.decode}(x)$ \\State \\textbf{return} $z_{\\text{new}}$ \\EndProcedure \\end{algorithmic} \\end{algorithm}",
    "crumbs": [
      "Conditioning Generation for Personalized Machine Translation",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Steering Language Models for Personalized Machine Translation</span>"
    ]
  },
  {
    "objectID": "chapters/chap-7-sae-litmt.html#sec-chap7-exp",
    "href": "chapters/chap-7-sae-litmt.html#sec-chap7-exp",
    "title": "7  Steering Language Models for Personalized Machine Translation",
    "section": "7.5 Experiments",
    "text": "7.5 Experiments\n\n7.5.1 Setup\nModel selection We evaluate our methods on the same three models used for our preliminary evaluation of Section 7.3. Our selection is guided by the availability of open-source pre-trained SAEs, which can be computationally expensive to train otherwise. For Gemma models, we employ SAEs from the GemmaScope suite (Lieberum et al., 2024); for the Llama 3.1 model, we employ the SAE released by McGrath et al. (2024). GemmaScope SAEs are available for every model layer, enabling us to steer Gemma models on their most informative layers for the task, which we identified in Section 7.3.3. On the contrary, a single SAE for the 19th layer is available for Llama, hence limiting our evaluation of SAE steering and potentially producing sub-optimal steering results for that model.\nMetrics We evaluate our approaches on a held-out test set sourced from the Par3 dataset for personalization and output quality. For personalization, we use the classifiers described in Section 7.3.1. We define three submetrics employing the classifier probability distribution over the three classes (MT, H\\(_1\\), H\\(_2\\)) to better analyze different aspects of classifiers’ predictions. First, we compute H accuracy as the classifier’s total probability assigned to human-like translations, \\(p(\\)H\\(_1\\)\\() + p(\\)H\\(_2\\)\\()\\), thereby measuring the generic human-like style of the text. To measure personalization, we employ the personalization P, corresponding only to the human translation currently selected as target (H\\(_1\\) or H\\(_2\\)). Finally, the more stringent P\\(_\\text{flip}\\) metric measures the proportion of examples for which the applied conditioning procedure (either prompting or steering) causally influences the resulting classifier prediction, identifying examples for which the label flips from MT to the desired target.\nTo ensure that our interventions do not result in a degradation of overall translation quality, we also employ comet7 (Rei et al., 2020) using the personalized translation as reference.\n\n\n\n\n\n\nSource/Method\nTranslation\n📊 Classifier\n☄️ COMET\n\n\n\n\nSource (FR)\nCette somme vous sera restituée à votre sortie de prison, dit le juge. En attendant, vous êtes libres sous caution.\n\n\n\n\nH1\nThis sum of money will be returned to you when you leave prison,' said the judge. 'In the meantime you are free on bail.\n-\n-\n\n\nZS\nThis amount will be returned to you upon your release from prison, the judge said. Meanwhile, you are free on bail.\nMT\n0.79\n\n\nSAE Cont.\\(_\\text{HT}\\) (\\(\\alpha\\)=5)\nThis sum will be repaid to you at your departure from prison, the judge declared. In the meantime, you are released under bond.\nMT\n0.85\n\n\nSAE Cont.\\(_\\text{HT}\\) (\\(\\alpha\\)=50)\n#echo all ought to goodness too was put moreover goodness behaving goodness goodness goodness goodness [...]\nH1\n0.20\n\n\nSAE Cont.\\(_\\text{HT}\\) (\\(\\alpha\\)=150)\nGURL callers goodness_Noneummy amid'ala Reportedaps Beaverishlor Decompiled_unset queer headquarters Headquarters twe ... [...]\nH1\n0.23\n\n\nSource (DE)\nNein – was Sie lieber haben, Herr Leutnant! Nur keine Zeremonien, es ist doch ganz einerlei.\n\n\n\n\nH2\n\"No, no-whatever you would rather have, Lieutenant Hofmiller! Please don't stand on ceremony, it makes no difference to us.\"\n-\n-\n\n\nZS\nNo, whatever you prefer, Lieutenant! Just no ceremonies, it doesn't matter.\nMT\n0.76\n\n\nSAE Cont.\\(_\\text{HT}\\) (\\(\\alpha\\)=5)\nNo, anything at all you want, sir! Just don't make a fuss about it, it really doesn't matter.\nH2\n0.79\n\n\nSAE Cont.\\(_\\text{HT}\\) (\\(\\alpha\\)=50)\n\"&gt;I Don't worry about that... I don't want a ceremony for this one. It's not important...\n[H2]{color=‘brand-color.lightbluedim’]\n0.46\n\n\nSAE Cont.\\(_\\text{HT}\\) (\\(\\alpha\\)=150)\nIWhenInWhatItDonIf Sometimes AIs Celebrating cerimonies... Sosir please don't have parties ey' [...]\nH2\n0.24\n\n\n\n\n\nTable 7.5: Examples from different languages being classified as Human when using extreme \\(\\alpha\\) values.\n\n\n\nQuality-accuracy trade-off We begin by verifying the optimal steering intensity \\(\\alpha\\) for our SAE steering technique. We primarily focus on results from Gemma 2 2B, for which we ran a comprehensive sweep over all relevant hyperparameters.8 Figure 7.3 illustrates the influence of \\(\\alpha\\) on MT personalization accuracy and fluency averaged across all translators for all tested languages. For values of \\(\\alpha \\leq 3\\), performance remains close to that of the MS baseline, indicating that the contrastive method is effectively isolating latents associated with human-like style. As \\(\\alpha\\) increases, performance generally exceeds the MS approach, achieving greater control and flexibility in guiding the model’s output with minimal impact on translation quality. However, for \\(\\alpha \\geq 10\\), we observe a notable degradation in comet, suggesting an important drop in translation fluency. Table 7.5 shows some examples of models generating output aligned with the Human translator according to the classifier, but with a low comet score corresponding to an almost unreadable output due to extreme \\(\\alpha\\) values. False positive classifications in such settings suggest that steering methods and classifiers are aligned with potentially spurious stylistic features, which are not necessarily indicative of high-quality translations. We leave the investigation of these spurious features to future work, focusing here on the trade-off between personalization and translation quality.\nFollowing Ferrando et al. (2025), which also employ SAEs for steering, we experiment with very high alpha values (up to 150), finding the classifier’s H accuracy approaching 100% for some languages. While this indicates that the contrastive steering is aggressively optimizing toward classifier preferences (Figure 7.4), the consequent drop in comet scores reveals a steep decline in translation quality, often resulting in incoherent or nonsensical generations from a human perspective. Ultimately, we identify \\(\\alpha\\) = 5 as an appropriate steering intensity to balance personalization and fluency, and employ it for our main evaluation.\n\n\n\n\n\n\nFigure 7.3: Personalization P and comet across various steering intensity \\(\\alpha\\) for SAE Cont.\\(_\\text{HT}\\) on Gemma 2 2B. The performance of zero-shot multi-shot (MS), zero-shot explain (Exp) and zero-shot (ZS = 0) baselines is also reported.\n\n\n\n\n\n\n\n\n\nFigure 7.4: comet and H accuracy across \\(\\alpha\\) steering intensity values for Gemma 2 2B, showing a major drop in translation quality for very high intensities (\\(\\alpha \\geq 50\\)).\n\n\n\n\n\n7.5.2 Results and Discussion\nTable 7.6 presents performances of tested models across prompting and steering setups, averaged across all languages and personalization targets (H\\(_1\\) and H\\(_2\\) for each language). We find that our SAE Cont.\\(_\\text{HT}\\) and SAE Cont.\\(_\\text{PT}\\) methods generally achieve the best trade-off between personalization accuracy and translation quality, especially for the smaller Gemma 2 2B model. This could be due to the larger models’ superior ability to incorporate in-context information naturally, reducing the relative benefit of explicit steering. Comparing the two contrastive setups (HT and PT) for the ZS-Exp and SAE Cont. methods, we find that using different human demonstrations as a contrastive baseline in PT generally produces better results for larger models. As for general performance, we conjecture this could be due to the larger models’ improved ability to disentangle personalization-critical factors without explicit guidance. For the smaller Gemma 2 2B, the difference between the two approaches is minimal, suggesting that the model cannot fully exploit the differences between the examples.\n\n\n\n\n\n\n\nGemma 2 2B\nGemma 2 9B\nLlama 3.1 8B\n\n\n\n\n\nH\nP\nPflip\n☄️\nH\nP\nPflip\n☄️\nH\nP\nPflip\n☄️\n\n\nZS\n0.21\n0.10\n0.05\n0.69\n0.15\n0.08\n0.04\n0.71\n0.24\n0.08\n0.05\n0.70\n\n\nZS-Exp.HT\n0.30\n0.22\n0.16\n0.68\n0.41\n0.22\n0.18\n0.72\n0.56\n0.23\n0.21\n0.69\n\n\nZS-Exp.PT\n--\n0.20\n0.14\n0.69\n--\n0.23\n0.19\n0.73\n--\n0.30\n0.26\n0.70\n\n\nMS\n0.37\n0.24\n0.16\n0.69\n0.48\n0.31\n0.27\n0.73\n0.58\n0.32\n0.28\n0.73\n\n\nActAdd\n0.27\n0.22\n0.12\n0.67\n0.32\n0.24\n0.20\n0.70\n0.55\n0.36\n0.28\n0.70\n\n\nReFT\n0.31\n0.22\n0.18\n0.70\n0.46\n0.34\n0.27\n0.67\n0.53\n0.38\n0.26\n0.70\n\n\nSAE Cont.HT\n0.39\n0.27\n0.19\n0.70\n0.46\n0.33\n0.29\n0.72\n0.59\n0.31\n0.27\n0.72\n\n\nSAE Cont.PT\n--\n0.27\n0.18\n0.69\n--\n0.35\n0.29\n0.73\n--\n0.33\n0.28\n0.72\n\n\n\n\n\nTable 7.6: Averaged metric scores across all tested languages (per-language breakdown in appendix). H: human style accuracy, i.e. \\(p(\\)H\\(_1\\)\\() + p(\\)H\\(_2\\)\\()\\). P: personalization accuracy p(H\\(_x\\)) for the target style. P\\(_\\text{flip}\\): Proportion of segments for which steering has a causal impact on personalization. \\(\\alpha\\) = 5 is used for SAE Cont. results.\n\n\n\nDo SAE Steering and MS Prompting Impact Activations in a Similar Way? Since SAE-based approaches perform on par or better than MS, we set out to investigate whether the two methods result in a similar impact on model representations. We collect the modified activations \\(z_{\\text{steer}}\\) obtained from the SAE Cont.\\(_\\text{HT}\\) steering setting and evaluate them using the probing classifier trained on MS-conditioned activations, as introduced in Section 7.3.3, for detecting personalization information. Table 7.7 shows probe accuracy in detecting the positive impact of SAE steering across the three possible outcomes of the steering procedure. We find that the probe trained on the SAE layer effectively distinguishes between activations corresponding to successful and unsuccessful SAE steering, despite having been exposed only to MS conditioning during training. This includes both instances where the classifier prediction is flipped after steering (MT \\(\\to\\) H) and settings where the conditioning fails (MT \\(\\to\\) MT). In settings where the original output already matches human style (H \\(\\to\\) H*), the probe obtains lower accuracy with broader confidence intervals, denoting higher uncertainty. These findings suggest that the SAE’s latents we extract through our contrastive method are meaningfully connected to the stylistic patterns embedded in the multi-shot examples, providing evidence that our intervention influences the internal representations of the model, aligning them to the natural effect of the MS approach.\n\n\n\n\n\n\n\nMT → H*\nMT → MT\nH* → H*\n\n\n\n\nGemma 2 2B\n0.94 ±0.01\n0.07 ±0.02\n0.72 ±0.15\n\n\nGemma 2 9B\n0.93 ±0.02\n0.12 ±0.10\n0.68 ±0.19\n\n\n\n\n\nTable 7.7: Probing accuracy on ContHT-steered activations averaged across languages and H\\(_1\\)/H\\(_2\\) translators. Probes trained on MS activations reliably detect the impact of SAE Cont. steering over model generations, suggesting similar mechanisms.",
    "crumbs": [
      "Conditioning Generation for Personalized Machine Translation",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Steering Language Models for Personalized Machine Translation</span>"
    ]
  },
  {
    "objectID": "chapters/chap-7-sae-litmt.html#limitations",
    "href": "chapters/chap-7-sae-litmt.html#limitations",
    "title": "7  Steering Language Models for Personalized Machine Translation",
    "section": "7.6 Limitations",
    "text": "7.6 Limitations\nWhile we demonstrates the potential of steering LLMs for MT personalization using sparse autoencoders, we acknowledge several limitations.\nFirstly, the generalizability of our findings is constrained by the scope of our experiments. We focused on literary translation from seven specific source languages into English and evaluated three LLMs of relatively small size. Consequently, the observed effectiveness of SAE-based steering and the identified optimal layers for intervention may not directly transfer to other language pairs, significantly different model architectures or sizes, or distinct domains beyond literary texts. Further research is needed to assess the robustness of our approach across a broader range of linguistic and modeling contexts.\nSecondly, the computational overhead associated with sparse autoencoders presents a practical challenge. Although we utilized pre-trained SAEs in our study, the initial training of these components is resource-intensive. This could limit the accessibility and scalability of our proposed method, particularly for researchers or practitioners with limited computational resources or when frequent retraining for new models or tasks is required. The current availability of pre-trained SAEs also restricts model choice, as seen with the Llama 3.1 8B model, where an SAE was only available for a potentially sub-optimal layer.\nFinally, our investigation primarily focused on downstream performance and the impact of various personalization strategies on model representations. However, we did not pursue a mechanistic understanding of the “personalization circuits” within the LLMs. Future work could adopt a more fine-grained mechanistic interpretability approach to study how specific SAE latents or combinations thereof encode and manipulate nuanced stylistic features, thereby providing deeper insights into the underlying processes of LLM personalization.",
    "crumbs": [
      "Conditioning Generation for Personalized Machine Translation",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Steering Language Models for Personalized Machine Translation</span>"
    ]
  },
  {
    "objectID": "chapters/chap-7-sae-litmt.html#conclusion",
    "href": "chapters/chap-7-sae-litmt.html#conclusion",
    "title": "7  Steering Language Models for Personalized Machine Translation",
    "section": "7.7 Conclusion",
    "text": "7.7 Conclusion\nWe conducted a broad evaluation of various prompting and steering approaches for personalizing LLM-generated translations. Our evaluation targets a practical, real-world application of literary translation and addresses the underexplored challenge of steering LLM generations in a linguistically rich and stylistically sensitive domain. Through comprehensive evaluation across multiple languages, novels, and models, we demonstrate that our proposed SAE-based approach outperforms prompting and alternative steering techniques.\nAlthough faithfully replicating individual human translation styles remains a highly challenging task, our approach achieves strong alignment with human translation quality, as reflected in both general human-likeness and translator-specific personalization metrics. These results highlight the method’s robustness and its potential to support high-fidelity translation workflows in real-world settings. Concretely, these results have important implications in the development of personalized MT systems based on LLMs. In particular, the notable effectiveness of our proposed approach on smaller models might enable MT customization when few examples are available, facilitating further research on how personalization information is encoded and produced by language models. Despite their effectiveness, the interpretability of the learned SAE latents and their potential use with larger LLMs—where increased capacity may further enhance the precision and fluency of personalized translations—remain open questions for future investigation.\nThe success of SAE-based steering for personalized translation highlights the effectiveness of internals-based interventions for controlling model generation. However, the practical deployment of such approaches requires a careful evaluation of their impact on users’ trust and behaviors. The first two parts of this thesis focused on developing methods for analyzing and steering model generation. The third and final part, beginning with the next chapter, focuses instead on the users of machine translation systems, specifically professional post-editors and translators, to explore how their interactions with machine-translated content are shaped by factors such as language similarity and translation quality. Finally, we investigate whether these interactions can be improved using the trove of information available from the inner workings of MT models.\n\n\n\n\nAndy Arditi, Oscar Obeso, Aaquib Syed, Daniel Paleka, Nina Panickssery, Wes Gurnee, and Neel Nanda. 2024. Refusal in language models is mediated by a single direction. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, Advances in neural information processing systems, volume 37, pages 136037–136083, Red Hook, NY, USA. Curran Associates, Inc.\n\n\nYonatan Belinkov. 2022. Probing classifiers: Promises, shortcomings, and advances. Computational Linguistics, 48(1):207–219.\n\n\nNathaniel Berger, Stefan Riezler, Miriam Exel, and Matthias Huck. 2024. Post-edits are preferences too. In Barry Haddow, Tom Kocmi, Philipp Koehn, and Christof Monz, editors, Proceedings of the ninth conference on machine translation, pages 1289–1300, Miami, Florida, USA. Association for Computational Linguistics.\n\n\nEleftheria Briakou, Jiaming Luo, Colin Cherry, and Markus Freitag. 2024. Translating step-by-step: Decomposing the translation process for improved translation quality of long-form texts. In Barry Haddow, Tom Kocmi, Philipp Koehn, and Christof Monz, editors, Proceedings of the ninth conference on machine translation, pages 1301–1317, Miami, Florida, USA. Association for Computational Linguistics.\n\n\nSviatoslav Chalnev, Matthew Siu, and Arthur Conmy. 2024. Improving steering vectors by targeting sparse autoencoder features. Arxiv.\n\n\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Unsupervised cross-lingual representation learning at scale. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault, editors, Proceedings of the 58th annual meeting of the association for computational linguistics, pages 8440–8451, Online. Association for Computational Linguistics.\n\n\nJavier Ferrando, Oscar Balcells Obeso, Senthooran Rajamanoharan, and Neel Nanda. 2025. Do i know this entity? Knowledge awareness and hallucinations in language models. In The thirteenth international conference on learning representations.\n\n\nLucie Flekova, Jordan Carpenter, Salvatore Giorgi, Lyle Ungar, and Daniel Preoţiuc-Pietro. 2016. Analyzing biases in human perception of user age and gender from text. In Katrin Erk and Noah A. Smith, editors, Proceedings of the 54th annual meeting of the association for computational linguistics (volume 1: Long papers), pages 843–854, Berlin, Germany. Association for Computational Linguistics.\n\n\nXavier Garcia and Orhan Firat. 2022. Using natural language prompts for machine translation. Arxiv.\n\n\nAna Guerberof-Arenas and Antonio Toral. 2022. Creativity in translation: Machine translation as a constraint for literary texts. Translation Spaces, 11(2):184–212.\n\n\nRoee Hendel, Mor Geva, and Amir Globerson. 2023. In-context learning creates task vectors. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Findings of the association for computational linguistics: EMNLP 2023, pages 9318–9333, Singapore. Association for Computational Linguistics.\n\n\nRobert Huben, Hoagy Cunningham, Logan Riggs Smith, Aidan Ewart, and Lee Sharkey. 2024. Sparse autoencoders find highly interpretable features in language models. In The twelfth international conference on learning representations.\n\n\nMarzena Karpinska and Mohit Iyyer. 2023. Large language models effectively leverage document-level context for literary translation, but critical errors persist. In Philipp Koehn, Barry Haddow, Tom Kocmi, and Christof Monz, editors, Proceedings of the eighth conference on machine translation, pages 419–451, Singapore. Association for Computational Linguistics.\n\n\nJihyeon Lee, Taehee Kim, Yunwon Tae, Cheonbok Park, and Jaegul Choo. 2023. PePe: Personalized post-editing model utilizing user-generated post-edits. In Andreas Vlachos and Isabelle Augenstein, editors, Findings of the association for computational linguistics: EACL 2023, pages 239–253, Dubrovnik, Croatia. Association for Computational Linguistics.\n\n\nTom Lieberum, Senthooran Rajamanoharan, Arthur Conmy, Lewis Smith, Nicolas Sonnerat, Vikrant Varma, Janos Kramar, Anca Dragan, Rohin Shah, and Neel Nanda. 2024. Gemma scope: Open sparse autoencoders everywhere all at once on gemma 2. In Yonatan Belinkov, Najoung Kim, Jaap Jumelet, Hosein Mohebbi, Aaron Mueller, and Hanjie Chen, editors, Proceedings of the 7th BlackboxNLP workshop: Analyzing and interpreting neural networks for NLP, pages 278–300, Miami, Florida, US. Association for Computational Linguistics.\n\n\nHuan Lin, Liang Yao, Baosong Yang, Dayiheng Liu, Haibo Zhang, Weihua Luo, Degen Huang, and Jinsong Su. 2021. Towards user-driven neural machine translation. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, Proceedings of the 59th annual meeting of the association for computational linguistics and the 11th international joint conference on natural language processing (volume 1: Long papers), pages 4008–4018, Online. Association for Computational Linguistics.\n\n\nXiaoming Liu, Zhaohan Zhang, Yichen Wang, Hang Pu, Yu Lan, and Chao Shen. 2023. CoCo: Coherence-enhanced machine-generated text detection under low resource with contrastive learning. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 conference on empirical methods in natural language processing, pages 16167–16188, Singapore. Association for Computational Linguistics.\n\n\nEvgeny Matusov. 2019. The challenges of using neural machine translation for literature. In James Hadley, Maja Popović, Haithem Afli, and Andy Way, editors, Proceedings of the qualities of literary machine translation, pages 10–19, Dublin, Ireland. European Association for Machine Translation.\n\n\nThomas McGrath, Daniel Balsam, Myra Deng, and Eric Ho. 2024. Understanding and steering llama 3 with sparse autoencoders. Goodfire Blog.\n\n\nPaul Michel and Graham Neubig. 2018. Extreme adaptation for personalized neural machine translation. In Iryna Gurevych and Yusuke Miyao, editors, Proceedings of the 56th annual meeting of the association for computational linguistics (volume 2: Short papers), pages 312–318, Melbourne, Australia. Association for Computational Linguistics.\n\n\nJoss Moorkens, Antonio Toral, Sheila Castilho, and Andy Way. 2018. Translators’ perceptions of literary post-editing using statistical and neural machine translation. Translation Spaces, 7(2):240–262.\n\n\nMaria Nadejde, Anna Currey, Benjamin Hsu, Xing Niu, Marcello Federico, and Georgiana Dinu. 2022. CoCoA-MT: A dataset and benchmark for contrastive controlled MT with application to formality. In Marine Carpuat, Marie-Catherine de Marneffe, and Ivan Vladimir Meza Ruiz, editors, Findings of the association for computational linguistics: NAACL 2022, pages 616–632, Seattle, United States. Association for Computational Linguistics.\n\n\nXing Niu, Marianna Martindale, and Marine Carpuat. 2017. A study of style in machine translation: Controlling the formality of machine translation output. In Martha Palmer, Rebecca Hwa, and Sebastian Riedel, editors, Proceedings of the 2017 conference on empirical methods in natural language processing, pages 2814–2819, Copenhagen, Denmark. Association for Computational Linguistics.\n\n\nOpenAI. 2023. Gpt-4 technical report. Arxiv.\n\n\nElla Rabinovich, Raj Nath Patel, Shachar Mirkin, Lucia Specia, and Shuly Wintner. 2017. Personalized machine translation: Preserving original author traits. In Mirella Lapata, Phil Blunsom, and Alexander Koller, editors, Proceedings of the 15th conference of the European chapter of the association for computational linguistics: Volume 1, long papers, pages 1074–1084, Valencia, Spain. Association for Computational Linguistics.\n\n\nRicardo Rei, Craig Stewart, Ana C Farinha, and Alon Lavie. 2020. COMET: A neural framework for MT evaluation. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, Proceedings of the 2020 conference on empirical methods in natural language processing (EMNLP), pages 2685–2702, Online. Association for Computational Linguistics.\n\n\nNina Rimsky, Nick Gabrieli, Julian Schulz, Meg Tong, Evan Hubinger, and Alexander Turner. 2024. Steering llama 2 via contrastive activation addition. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd annual meeting of the association for computational linguistics (volume 1: Long papers), pages 15504–15522, Bangkok, Thailand. Association for Computational Linguistics.\n\n\nGabriele Sarti, Phu Mon Htut, Xing Niu, Benjamin Hsu, Anna Currey, Georgiana Dinu, and Maria Nadejde. 2023. RAMP: Retrieval and attribute-marking enhanced prompting for attribute-controlled translation. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st annual meeting of the association for computational linguistics (volume 2: Short papers), pages 1476–1490, Toronto, Canada. Association for Computational Linguistics.\n\n\nDanielle Saunders and Bill Byrne. 2020. Reducing gender bias in neural machine translation as a domain adaptation problem. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault, editors, Proceedings of the 58th annual meeting of the association for computational linguistics, pages 7724–7736, Online. Association for Computational Linguistics.\n\n\nDaniel Scalena, Gabriele Sarti, and Malvina Nissim. 2024. Multi-property steering of large language models with dynamic activation composition. In Yonatan Belinkov, Najoung Kim, Jaap Jumelet, Hosein Mohebbi, Aaron Mueller, and Hanjie Chen, editors, Proceedings of the 7th BlackboxNLP workshop: Analyzing and interpreting neural networks for NLP, pages 577–603, Miami, Florida, US. Association for Computational Linguistics.\n\n\nDaniel Scalena^*, Gabriele Sarti^*, Arianna Bisazza, Elisabetta Fersini, and Malvina Nissim. 2025. Steering large language models for machine translation personalization. Arxiv Preprint.\n\n\nRico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Controlling politeness in neural machine translation via side constraints. In Kevin Knight, Ani Nenkova, and Owen Rambow, editors, Proceedings of the 2016 conference of the north American chapter of the association for computational linguistics: Human language technologies, pages 35–40, San Diego, California. Association for Computational Linguistics.\n\n\nGemma Team. 2024a. Gemma 2: Improving open language models at a practical size. Arxiv.\n\n\nLlama Team. 2024b. The llama 3 herd of models. Arxiv.\n\n\nKatherine Thai, Marzena Karpinska, Kalpesh Krishna, Bill Ray, Moira Inghilleri, John Wieting, and Mohit Iyyer. 2022. Exploring document-level literary machine translation with parallel paragraphs from world literature. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Proceedings of the 2022 conference on empirical methods in natural language processing, pages 9882–9902, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\n\n\nEric Todd, Millicent L. Li, Arnab Sen Sharma, Aaron Mueller, Byron C. Wallace, and David Bau. 2024. Function vectors in large language models. In Proceedings of the 2024 international conference on learning representations.\n\n\nAntonio Toral and Andy Way. 2015. Translating literary text between related languages using SMT. In Anna Feldman, Anna Kazantseva, Stan Szpakowicz, and Corina Koolen, editors, Proceedings of the fourth workshop on computational linguistics for literature, pages 123–132, Denver, Colorado, USA. Association for Computational Linguistics.\n\n\nAntonio Toral and Andy Way. 2018. What level of quality can neural machine translation attain on literary text? In Translation quality assessment: From principles to practice, pages 263–287. Springer International Publishing, Cham.\n\n\nEva Vanmassenhove, Christian Hardmeier, and Andy Way. 2018. Getting gender right in neural machine translation. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun’ichi Tsujii, editors, Proceedings of the 2018 conference on empirical methods in natural language processing, pages 3003–3008, Brussels, Belgium. Association for Computational Linguistics.\n\n\nRob Voigt and Dan Jurafsky. 2012. Towards a literary machine translation: The role of referential cohesion. In David Elson, Anna Kazantseva, Rada Mihalcea, and Stan Szpakowicz, editors, Proceedings of the NAACL-HLT 2012 workshop on computational linguistics for literature, pages 18–25, Montréal, Canada. Association for Computational Linguistics.\n\n\nLongyue Wang, Siyou Liu, Chenyang Lyu, Wenxiang Jiao, Xing Wang, Jiahao Xu, Zhaopeng Tu, Yan Gu, Weiyu Chen, Minghao Wu, Liting Zhou, Philipp Koehn, Andy Way, and Yulin Yuan. 2024a. Findings of the WMT 2024 shared task on discourse-level literary translation. In Barry Haddow, Tom Kocmi, Philipp Koehn, and Christof Monz, editors, Proceedings of the ninth conference on machine translation, pages 699–700, Miami, Florida, USA. Association for Computational Linguistics.\n\n\nLongyue Wang, Chenyang Lyu, Tianbo Ji, Zhirui Zhang, Dian Yu, Shuming Shi, and Zhaopeng Tu. 2023a. Document-level machine translation with large language models. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 conference on empirical methods in natural language processing, pages 16646–16661, Singapore. Association for Computational Linguistics.\n\n\nLongyue Wang, Zhaopeng Tu, Yan Gu, Siyou Liu, Dian Yu, Qingsong Ma, Chenyang Lyu, Liting Zhou, Chao-Hong Liu, Yufeng Ma, Weiyu Chen, Yvette Graham, Bonnie Webber, Philipp Koehn, Andy Way, Yulin Yuan, and Shuming Shi. 2023b. Findings of the WMT 2023 shared task on discourse-level literary translation: A fresh orb in the cosmos of LLMs. In Philipp Koehn, Barry Haddow, Tom Kocmi, and Christof Monz, editors, Proceedings of the eighth conference on machine translation, pages 55–67, Singapore. Association for Computational Linguistics.\n\n\nYue Wang, Cuong Hoang, and Marcello Federico. 2021. Towards modeling the style of translators in neural machine translation. In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou, editors, Proceedings of the 2021 conference of the north american chapter of the association for computational linguistics: Human language technologies, pages 1193–1199, Online. Association for Computational Linguistics.\n\n\nYuxia Wang, Jonibek Mansurov, Petar Ivanov, Jinyan Su, Artem Shelmanov, Akim Tsvigun, Osama Mohammed Afzal, Tarek Mahmoud, Giovanni Puccetti, and Thomas Arnold. 2024b. SemEval-2024 task 8: Multidomain, multimodel and multilingual machine-generated text detection. In Atul Kr. Ojha, A. Seza Doğruöz, Harish Tayyar Madabushi, Giovanni Da San Martino, Sara Rosenthal, and Aiala Rosá, editors, Proceedings of the 18th international workshop on semantic evaluation (SemEval-2024), pages 2057–2079, Mexico City, Mexico. Association for Computational Linguistics.\n\n\nYuxia Wang, Jonibek Mansurov, Petar Ivanov, Jinyan Su, Artem Shelmanov, Akim Tsvigun, Osama Mohammed Afzal, Tarek Mahmoud, Giovanni Puccetti, Thomas Arnold, Alham Aji, Nizar Habash, Iryna Gurevych, and Preslav Nakov. 2024c. M4GT-bench: Evaluation benchmark for black-box machine-generated text detection. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd annual meeting of the association for computational linguistics (volume 1: Long papers), pages 3964–3992, Bangkok, Thailand. Association for Computational Linguistics.\n\n\nMinghao Wu, Jiahao Xu, Yulin Yuan, Gholamreza Haffari, Longyue Wang, Weihua Luo, and Kaifu Zhang. 2025. (Perhaps) beyond human translation: Harnessing multi-agent collaboration for translating ultra-long literary texts. Arxiv.\n\n\nWu Youyou, Michal Kosinski, and David Stillwell. 2015. Computer-based personality judgments are more accurate than those made by humans. Proceedings of the National Academy of Sciences, 112(4):1036–1040.\n\n\nPeng Zhang, Zhengqing Guan, Baoxi Liu, Xianghua (Sharon) Ding, Tun Lu, Hansu Gu, and Ning Gu. 2022. Building user-oriented personalized machine translator based on user-generated textual content. Proc. ACM Hum.-Comput. Interact., 6(CSCW2).\n\n\nYu Zhao, Alessio Devoto, Giwon Hong, Xiaotang Du, Aryo Pradipta Gema, Hongru Wang, Xuanli He, Kam-Fai Wong, and Pasquale Minervini. 2025. Steering knowledge selection behaviours in LLMs via SAE-based representation engineering. In Luis Chiruzzo, Alan Ritter, and Lu Wang, editors, Proceedings of the 2025 conference of the nations of the americas chapter of the association for computational linguistics: Human language technologies (volume 1: Long papers), pages 5117–5136, Albuquerque, New Mexico. Association for Computational Linguistics.",
    "crumbs": [
      "Conditioning Generation for Personalized Machine Translation",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Steering Language Models for Personalized Machine Translation</span>"
    ]
  },
  {
    "objectID": "chapters/chap-7-sae-litmt.html#footnotes",
    "href": "chapters/chap-7-sae-litmt.html#footnotes",
    "title": "7  Steering Language Models for Personalized Machine Translation",
    "section": "",
    "text": "FacebookAI/xlm-roberta-large↩︎\nTwo human annotators were asked to label 100 translated paragraphs from the novel Pinocchio (IT\\(\\rightarrow\\)EN) as either human or MT, resulting in an accuracy of \\(\\sim60\\%\\).↩︎\nExamples are resampled for every test paragraph to prevent the probe from overfitting on spurious prompt features.↩︎\nWe find probes for layers 13 and 21 to perform best for the 2B and 9B models, respectively.↩︎\nDetails on the prompt templates are in Section B.2.1.4.↩︎\nBy contrast, traditional SAE-based steering methods only employ features associated with the positive class (Chalnev et al., 2024; Arditi et al., 2024).↩︎\nUnbabel/wmt22-comet-da↩︎\nLarger models were evaluated using a subset of the best-performing configurations. Details in Section B.2.2.↩︎",
    "crumbs": [
      "Conditioning Generation for Personalized Machine Translation",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Steering Language Models for Personalized Machine Translation</span>"
    ]
  },
  {
    "objectID": "chapters/chap-8-divemt.html",
    "href": "chapters/chap-8-divemt.html",
    "title": "8  Machine Translation Post-editing for Typologically Diverse Languages",
    "section": "",
    "text": "8.1 Introduction\nRecent advances in neural language modeling and multilingual training have led to the widespread adoption of machine translation (MT) technologies across an unprecedented range of languages worldwide. While the benefits of state-of-the-art MT for cross-lingual information access are undisputed (Gene, 2021), its usefulness as an aid to professional translators varies considerably across domains, subjects and language combinations (Zouhar et al., 2021). In the last decade, the MT community has been including an increasing number of languages in its automatic and human evaluation efforts (Bojar et al., 2013; Barrault et al., 2021). However, the results of these evaluations are typically not directly comparable across different language pairs for several reasons. First, reference-based automatic quality metrics are hardly comparable across different target languages (Bugliarello et al., 2020). Second, human judgments are collected independently for different language pairs, making their cross-lingual comparison vulnerable to confounding factors such as tested domains and training data sizes. Similarly, recent work on NMT post-editing efficiency has focused on specific language pairs such as English-Czech (Zouhar et al., 2021), German-Italian, German-French (Läubli et al., 2019) and English-Hindi (Ahsan et al., 2021). However, a controlled comparison across a set of typologically diverse languages is still lacking.\nIn this chapter, we conduct an initial assessment of the usefulness of state-of-the-art NMT in professional translation with a strictly controlled cross-language setup (Figure 8.1). Specifically, professionals were asked to translate the same English documents into six typologically distinct languages—Arabic, Dutch, Italian, Turkish, Ukrainian, and Vietnamese—using the same platform and guidelines. Three translation modalities were adopted: human translation from scratch (HT), post-editing of Google Translate’s translation (PE\\(_1\\)), and post-editing of mBART-50’s translation (PE\\(_2\\)), the latter being a state-of-the-art open-source, multilingual NMT system. In addition to post-editing results, subjects’ fine-grained editing behavior, including keystrokes and time information, was logged to measure productivity and effort across languages, systems and translation modalities. Finally, translators were asked to complete a qualitative assessment regarding their perceptions of MT quality and post-editing effort. The resulting DivEMT dataset, to our best knowledge, is the first public resource that allows a direct comparison of professional translators’ productivity and fine-grained editing information across a set of typologically diverse languages. All collected data are publicly released1 alongside this paper to foster further research in the language- and system-dependent nature of NMT advances in real-world translation scenarios.",
    "crumbs": [
      "Interpretability in Human Translation Workflows",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Machine Translation Post-editing for Typologically Diverse Languages</span>"
    ]
  },
  {
    "objectID": "chapters/chap-8-divemt.html#introduction",
    "href": "chapters/chap-8-divemt.html#introduction",
    "title": "8  Machine Translation Post-editing for Typologically Diverse Languages",
    "section": "",
    "text": "Figure 8.1: The DivEMT data collection process. For every English source document, 18 professional translators are tasked with translating it from scratch (HT) or post-editing NMT systems’ outputs (PE\\(_1\\)/PE\\(_2\\)) into six typologically diverse target languages. Behavioral data and qualitative assessments are collected during and after the process, respectively.",
    "crumbs": [
      "Interpretability in Human Translation Workflows",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Machine Translation Post-editing for Typologically Diverse Languages</span>"
    ]
  },
  {
    "objectID": "chapters/chap-8-divemt.html#related-work",
    "href": "chapters/chap-8-divemt.html#related-work",
    "title": "8  Machine Translation Post-editing for Typologically Diverse Languages",
    "section": "8.2 Related Work",
    "text": "8.2 Related Work\nCross-lingual MT Evaluation Before the advent of NMT, Birch et al. (2008) studied how various language properties affected the quality of Statistical MT (SMT) across a sizeable sample of European language pairs. The comparison, however, was solely based on BLEU, which is not directly comparable across different target languages (Bugliarello et al., 2020). Recent work on neural models introduced more principled ways to measure the intrinsic difficulty of language-modeling (Gerz et al., 2018; Cotterell et al., 2018; Mielke et al., 2019) and machine-translating (Bugliarello et al., 2020; Bisazza et al., 2021) different languages. However, reliably achieving this without human evaluation remains an open research question. Concurrently to our research, Licht et al. (2022) proposed a new human evaluation protocol to improve consistency in cross-lingual MT quality assessment.\nPost-editing NMT Recent work highlighted the productivity gains driven by NMT post-editing on a broader array of languages that were previously challenging for MT, such as English-Dutch (Daems et al., 2017), English-Hindi (Ahsan et al., 2021), English-Greek (Stasimioti and Sosoni, 2020), English-Finnish and English-Swedish (Koponen et al., 2020), all showing a considerable variance among language pairs and subjects. Interestingly, Zouhar et al. (2021) found that NMT post-editing speed was comparable to translation from scratch in English-Czech, and highlighted a disconnect between moderate increases in automatic MT quality metrics and improved post-editing productivity. In summary, research on post-editing NMT generally reports increased fluency and output quality; however, productivity gains are hardly generalizable across language pairs and domains. Importantly, to our knowledge, no previous work has studied NMT post-editing over a set of typologically different languages while controlling for the effects of content types and domains, NMT engines, and translation interfaces.",
    "crumbs": [
      "Interpretability in Human Translation Workflows",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Machine Translation Post-editing for Typologically Diverse Languages</span>"
    ]
  },
  {
    "objectID": "chapters/chap-8-divemt.html#the-divemt-dataset",
    "href": "chapters/chap-8-divemt.html#the-divemt-dataset",
    "title": "8  Machine Translation Post-editing for Typologically Diverse Languages",
    "section": "8.3 The DivEMT Dataset",
    "text": "8.3 The DivEMT Dataset\nDivEMT’s primary purpose is to assess the usefulness of state-of-the-art NMT for professional translators and to study how this usefulness varies across target languages with different typological properties. We present below our data collection setup, which strikes a balance between simulating a realistic professional translation workflow and maximizing the comparability of results across languages.\n\n8.3.1 Subjects and Task Scheduling\nTo control for the effect of individual translators’ preferences and styles, we involve a total of 18 subjects (three per target language). During the experiment, each subject receives a series of short documents (3 to 5 sentences each) where the source text is presented in isolation (HT) or alongside a translation proposal produced by one of the NMT systems (PE\\(_1\\), PE\\(_2\\)). The experiment comprises two phases: during the warm-up phase a set of 5 documents is translated by all subjects following the same, randomly sampled sequence of modalities (HT, PE\\(_1\\) or PE\\(_2\\)). This phase allows the subjects to become accustomed to the setup and enables us to identify potential issues in the logged behavioral data before proceeding.2 In the main collection phase, each subject is asked to translate documents in a pseudo-random sequence of modalities. This time, however, the sequence is different for each translator and chosen so that each document gets translated in all three modalities. This allows us to measure translation productivity independently from the subject’s productivity and document-specific difficulties.\nTable 8.1 shows an example of the adopted modality scheduling. The modality of document docM\\(_i\\) for translator T\\(_j\\) in the main task is picked randomly among the two modalities that were not seen by the same translator for docM\\(_{i-1}\\), enforcing consecutive documents given to the same translator to be assigned different modalities to avoid periodicity in repetition and enable same-language comparisons. Importantly, although all three modes were collected for every document, we did not enforce mode consistency across the same translator identifier across languages (i.e. T\\(_1\\) for Italian does not have the same sequence of modalities of translator T\\(_1\\) in Arabic, for example). For this reason, individual subjects are not directly comparable across languages. This is relevant since comparable editing behavior should be attributed to similar personal preferences rather than an identical modality assignment of the same sentences. Despite modality scheduling, we have no guarantees that translators consistently follow the order of documents presented in PET, and thus possibly operate on documents assigned to the same modality consecutively. However, this possibility reduces to random guessing due to a lack of any identifying information related to the modality until the document is entered for editing. The sequence of modalities for the warmup task is fixed and is: HT, PE\\(_2\\), PE\\(_1\\), HT, PE\\(_2\\).\n\n\n\n\n\n\n\n\nT\\(_1\\)\nT\\(_2\\)\nT\\(_3\\)\n\n\n\n\nwarm-up\ndocW\\(_1\\)\nHT\nHT\nHT\n\n\ndocW\\(_2\\)\nPE\\(_1\\)\nPE\\(_1\\)\nPE\\(_1\\)\n\n\n...\n\n\n\n\n\ndocW\\(_N\\)\nPE\\(_2\\)\nPE\\(_2\\)\nPE\\(_2\\)\n\n\nmain\ndocM\\(_1\\)\nHT\nPE\\(_1\\)\nPE\\(_2\\)\n\n\ndocM\\(_2\\)\nPE\\(_2\\)\nHT\nPE\\(_1\\)\n\n\ndocM\\(_3\\)\nHT\nPE\\(_2\\)\nPE\\(_1\\)\n\n\n...\n\n\n\n\n\n\ndocM\\(_N\\)\nPE\\(_2\\)\nPE\\(_1\\)\nHT\n\n\n\n\n\nTable 8.1: Modality scheduling overview. For each language, each subject (T\\(_i\\)) works with a pseudo-random sequence of modalities (HT, PE\\(_1\\), PE\\(_2\\)). For the warm-up task (N=5), all translators are provided with the same documents in the same modalities. For the main task (N=107), each translator is assigned a modality at random. Each document is translated once for every modality. The same procedure is repeated independently for all the languages.\n\n\n\nAs productivity and other behavioral metrics can only be estimated with a sizable sample, we prioritize the number of documents over the number of subjects per language during budget allocation. In future analyses, a larger set of post-edited documents would also provide more insight into the error type distribution of NMT systems across different language pairs.\nAll subjects are professional translators with at least 3 years of professional experience, including at least 1 year of post-editing experience, and strong proficiency in CAT tools.3 Translators were provided with links to the source articles to facilitate contextualization, were asked to produce translations of publishable quality and were instructed not to use any external MT engine to produce their translations. Assessing the final quality of the post-edited material is out of the scope of the current study, although we realize that this is an important consideration to assess usability in a professional context.4\n\n\n8.3.2 Choice of Source Texts\nThe selected documents represent a subset of the FLORES-101 benchmark (Goyal et al., 2022) consisting of sentences taken from English Wikipedia, and covering a mix of topics and domains.5 While professional translators generally specialize in one or a few domains, we opt for a mixed-domain dataset to minimize domain adaptation efforts by the subjects and maximize the generalizability of our results. Importantly, FLORES-101 includes high-quality human translations into 101 languages, which enables the automatic estimation of NMT quality and the discarding of excessively low-scoring models or language pairs before our experiment. FLORES-101 also provides valuable metadata, e.g. source URL, which allows us to ensure the absence of public translations of the selected contents, which could be leveraged by translators and compromise the validity of our setup. The documents used for our study are fragments of contiguous sentences extracted from Wikipedia articles that compose the original FLORES-101 corpus. Even if small, the context provided by document structure allows us to simulate a more realistic translation workflow if compared to out-of-context sentences.\nBased on our available budget, we selected 112 English documents from the devtest portion of FLORES-101, corresponding to 450 sentences and 9,626 words. More details on the data selection process are provided in Section C.1.3.\n\n\n\n\n\n\n\nGenus:Family\nd\\(_{syn}\\)\nMorphology\nMSP\nTTR\nScript\n\n\n\n\nEng\nIndo-European:Germanic\n--\nFusional\n1.17\n0.28\nlatin\n\n\nAra\nAfro-Asiatic:Semitic\n0.57\nIntroflexive\n1.67\n0.46\narabic\n\n\nNld\nIndo-European:Germanic\n0.49\nFusional\n1.16\n0.28\nlatin\n\n\nIta\nIndo-European:Romance\n0.51\nFusional\n1.30\n0.30\nlatin\n\n\nTur\nAltaic:Turkic\n0.70\nAgglutinative\n2.28\n0.50\nlatin\n\n\nUkr\nIndo-European:Slavic\n0.51\nFusional\n1.42\n0.47\ncyrillic\n\n\nVie\nAustro-Asiatic:VietMuong\n0.57\nIsolating\n1.00\n0.12\nlatin\n\n\n\n\n\nTable 8.2: Typological diversity of our language sample. d\\(_{syn}\\): Syntactic distance w.r.t. English Lin et al. (2019). MSP: Mean size of paradigm, from Çöltekin and Rama (2023). TTR: Type-token ratio measured on FLORES-101. Shading indicates genetic/syntactic relatedness to English and morphological complexity/lexical richness.\n\n\n\n\n\n8.3.3 Choice of Languages\nTraining data is one of the most important factors in determining the quality of an NMT system. Unfortunately, using strictly comparable or multi-parallel datasets, such as Europarl (Koehn, 2005) or the Bible corpus (Mayer and Cysouw, 2014), would dramatically restrict the diversity of languages available to our study or imply prohibitively low translation quality on general-domain text. In order to minimize the effect of training data disparity while maximizing language diversity, we choose representatives of six different language families for which comparable amounts of training data are available in our open-source model, namely Arabic, Dutch, Italian, Turkish, Ukrainian, and Vietnamese. As shown in Table 8.2, our language sample exhibits a good diversity in terms of language family, relatedness to English, type of morphological system, morphological complexity, measured by the mean size of paradigm (MSP, Xanthos et al., 2011), and script. We also report the type-token ratio (TTR), the only language property found to correlate significantly with translation difficulty in a sample of European languages (Bugliarello et al., 2020). While the amount of language-specific parallel sentence pairs used for the multilingual fine-tuning of mBART-50 varies widely (4K \\(&lt; N &lt;\\) 45M), all our selected language pairs fall within the 100K-250K range (mid-resourced, see Table 8.3), enabling a fair cross-lingual performance comparison.\n\n\n\n\n\n\n\nGoogle Translate (PE\\(_1\\))\nmBART-50 (PE\\(_2\\))\n# Pairs\n\n\n\n\nAra\n34.1 / 65.6 / .737\n17.0 / 48.5 / .452\n226K\n\n\nNld\n29.1 / 60.0 / .667\n22.6 / 53.9 / .532\n226K\n\n\nIta\n32.8 / 61.4 / .781\n24.4 / 54.7 / .648\n233K\n\n\nTur\n35.0 / 65.5 / 1.00\n18.8 / 52.7 / .755\n204K\n\n\nUkr\n31.1 / 59.8 / .758\n21.9 / 50.7 / .587\n104K\n\n\nVie\n45.1 / 61.9 / .724\n34.7 / 54.0 / .608\n127K\n\n\n\n\n\nTable 8.3: MT quality of the selected NMT systems for English-to-Target translation on the full FLORES-101 devtest split, in Bleu / ChrF / Comet format. Best scores are highlighted in bold. We report the number of sentence pairs used for mBART-50 multilingual fine-tuning by Tang et al. (2021).\n\n\n\n\n\n8.3.4 Choice of MT Systems\nWhile most of the best-performing general-domain NMT systems are commercial, experiments based on such systems are not replicable, as their backends are silently updated over time. Moreover, without knowing the exact training specifics, we cannot attribute differences in the cross-lingual results to intrinsic language properties. We balance these observations by including two NMT systems in our study: Google Translate (GTrans)6 as a representative of commercial quality, and mBART-50 one-to-Many7 (Tang et al., 2021) as a representative of state-of-the-art open-source multilingual NMT technology. The original multilingual BART model (Liu et al., 2020) is an encoder-decoder transformer model pre-trained on monolingual documents in 25 languages. Tang et al. (2021) extend mBART by further pre-training on 25 new languages and performing multilingual translation fine-tuning for the full set of 50 languages, producing three configurations of multilingual NMT models: many-to-one, one-to-many, and many-to-many. Our choice of mBART-50 is primarily motivated by its manageable size, good performance across the set of evaluated languages (see Table 8.3), and its adoption for other NMT studies (Liu et al., 2021) and post-editing evaluations (Fomicheva et al., 2022). Although mBART-50 performances are usually comparable or slightly worse than those of tested bilingual NMT models,8 using a multilingual model allows us to evaluate the downstream effectiveness of a single, unified system trained on pairs evenly distributed across tested languages. Finally, adopting two systems with marked differences in automatic evaluation scores allows us to estimate how a significant increase in metrics such as BLEU, ChrF and comet (Papineni et al., 2002; Popović, 2015; Rei et al., 2020) impacts downstream productivity across languages in a realistic post-editing scenario.\n\n\n8.3.5 Translation Platform and Collected Data\nTranslators were asked to use PET (Aziz et al., 2012), a computer-assisted translation tool that supports both translating from scratch and post-editing. This tool was chosen because (i) it logs information about the post-editing process, which we use to assess effort (see Section 8.4); and (ii) it is a mature research-oriented tool that has been successfully used in several previous studies (Koponen et al., 2012; Toral et al., 2018), and we modify it slightly to support right-to-left languages like Arabic. Using PET, we collect three types of data:\n\nResulting translations produced by translators in either HT or PE modes, constituting a multilingual corpus with one source text and 18 translations (one per language-modality combination) exemplified in Table 8.4.\nBehavioral data for translated sentences, including editing time, amount and type of keystrokes (content, navigation, erase, etc.), and number and duration of pauses above 300/1000 milliseconds (Lacruz et al., 2014).\nPre- and post-task questionnaire. The former focuses on demographics, education, and work experience with translation and post-editing. The latter elicits subjective assessments of post-editing quality, effort and enjoyability compared to translating from scratch.\n\n\n\n\n\n\n\nEng\nSrc\nInland waterways can be a good theme to base a holiday around.\n\n\nAra\nHT\nيمكن أن تكون الممرات المائية الداخلية خياراً جيداً لتخطيط عطلة حولها\n\n\n\nMT\nيمكن أن تكون السكك الحديدية الداخلية موضوعًا جيدًا لإقامة عطلة حول\n\n\n\nPE\nقدتكونالممراتالمائيةالداخليةمكانًاجيدًالقضاءعطلةحولها\n\n\nNld\nHT\nBinnenlandse waterwegen kunnen een goed thema zijn voor een vakantie.\n\n\n\nMT\nBinnenwaterwegen kunnen een goed thema zijn om een vakantie rond te zetten.\n\n\n\nPE\nBinnenwaterwegen kunnen een goed thema zijn om een vakantie rond te organiseren.\n\n\nIta\nHT\nI corsi d'acqua dell'entroterra possono essere un ottimo punto di partenza da cui organizzare una vacanza.\n\n\n\nMT\nI corsi d’acqua interni possono essere un buon tema per fondare una vacanza.\n\n\n\nPE\nI corsi d’acqua interni possono essere un buon tema su cui basare una vacanza.\n\n\nTur\nHT\nİç bölgelerdeki su yolları, tatil planı için iyi bir tema olabilir.\n\n\n\nMT\nİç suyolları, tatil için uygun bir tema olabilir.\n\n\n\nPE\nİç sular tatil için uygun bir tema olabilir.\n\n\nUkr\nHT\nМожна спланувати вихідні, взявши за основу подорож внутрішніми водними шляхами.\n\n\n\nMT\nВодні шляхи можуть бути хорошим об ’єктом для базування відпочинку навколо.\n\n\n\nPE\nМісцевість навколо внутрішніх водних шляхів може бути гарним вибором для організації відпочинку.\n\n\nVie\nHT\nDu lịch trên sông có thể là một lựa chọn phù hợp cho kỳ nghỉ.\n\n\n\nMT\nCác tuyến nước nội địa có thể là một chủ đề tốt để xây dựng một kì nghỉ.\n\n\n\nPE\nDu lịch bằng đường thủy nội địa là một ý tưởng nghỉ dưỡng không tồi.\n\n\n\n\n\nTable 8.4: A DivEMT corpus entry, including the English source (Src), its translation from scratch (HT), the MT output of mBART-50 (MT) and its post-edited version (PE) for all languages. We highlight insertions, deletions, substitutions and shifts computed with Tercom Snover et al. (2006). Full examples available in Table C.5.",
    "crumbs": [
      "Interpretability in Human Translation Workflows",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Machine Translation Post-editing for Typologically Diverse Languages</span>"
    ]
  },
  {
    "objectID": "chapters/chap-8-divemt.html#sec-pe_effort",
    "href": "chapters/chap-8-divemt.html#sec-pe_effort",
    "title": "8  Machine Translation Post-editing for Typologically Diverse Languages",
    "section": "8.4 Post-Editing Effort Across Languages",
    "text": "8.4 Post-Editing Effort Across Languages\nIn this section, we use the DivEMT dataset to quantify the post-editing effort of professional translators across our diverse set of target languages. We consider two main objective indicators of editing effort: temporal measurements (and related productivity gains) and post-editing rates, measured by the Human-targeted Translation Edit Rate (HTER, Snover et al. (2006)). Finally, we assess the subjective perception of PE gains by examining the post-task questionnaires. We reiterate that all scores in this section are computed on the same set of source sentences for all languages, resulting in a faithful cross-lingual comparison of post-editing effort thanks to DivEMT’s controlled setup.\n\n8.4.1 Temporal Effort and Productivity Gains\nWe begin by comparing task time (seconds per processed source word) across languages and modalities. For this purpose, edit times are computed for every document in every language without considering the presence of multiple translators for every language. As shown in Figure 8.2, translation time varies considerably across languages even when no MT system is involved (HT), suggesting an intrinsic variability in translation complexity for different subjects and language pairs. Indeed, for the HT modality, the time required for the “slowest” target languages (Italian, Ukrainian) is roughly twice that of the “fastest” one (Turkish). This pattern cannot be easily explained and contrasts with factors commonly tied to MT complexity, such as source-target morphological richness and language relatedness (Birch et al., 2008; Belinkov et al., 2017). On the other hand, we find that the relation PE\\(_1\\) &lt; PE\\(_2\\) &lt; HT (where PE\\(_1\\) is the fastest, PE\\(_2\\) has a medium speed, and HT is the slowest) holds for all the evaluated languages.\n\n\n\n\n\n\nFigure 8.2: Temporal effort across languages and translation modalities, measured in seconds per processed source word. Each point represents a document, with higher scores denoting slower editing. \\(\\uparrow\\): amount of data points per language not shown in the plot.\n\n\n\nFor a measure of productivity gains that is easier to interpret and more in line with translation industry practices, we turn to productivity expressed in source words processed per minute and compute the speed-up induced by the two post-editing modalities over translating from scratch (\\(\\Delta\\)HT). Table 8.5 presents our results. Across systems, we find that large differences among automatic MT quality metrics indeed reflect post-editing effort, suggesting a nuanced picture that complements the findings of Zouhar et al. (2021). While post-editing time gains were observed to quickly saturate for slight changes in high-quality MT, we find that moving from medium-quality to high-quality MT yields meaningful productivity improvements across most evaluated languages. Across languages, too, the magnitude of productivity gains ranges widely, from doubling in some languages (Dutch PE\\(_1\\), Italian PE\\(_1\\) and PE\\(_2\\)) to only about 10% (Arabic, Turkish and Ukrainian PE\\(_2\\)). When only considering the better-performing system (PE\\(_1\\)), post-editing remains clearly beneficial in all languages despite the high variability in \\(\\Delta\\)HT scores. Results are more nuanced for the open-source system (PE\\(_2\\)), with three out of six languages displaying only marginal gains (&lt;15% in Arabic, Turkish and Ukrainian). Despite its overall inferior performance, mBART-50 (PE\\(_2\\)) is the only system that enables a fair comparison across languages (in terms of training data size and architecture, see Section 8.3.4). Interestingly, when focusing on the productivity gains achieved by this system, factors such as language relatedness and morphological complexity become relevant. Specifically, Italian (+95%), Dutch (+61%) and Ukrainian (+14%) are genetically and syntactically related to English, but Ukrainian has a richer morphology (see Table 8.2). On the other hand, Vietnamese (+23%), Turkish (+12%) and Arabic (+10%) all belong to different families. However, Vietnamese is isolating (little to no morphology), while Turkish and Arabic have rich morphological systems (respectively agglutinative and introflexive, the latter of which is especially problematic for subword segmentation, Amrhein and Sennrich (2021)). Other differences, however, are more difficult to explain. For instance, Dutch is closely related to English and has a simpler morphology than Italian, but its productivity gain with mBART-50 is lower (61% vs 95%). This finding is accompanied by an important gap in BLEU and comet scores achieved by mBART-50 on the two languages (22.6 vs 24.4 BLEU and 0.532 vs 0.648 comet for Dutch vs Italian, resp.), which cannot be explained by training data size.\n\n\n\n\n\n\n\nProd \\(\\uparrow\\)\n\\(\\Delta\\)HT \\(\\uparrow\\)\n\n\nHT\nPE\\(_1\\)\nPE\\(_2\\)\nPE\\(_1\\)\nPE\\(_2\\)\n\n\n\n\nAra\n13.1\n21.7\n16.3\n+84%\n+10%\n\n\nNld\n13.6\n28.7\n21.7\n+119%\n+61%\n\n\nIta\n8.8\n18.6\n15.6\n+96%\n+95%\n\n\nTur\n17.9\n25.5\n21.0\n+34%\n+12%\n\n\nUkr\n8.0\n12.3\n9.8\n+71%\n+14%\n\n\nVie\n10.2\n13.0\n11.1\n+32%\n+23%\n\n\n\n\n\nTable 8.5: Median productivity (Prod, # processed source words per minute) and median % post-editing speedup (\\(\\Delta\\)HT) for all analyzed languages and modalities. Arrows denote the direction of improvement.\n\n\n\nIn summary, our findings confirm the overall positive impact of NMT post-editing on translation productivity observed in previous PE studies. However, we note that the magnitude of this impact is highly variable across systems and languages, with inter-subject variability also playing an important role, in line with previous studies (Koponen et al., 2020) (see Section 8.5 for more details). The small size of our language sample does not allow us to draw direct causal links between specific typological properties and post-editing efficiency. That said, we believe these results have important implications for the claimed `universality’ of current state-of-the-art MT and NLP systems, primarily based on the transformer architecture (Vaswani et al., 2017) and BPE-style subword segmentation techniques (Sennrich et al., 2016).\n\n8.4.1.1 Modeling Temporal Effort\nGiven the high variability among translators, segments and translation modalities, we assess the validity of our observations via statistical analysis of temporal effort using a linear mixed-effects regression model (LMER, Lindstrom and Bates (1988)), following Green et al. (2013) and Toral et al. (2018). Linear Mixed Effects models (LMER) are used for regression analyses involving dependent data, such as longitudinal studies with multiple observations per subject. We fit our model on \\(n=7434\\) instances, corresponding to 413 sentences translated by 18 translators, using translation time as the dependent variable, and translation modality, target language, their interaction and length of source segment in characters as fixed predictors:\nedit_time ~ src_len_chr + lang_id * task_type +\n(1|subject_id) +\n(1 | document_id/item_id) +\n(0 + task_type | document_id/item_id)\nWe log-transform the dependent variable, edit time in seconds, given its long right tail. The models are built by adding one element at a time and checking whether such an addition leads to a significantly better model, as indicated by a reduction in AIC (i.e., a decrease of at least 2). Our random effects structure includes random intercepts for different segments (nested with documents) and translators, as well as a random slope for modality over individual segments. We start with an initial model that includes only the two random intercepts (by-translator and by-segment) and proceed by (i) finding significance for nested document/segment random effect; (ii) adding fixed predictors one by one; (iii) adding interactions between fixed predictors; and (iv) adding the random slopes.[^8-8]\nTable 8.6 presents the set of predictors included in the final model, along with an estimate of their impact on edit times and their corresponding significance. We find that both PE modalities significantly reduce translation times (\\(p &lt; 0.001\\)), with PE\\(_1\\) being significantly faster than PE\\(_2\\) (\\(p &lt; 0.001\\)) across all languages. Considering Ukrainian—the language for which HT is slowest—as the reference level, the reduction in time brought by Google is significantly more pronounced for Italian, Dutch (\\(p&lt;0.001\\)), and Turkish (\\(p&lt;0.05\\)). For mBART-50, however, we only observe significantly more pronounced increases in productivity for Italian and Dutch (\\(p&lt;0.001\\)) compared to the reference. We find these results to corroborate the observations of the previous section.\n\n\n\n\n\n\nPredictor\nEstim.\np-value\nSig.\n\n\n\n\n(intercept)\n4.92\n1.12e-11\n***\n\n\nsource length\n0.38\n&lt; 2e-16\n***\n\n\nlang_ara\n-0.49\n0.1209\n\n\n\nlang_ita\n-0.14\n0.6407\n\n\n\nlang_nld\n-0.58\n0.0733\nx\n\n\nlang_tur\n-0.82\n0.0162\n*\n\n\nlang_vie\n-0.24\n0.4254\n\n\n\ntask_pe1\n-0.49\n&lt; 2e-16\n***\n\n\ntask_pe2\n-0.22\n1.77e-07\n***\n\n\nlang_ara:task_pe1\n-0.11\n0.0505\nx\n\n\nlang_ita:task_pe1\n-0.40\n8.97e-12\n***\n\n\nlang_nld:task_pe1\n-0.41\n5.74e-12\n***\n\n\nlang_tur:task_pe1\n-0.14\n0.0194\n*\n\n\nlang_vie:task_pe1\n0.13\n0.0290\n*\n\n\nlang_ara:task_pe2\n0.05\n0.3535\n\n\n\nlang_ita:task_pe2\n-0.39\n3.30e-11\n***\n\n\nlang_nld:task_pe2\n-0.29\n4.46e-07\n***\n\n\nlang_tur:task_pe2\n0.03\n0.5811\n\n\n\nlang_vie:task_pe2\n0.04\n0.5289\n\n\n\n\n\n\nTable 8.6: LMER modeling results using translation time as the dependent variable. The reference levels for predictors lang and task are Ukrainian and Translation from scratch (HT), respectively. Estimate impact on edit time for every predictor is provided in log seconds. Significance: *** = &lt; 0.001, * = &lt; 0.05, x = &lt; 0.1\n\n\n\n\n\n\n8.4.2 Post-Editing Rate\nWe proceed to study the post-editing patterns using the widely adopted Human-targeted Translation Edit Rate (HTER, Snover et al. (2006)), which is computed as the length-normalized sum of word-level substitutions, insertions, deletions, and shift operations performed during post-editing.\nAs shown in Figure 8.3, PE\\(_1\\) required less editing than PE\\(_2\\) for all languages, and a high variability is observed across the two systems and all languages. Because translators were not informed about the presence of two MT systems, we exclude the possibility that these results reflect an over-reliance or distrust towards a specific MT system. For Google Translate, Ukrainian shows the heaviest edit rate, followed by Vietnamese, whereas Arabic, Dutch, Italian and Turkish all show relatively low amounts of edits. Focusing again on mBART-50 for a more fair cross-lingual comparison, Ukrainian is by far the most heavily edited language, followed by a medium-tier group composed of Vietnamese, Arabic and Turkish, and finally by Dutch and Italian as low-edit languages. Results show that several of our observations on linguistic relatedness and morphology type also apply to edit rates, with languages less related to English or having richer morphology requiring more post-edits on average.\n\n\n\n\n\n\n\n\nFigure 8.3: Human-targeted Translation Edit Rate (HTER) for Google Translate and mBART-50 post-editing across available languages.\n\n\n\n\n\n\n\n\n\n\n\nFigure 8.4: Distribution of error-less machine translation sentence outputs (no edits performed during post-editing) for each translator and every language.\n\n\n\n\n\nFigure 8.4 visualizes the large gap in edit rates across languages and subjects by presenting the amount of ‘errorless’ MT sentences that were accepted directly, i.e. without any post-editing. We note again how the NMT system significantly influences the rate of occurrence of such sentences, yet nonetheless shows that Dutch and Italian generally present more error-free sentences than Ukrainian and Vietnamese. In particular, for Google Translate outputs, the average rate of error-free sentences is roughly 25% for the former target languages, while for the latter, it accounts for only 3% of total translations. Surprisingly, the English-Turkish pair also fares well, despite the low relatedness between the source and target languages. We note that post-editing effort appears to correlate poorly with the automatic MT quality metrics reported in Table 8.3 (e.g., see the high scores of Vietnamese and the low scores of Dutch PE\\(_1\\)), highlighting a difficulty in predicting the benefits of MT post-editing over HT for new language pairs.\nWhile HTER is a standard metric adopted in both academic and industrial settings, we also evaluated its character-level variant, CharacTER (Wang et al., 2016), to assess whether it could better account for the editing process of morphologically rich languages. Figure 8.5 presents the CharacTER results. When comparing this plot to the HTER one (Figure 8.3), we notice that CharacTER preserves the overall trends but slightly improves the edit rate for Arabic and Turkish compared to other languages. Nevertheless, we find that HTER correlates slightly better with productivity scores across all tested languages, both at the sentence and document levels.\n\n\n\n\n\n\nFigure 8.5: Character-level Human-targeted Translation Edit Rate (CharacTER) for Google Translate and mBART-50 post-editing across available languages.\n\n\n\n\n\n8.4.3 Perception of Productivity Gain\nWe conclude our analysis by examining the post-task questionnaires, in which participants expressed their perceptions of MT quality and translation speed across HT and PE modalities (HT\\(_s\\), PE\\(_s\\))9 using a 1-7 Likert scale (where 1 is the slowest and 7 is the fastest). We use these to compute the Perceived Productivity Gain (PPG) as \\(\\textrm{PPG} = \\textrm{PE}_s - \\textrm{HT}_s\\) and visualize it in Figure 8.6. We observe that Italian and Dutch, the only target languages with marked productivity gains (\\(\\Delta\\)HT) regardless of the PE system in Table 8.5, are also the only ones having consistently high (\\(\\geq 2\\)) PPG scores across all subjects. Moreover, we remark how PPG for target languages with a wide gap in \\(\\Delta\\)HT scores between high-PE\\(_1\\) and low-PE\\(_2\\) (Arabic, Ukrainian) are hardly distinguishable from those of languages in which \\(\\Delta\\)HT is low for both PE systems (Turkish, Vietnamese). Notably, 4 out of 18 subjects attribute negative PPGs to the PE modality, despite productivity gains being reported across all subjects and languages. These results suggest that worst-case usage scenarios may play an important role in driving PPG, i.e. that subjects’ perception of quality is shaped mainly by particularly challenging or unsatisfying interactions with the NMT system, rather than the average case. Finally, from the post-task questionnaire, PPG scores exhibit a strong positive correlation with the perception of MT adequacy (\\(\\rho\\)=0.66), fluency (\\(\\rho\\)=0.46) and overall quality (\\(\\rho\\)=0.69), and more generally with a higher enjoyability of PE (\\(\\rho\\)=0.60), while being inversely correlated with the perception of problematic mistranslations (\\(\\rho\\)=-0.60).\n\n\n\n\n\n\nFigure 8.6: Perceived productivity gains (PPG) between the HT and PE translation modalities, assessed for all subjects after task completion.",
    "crumbs": [
      "Interpretability in Human Translation Workflows",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Machine Translation Post-editing for Typologically Diverse Languages</span>"
    ]
  },
  {
    "objectID": "chapters/chap-8-divemt.html#sec-limitations",
    "href": "chapters/chap-8-divemt.html#sec-limitations",
    "title": "8  Machine Translation Post-editing for Typologically Diverse Languages",
    "section": "8.5 Limitations",
    "text": "8.5 Limitations\nThe subjective component introduced by the presence of multiple translators is an important confounding factor in our setup, particularly given the relatively small number of subjects for each language. In our study, we aimed to strike a balance between thorough control of other noise components and faithful reproduction of a realistic translation scenario. However, we recognize that the combination of the limited document context provided by FLORES-101, the variety of topics covered in the texts, and the experimental nature of the PET platform constitutes an atypical setting that may have impacted the translators’ natural productivity. Moreover, variability in the content of mBART-50 fine-tuning data, despite their comparable sizes, may have played a role in the observed variability in automatic MT evaluation and PE gains across languages.",
    "crumbs": [
      "Interpretability in Human Translation Workflows",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Machine Translation Post-editing for Typologically Diverse Languages</span>"
    ]
  },
  {
    "objectID": "chapters/chap-8-divemt.html#conclusions",
    "href": "chapters/chap-8-divemt.html#conclusions",
    "title": "8  Machine Translation Post-editing for Typologically Diverse Languages",
    "section": "8.6 Conclusions",
    "text": "8.6 Conclusions\nWe introduced DivEMT, the outcome of a post-editing study that spanned two state-of-the-art NMT systems, involved 18 professional translators, and employed six typologically diverse target languages under a unified setup. We leveraged DivEMT’s behavioral data to perform a controlled cross-language analysis of NMT post-editing effort along its temporal and editing effort dimensions. The analysis reveals that NMT drives significant improvements in productivity across all evaluated languages; however, the magnitude of these improvements depends heavily on the language and the underlying NMT system. In this setting, productivity measurements across modalities were found to be generally consistent with the recorded editing patterns. Our results indicate that translators working on language pairs with significant post-editing productivity gains, on average, perform fewer edits and accept more machine-generated translations without any editing. We have also observed a disconnect between post-editing productivity gains and MT quality metrics collected for the same NMT systems. Finally, low source-language relatedness and target morphological complexity seem to hinder productivity when NMT is adopted, even in settings where system architecture and training data are controlled for.\nIn our qualitative analysis, translators’ perception of post-editing usefulness was found to be strongly shaped by problematic mistranslations. Languages showing large productivity gains for both NMT systems were the only ones associated with a positive perception of PE-mediated gains, as opposed to mixed or negative opinions for other translation directions.\nOverall, our findings reveal significant variation in post-editing effectiveness across languages and systems, highlighting the need for fine-grained quality assessment tools. In the next chapter, we build upon these insights by conducting a second study with professional post-editors, assessing the impact of word-level error detection methods—including unsupervised approaches that leverage model internals—on the quality and productivity of human post-editing.\n\n\n\n\nArafat Ahsan, Vandan Mujadia, and Dipti Misra Sharma. 2021. Assessing post-editing effort in the English-Hindi direction. In Sivaji Bandyopadhyay, Sobha Lalitha Devi, and Pushpak Bhattacharyya, editors, Proceedings of the 18th international conference on natural language processing (ICON), pages 44–53, National Institute of Technology Silchar, Silchar, India. NLP Association of India (NLPAI).\n\n\nChantal Amrhein and Rico Sennrich. 2021. How suitable are subword segmentation strategies for translating non-concatenative morphology? In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Findings of the association for computational linguistics: EMNLP 2021, pages 689–705, Punta Cana, Dominican Republic. Association for Computational Linguistics.\n\n\nWilker Aziz, Sheila Castilho, and Lucia Specia. 2012. PET: A tool for post-editing and assessing machine translation. In Nicoletta Calzolari, Khalid Choukri, Thierry Declerck, Mehmet Uğur Doğan, Bente Maegaard, Joseph Mariani, Asuncion Moreno, Jan Odijk, and Stelios Piperidis, editors, Proceedings of the eighth international conference on language resources and evaluation (LREC‘12), pages 3982–3987, Istanbul, Turkey. European Language Resources Association (ELRA).\n\n\nLoic Barrault, Ondrej Bojar, Fethi Bougares, Rajen Chatterjee, Marta R. Costa-jussa, Christian Federmann, Mark Fishel, Alexander Fraser, Markus Freitag, Yvette Graham, Roman Grundkiewicz, Paco Guzman, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes, Philipp Koehn, Tom Kocmi, Andre Martins, Makoto Morishita, et al., editors. 2021. Proceedings of the sixth conference on machine translation. Association for Computational Linguistics, Online.\n\n\nYonatan Belinkov, Nadir Durrani, Fahim Dalvi, Hassan Sajjad, and James Glass. 2017. What do neural machine translation models learn about morphology? In Regina Barzilay and Min-Yen Kan, editors, Proceedings of the 55th annual meeting of the association for computational linguistics (volume 1: Long papers), pages 861–872, Vancouver, Canada. Association for Computational Linguistics.\n\n\nAlexandra Birch, Miles Osborne, and Philipp Koehn. 2008. Predicting success in machine translation. In Mirella Lapata and Hwee Tou Ng, editors, Proceedings of the 2008 conference on empirical methods in natural language processing, pages 745–754, Honolulu, Hawaii. Association for Computational Linguistics.\n\n\nArianna Bisazza, Ahmet Üstün, and Stephan Sportel. 2021. On the difficulty of translating free-order case-marking languages. Transactions of the Association for Computational Linguistics, 9:1233–1248.\n\n\nOndřej Bojar, Christian Buck, Chris Callison-Burch, Christian Federmann, Barry Haddow, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2013. Findings of the 2013 Workshop on Statistical Machine Translation. In Ondrej Bojar, Christian Buck, Chris Callison-Burch, Barry Haddow, Philipp Koehn, Christof Monz, Matt Post, Herve Saint-Amand, Radu Soricut, and Lucia Specia, editors, Proceedings of the eighth workshop on statistical machine translation, pages 1–44, Sofia, Bulgaria. Association for Computational Linguistics.\n\n\nEmanuele Bugliarello, Sabrina J. Mielke, Antonios Anastasopoulos, Ryan Cotterell, and Naoaki Okazaki. 2020. It‘s easier to translate out of English than into it: Measuring neural translation difficulty by cross-mutual information. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault, editors, Proceedings of the 58th annual meeting of the association for computational linguistics, pages 1640–1649, Online. Association for Computational Linguistics.\n\n\nÇağrı Çöltekin and Taraka Rama. 2023. What do complexity measures measure? Correlating and validating corpus-based measures of morphological complexity. Linguistics Vanguard, 9(s1):27–43.\n\n\nRyan Cotterell, Sabrina J. Mielke, Jason Eisner, and Brian Roark. 2018. Are all languages equally hard to language-model? In Marilyn Walker, Heng Ji, and Amanda Stent, editors, Proceedings of the 2018 conference of the north American chapter of the association for computational linguistics: Human language technologies, volume 2 (short papers), pages 536–541, New Orleans, Louisiana. Association for Computational Linguistics.\n\n\nJoke Daems, Sonia Vandepitte, Robert Hartsuiker, and Lieve Macken. 2017. Translation methods and experience: A comparative analysis of human translation and post-editing with students and professional translators. Meta : journal des traducteurs / Meta: Translators’ Journal, 62(2):245–270.\n\n\nMarina Fomicheva, Shuo Sun, Erick Fonseca, Chrysoula Zerva, Frédéric Blain, Vishrav Chaudhary, Francisco Guzmán, Nina Lopatina, Lucia Specia, and André F. T. Martins. 2022. MLQE-PE: A multilingual quality estimation and post-editing dataset. In Nicoletta Calzolari, Frédéric Béchet, Philippe Blache, Khalid Choukri, Christopher Cieri, Thierry Declerck, Sara Goggi, Hitoshi Isahara, Bente Maegaard, Joseph Mariani, Hélène Mazo, Jan Odijk, and Stelios Piperidis, editors, Proceedings of the thirteenth language resources and evaluation conference, pages 4963–4974, Marseille, France. European Language Resources Association.\n\n\nViveta Gene. 2021. The post-editing workflow: Training challenges for LSPs, post-editors and academia. In Ruslan Mitkov, Vilelmini Sosoni, Julie Christine Giguère, Elena Murgolo, and Elizabeth Deysel, editors, Proceedings of the translation and interpreting technology online conference, pages 187–198, Held Online. INCOMA Ltd.\n\n\nDaniela Gerz, Ivan Vulić, Edoardo Maria Ponti, Roi Reichart, and Anna Korhonen. 2018. On the relation between linguistic typology and (limitations of) multilingual language modeling. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun’ichi Tsujii, editors, Proceedings of the 2018 conference on empirical methods in natural language processing, pages 316–327, Brussels, Belgium. Association for Computational Linguistics.\n\n\nNaman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-Jen Chen, Guillaume Wenzek, Da Ju, Sanjana Krishnan, Marc’Aurelio Ranzato, Francisco Guzmán, and Angela Fan. 2022. The Flores-101 evaluation benchmark for low-resource and multilingual machine translation. Transactions of the Association for Computational Linguistics, 10:522–538.\n\n\nSpence Green, Jeffrey Heer, and Christopher D. Manning. 2013. The efficacy of human post-editing for language translation. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, pages 439–448, New York, NY, USA. Association for Computing Machinery.\n\n\nPhilipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In Proceedings of machine translation summit x: papers, pages 79–86, Phuket, Thailand.\n\n\nMaarit Koponen, Wilker Aziz, Luciana Ramos, and Lucia Specia. 2012. Post-editing time as a measure of cognitive effort. In Workshop on post-editing technology and practice.\n\n\nMaarit Koponen, Umut Sulubacak, Kaisa Vitikainen, and Jörg Tiedemann. 2020. MT for subtitling: User evaluation of post-editing productivity. In André Martins, Helena Moniz, Sara Fumega, Bruno Martins, Fernando Batista, Luisa Coheur, Carla Parra, Isabel Trancoso, Marco Turchi, Arianna Bisazza, Joss Moorkens, Ana Guerberof, Mary Nurminen, Lena Marg, and Mikel L. Forcada, editors, Proceedings of the 22nd annual conference of the european association for machine translation, pages 115–124, Lisboa, Portugal. European Association for Machine Translation.\n\n\nIsabel Lacruz, Michael Denkowski, and Alon Lavie. 2014. Cognitive demand and cognitive effort in post-editing. In Sharon O’Brien, Michel Simard, and Lucia Specia, editors, Proceedings of the 11th conference of the association for machine translation in the americas, pages 73–84, Vancouver, Canada. Association for Machine Translation in the Americas.\n\n\nSamuel Läubli, Chantal Amrhein, Patrick Düggelin, Beatriz Gonzalez, Alena Zwahlen, and Martin Volk. 2019. Post-editing productivity with neural machine translation: An empirical assessment of speed and quality in the banking and finance domain. In Mikel Forcada, Andy Way, Barry Haddow, and Rico Sennrich, editors, Proceedings of machine translation summit XVII: Research track, pages 267–272, Dublin, Ireland. European Association for Machine Translation.\n\n\nDaniel Licht, Cynthia Gao, Janice Lam, Francisco Guzman, Mona Diab, and Philipp Koehn. 2022. Consistent human evaluation of machine translation across language pairs. In Kevin Duh and Francisco Guzmán, editors, Proceedings of the 15th biennial conference of the association for machine translation in the americas (volume 1: Research track), pages 309–321, Orlando, USA. Association for Machine Translation in the Americas.\n\n\nYu-Hsiang Lin, Chian-Yu Chen, Jean Lee, Zirui Li, Yuyan Zhang, Mengzhou Xia, Shruti Rijhwani, Junxian He, Zhisong Zhang, Xuezhe Ma, Antonios Anastasopoulos, Patrick Littell, and Graham Neubig. 2019. Choosing transfer languages for cross-lingual learning. In Anna Korhonen, David Traum, and Lluís Màrquez, editors, Proceedings of the 57th annual meeting of the association for computational linguistics, pages 3125–3135, Florence, Italy. Association for Computational Linguistics.\n\n\nMary J. Lindstrom and Douglas M. Bates. 1988. Newton—raphson and EM algorithms for linear mixed-effects models for repeated-measures data. Journal of the American Statistical Association, 83(404):1014–1022.\n\n\nYinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, and Luke Zettlemoyer. 2020. Multilingual denoising pre-training for neural machine translation. Transactions of the Association for Computational Linguistics, 8:726–742.\n\n\nZihan Liu, Genta Indra Winata, and Pascale Fung. 2021. Continual mixed-language pre-training for extremely low-resource neural machine translation. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, Findings of the association for computational linguistics: ACL-IJCNLP 2021, pages 2706–2718, Online. Association for Computational Linguistics.\n\n\nThomas Mayer and Michael Cysouw. 2014. Creating a massively parallel Bible corpus. In Nicoletta Calzolari, Khalid Choukri, Thierry Declerck, Hrafn Loftsson, Bente Maegaard, Joseph Mariani, Asuncion Moreno, Jan Odijk, and Stelios Piperidis, editors, Proceedings of the ninth international conference on language resources and evaluation (LREC‘14), pages 3158–3163, Reykjavik, Iceland. European Language Resources Association (ELRA).\n\n\nSabrina J. Mielke, Ryan Cotterell, Kyle Gorman, Brian Roark, and Jason Eisner. 2019. What kind of language is hard to language-model? In Anna Korhonen, David Traum, and Lluís Màrquez, editors, Proceedings of the 57th annual meeting of the association for computational linguistics, pages 4975–4989, Florence, Italy. Association for Computational Linguistics.\n\n\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: A method for automatic evaluation of machine translation. In Pierre Isabelle, Eugene Charniak, and Dekang Lin, editors, Proceedings of the 40th annual meeting of the association for computational linguistics, pages 311–318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.\n\n\nMaja Popović. 2015. ChrF: Character n-gram F-score for automatic MT evaluation. In Ondřej Bojar, Rajan Chatterjee, Christian Federmann, Barry Haddow, Chris Hokamp, Matthias Huck, Varvara Logacheva, and Pavel Pecina, editors, Proceedings of the tenth workshop on statistical machine translation, pages 392–395, Lisbon, Portugal. Association for Computational Linguistics.\n\n\nRicardo Rei, Craig Stewart, Ana C Farinha, and Alon Lavie. 2020. COMET: A neural framework for MT evaluation. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, Proceedings of the 2020 conference on empirical methods in natural language processing (EMNLP), pages 2685–2702, Online. Association for Computational Linguistics.\n\n\nGabriele Sarti, Arianna Bisazza, Ana Guerberof-Arenas, and Antonio Toral. 2022. DivEMT: Neural machine translation post-editing effort across typologically diverse languages. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Proceedings of the 2022 conference on empirical methods in natural language processing, pages 7795–7816, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\n\n\nRico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural machine translation of rare words with subword units. In Katrin Erk and Noah A. Smith, editors, Proceedings of the 54th annual meeting of the association for computational linguistics (volume 1: Long papers), pages 1715–1725, Berlin, Germany. Association for Computational Linguistics.\n\n\nMatthew Snover, Bonnie Dorr, Rich Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proceedings of the 7th conference of the association for machine translation in the americas: Technical papers, pages 223–231, Cambridge, Massachusetts, USA. Association for Machine Translation in the Americas.\n\n\nMaria Stasimioti and Vilelmini Sosoni. 2020. Translation vs post-editing of NMT output: Insights from the English-Greek language pair. In John E. Ortega, Marcello Federico, Constantin Orasan, and Maja Popovic, editors, Proceedings of 1st workshop on post-editing in modern-day translation, pages 109–124, Virtual. Association for Machine Translation in the Americas.\n\n\nYuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Naman Goyal, Vishrav Chaudhary, Jiatao Gu, and Angela Fan. 2021. Multilingual translation from denoising pre-training. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, Findings of the association for computational linguistics: ACL-IJCNLP 2021, pages 3450–3466, Online. Association for Computational Linguistics.\n\n\nAntonio Toral, Martijn Wieling, and Andy Way. 2018. Post-editing effort of a novel with statistical and neural machine translation. Frontiers in Digital Humanities, 5:1–11.\n\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in neural information processing systems, volume 30. Curran Associates, Inc.\n\n\nWeiyue Wang, Jan-Thorsten Peter, Hendrik Rosendahl, and Hermann Ney. 2016. CharacTer: Translation edit rate on character level. In Ondřej Bojar, Christian Buck, Rajen Chatterjee, Christian Federmann, Liane Guillou, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes, Aurélie Névéol, Mariana Neves, Pavel Pecina, Martin Popel, Philipp Koehn, Christof Monz, Matteo Negri, Matt Post, Lucia Specia, Karin Verspoor, Jörg Tiedemann, et al., editors, Proceedings of the first conference on machine translation: Volume 2, shared task papers, pages 505–510, Berlin, Germany. Association for Computational Linguistics.\n\n\nAris Xanthos, Sabine Laaha, Steven Gillis, Ursula Stephany, Ayhan Aksu-Koç, Anastasia Christofidou, Natalia Gagarina, Gordana Hrzica, F. Nihan Ketrez, Marianne Kilani-Schoch, Katharina Korecky-Kröll, Melita Kovačević, Klaus Laalo, Marijan Palmović, Barbara Pfeiler, Maria D. Voeikova, and Wolfgang U. Dressler. 2011. On the role of morphological richness in the early development of noun and verb inflection. First Language, 31(4):461–479.\n\n\nVilém Zouhar, Martin Popel, Ondřej Bojar, and Aleš Tamchyna. 2021. Neural machine translation quality and post-editing performance. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Proceedings of the 2021 conference on empirical methods in natural language processing, pages 10204–10214, Online; Punta Cana, Dominican Republic. Association for Computational Linguistics.",
    "crumbs": [
      "Interpretability in Human Translation Workflows",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Machine Translation Post-editing for Typologically Diverse Languages</span>"
    ]
  },
  {
    "objectID": "chapters/chap-8-divemt.html#footnotes",
    "href": "chapters/chap-8-divemt.html#footnotes",
    "title": "8  Machine Translation Post-editing for Typologically Diverse Languages",
    "section": "",
    "text": "GroNLP/divemt↩︎\nWarm-up data are excluded from the analysis of Section 8.4.↩︎\nAdditional subjects’ details are available in Section C.1.1.↩︎\nA summary of our translation guidelines is provided in Section C.1.2.↩︎\nWe use a balanced sample of articles sourced from WikiNews, WikiVoyage and WikiBooks.↩︎\nEvaluation performed in October 2021.↩︎\nmbart-large-50-one-to-many↩︎\nSee Section C.1.4 for automatic MT quality results by five different models over a larger set of 10 target languages.↩︎\nWe reemphasize that subjects were unaware of the presence of two distinct MT systems.↩︎",
    "crumbs": [
      "Interpretability in Human Translation Workflows",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Machine Translation Post-editing for Typologically Diverse Languages</span>"
    ]
  },
  {
    "objectID": "chapters/chap-9-qe4pe.html",
    "href": "chapters/chap-9-qe4pe.html",
    "title": "9  Word-level Quality Estimation for Machine Translation Post-editing",
    "section": "",
    "text": "9.1 Introduction\nRecent years saw a steady increase in the quality of machine translation systems and their widespread adoption in professional translation workflows (Kocmi et al., 2024a). Still, human post-editing of MT outputs remains a fundamental step to ensure high-quality translations, particularly for challenging textual domains requiring native fluency and specialized terminology (Liu et al., 2024). Quality estimation (QE) techniques were introduced to reduce post-editing effort by automatically identifying problematic MT outputs without the need for human-written reference translations and were quickly integrated into industry platforms (Tamchyna, 2021).\nSegment-level QE models correlate well with human perception of quality (Freitag et al., 2024) and exceed the performance of reference-based metrics in specific settings (Rei et al., 2021; Amrhein et al., 2022; Amrhein et al., 2023). On the other hand, word-level QE methods for identifying error spans requiring revision have received less attention in the past due to their modest agreement with human annotations, despite their promise for more granular and interpretable quality assessment in line with modern MT practices (Zerva et al., 2024). In particular, while the accuracy of these approaches is regularly assessed in evaluation campaigns, research has rarely focused on assessing the impact of such techniques in realistic post-editing workflows, with notable exceptions suggesting limited benefits (Shenoy et al., 2021; Eo et al., 2022). This hinders current QE evaluation practices: by foregoing experimental evaluation with human editors, it is implicitly assumed that word-level QE will become helpful once sufficient accuracy is achieved, without accounting for the additional challenges associated with successfully integrating these methods into post-editing workflows.\nIn this chapter, we present a study, which we dub QE4PE (Quality Estimation for Post Editing), addressing this gap through a large-scale study with 42 professional translators for the English\\(\\rightarrow\\)Italian and English\\(\\rightarrow\\)Dutch directions to measure the impact of word-level QE on editing quality, productivity and usability. We aim for a realistic and reproducible setup, employing the high-quality open-source NLLB 3.3B MT model (NLLB Team et al., 2024) to translate challenging documents from biomedical and social media domains. We then conduct a controlled evaluation of post-editing with error spans in four highlight modalities, i.e. using highlights derived from four word-level QE methods: a supervised state-of-the-art QE model trained on human error annotations (xcomet, Guerreiro et al., 2024), an unsupervised method leveraging the uncertainty of the MT model during generation, oracle error spans obtained from the consensus of previous human post-editors, and a no highlight baseline. The human post-editing is performed using GroTE, a simple online interface we built to support the real-time logging of granular editing data, enabling a quantitative assessment of editing effort and productivity across highlight modalities. We also survey professionals using an online questionnaire to collect qualitative feedback about the usability and quality of the MT model, as well as the interface and error span highlights. Finally, a subset of the original MT outputs and their post-edited variants is annotated following the MQM and ESA protocols (Lommel et al., 2013; Kocmi et al., 2024b) to verify quality improvements after post-editing. See Figure 9.1 for an overview of the study. Most similar to our study, Shenoy et al. (2021) investigated the effect of synthetic word-level QE highlights for English\\(\\rightarrow\\)German post-editing on Wikipedia data, concluding that word-level QE accuracy was at the time still insufficient to produce tangible productivity benefits in human editing workflows. We expand the scope of such evaluation by including two translation directions, two challenging real-world text domains and state-of-the-art MT and QE systems and methods. In summary, our work represents a step towards evaluating translation technologies and methods that leverage model internals, centered on users’ experience (Guerberof-Arenas and Moorkens, 2023; Savoldi et al., 2025).\nWe release all data, code and the GroTE editing interface to foster future studies on the usability of error span highlighting techniques for other word-level QE methods and translation directions.1",
    "crumbs": [
      "Interpretability in Human Translation Workflows",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Word-level Quality Estimation for Machine Translation Post-editing</span>"
    ]
  },
  {
    "objectID": "chapters/chap-9-qe4pe.html#sec-qe4pe-intro",
    "href": "chapters/chap-9-qe4pe.html#sec-qe4pe-intro",
    "title": "9  Word-level Quality Estimation for Machine Translation Post-editing",
    "section": "",
    "text": "Figure 9.1: A summary of the QE4PE study. Documents are translated by a neural MT model and reviewed by professional editors across two translation directions and four highlight modalities. Editing effort, productivity and usability across modalities are estimated from editing logs and questionnaires. Finally, the quality of MT and edited outputs is assessed with MQM/ESA human annotations and automatic metrics.",
    "crumbs": [
      "Interpretability in Human Translation Workflows",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Word-level Quality Estimation for Machine Translation Post-editing</span>"
    ]
  },
  {
    "objectID": "chapters/chap-9-qe4pe.html#sec-qe4pe-experiment-setup",
    "href": "chapters/chap-9-qe4pe.html#sec-qe4pe-experiment-setup",
    "title": "9  Word-level Quality Estimation for Machine Translation Post-editing",
    "section": "9.2 Experimental Setup",
    "text": "9.2 Experimental Setup\n\n9.2.1 Structure of the Study\nOur study is organized in five stages:\n1) Oracle Post-editing As a preliminary step, segments later used in the main assessment are post-edited by three professionals per direction using their preferred interface without logging. This allows us to obtain post-edits and produce oracle word-level spans based on the editing consensus of multiple human professionals. Translators involved in this stage are not involved further in the study.\n2) Pretask (Pre) The pretask allows the core translators (12 per language direction, see Section 9.2.4) to familiarize themselves with the GroTE interface and text highlights. Before starting, all translators complete a questionnaire to provide demographic and professional information about their profile (Table 9.9). In the pretask, all translators work in an identical setup, post-editing a small set of documents similar to those of the main task with Oracle highlights. We assign core translators into three groups based on their speed, as determined by their editing logs (4 translators per group for faster, average and slower groups in each direction). Individuals from each group are then randomly assigned to each highlight modality to ensure an equal representation of editing speeds, resulting in 1 faster, 1 average, and 1 slower translator for each highlight modality. This procedure is repeated independently for both translation directions.\n3) Main Task (Main) This task, conducted in the two weeks following the pretask, covers the majority of the collected data and is the main object of study for the analyses of Section 9.3. In the main task, 24 core translators work on the same texts using the GroTE interface, with three translators per modality in each translation direction, as shown in Figure 9.1. After the main task, translators complete a questionnaire on the quality and usability of the MT outputs, the interface and, where applicable, word highlights.2\n4) Post-Task (Post) After Main, the 12 core translators per direction are asked to post-edit an additional small set of related documents with GroTE, but this time working all with the No Highlight modality. This step lets us obtain baseline editing patterns for each translator, allowing us to estimate individual speed and editing differences across highlight modalities without the confounding effect of interface proficiency accounted for in the Pre stage.\n5) Quality Assessment (QA) Finally, a subset consisting of 148 main task segments is randomly selected for manual annotation by six new translators per direction (see Section 9.2.4). For each segment, the original MT output and all its post-edited versions are annotated with MQM error spans, including minor/major error severity and a subset of MQM error categories, including e.g., mistranslations, omissions, and stylistic errors (Lommel et al., 2013). Table 9.1 presents our annotation guidelines.3 Moreover, the annotator proposes corrections for each error span, ultimately providing a 0-100 quality score, which matches the common DA scoring adopted in multiple WMT campaigns. We adopt this scoring system, which closely adheres to the ESA evaluation protocol (Kocmi et al., 2024b), following recent results showing its effectiveness and efficiency for ranking MT systems.\nIn summary, for each translation direction, we collect three full sets of oracle post-edits, 12 full sets of edits with behavioral logs for pre, main and post task data, and 13 subsets of main task data (12 post-edits, plus the original MT output) annotated with MQM error spans, corrections and segment-level ESA ratings. Moreover, we also collect 12 pre- and post-task questionnaire responses from core set translators to obtain a qualitative view of the editing process.\n\n\n\n\n\n\nError category\nSubcategory\nDescription\n\n\n\n\nAccuracyIncorrect meaning has been transferred to the source text.\nAddition\nTranslation includes the information that is not present in the source and it changes or distorts the original message.\n\n\nOmission\nTranslation is missing the information that is present in the source, which is important to convey the message.\n\n\nMistranslation\nTranslation does not accurately represent the source content meaning.\n\n\nInconsistency\nThere are internal inconsistencies in the translation (for example, using different verb forms in the bullet list or in CTAs, calling the same UI element differently, terminology used inconsistently etc).\n\n\nUntranslated\nContent that should have been translated has been left untranslated.\n\n\nLinguisticOfficial linguistic reference sources such as grammar books.\nPunctuation\nPunctuation is used incorrectly (for the locale or style), including missing or extra white spaces and the incorrect use of space (non-breaking space). Violation of typographic conventions of the locale.\n\n\nSpelling\nIssues related to spelling of words, including typos, wrong word hyphenation, word breaks and capitalization.\n\n\nGrammar\nIssues related to the grammar or syntax of the text, other than spelling.\n\n\nStyleNot suitable/native; too literal or awkward.\nInconsistent Style\nStyle is inconsistent within a text.\n\n\nReadability\nTranslation does not read well (due to heavy sentence structure, frequent repetitions, unidiomatic).\n\n\nWrong Register\nInappropriate style for the specific subject field, the level of formality, and the mode of discourse (e.g., written text versus transcribed speech).\n\n\n\n\n\n\n\n\n\nSeverity\nDescription\n\n\n\n\nMajor\nThe Severity Level of an error that seriously affects the understandability, reliability, or usability of the content for its intended purpose or hinders the proper use of the product or service due to a significant loss or change in meaning or because the error appears in a highly visible or important part of the content.\n\n\nMinor\nThe Severity Level of an error that does not seriously impede the usability, understandability, or reliability of the content for its intended purpose, but has a limited impact on, for example, accuracy, stylistic quality, consistency, fluency, clarity, or general appeal of the content.\n\n\nNeutral\nThe Severity Level of an error that differs from a quality evaluator's preferential translation or that is flagged for the translator's attention but is an acceptable translation.\n\n\n\n\n\n\n\n\nTable 9.1: Annotation instructions for our MQM-inspired error taxonomy.\n\n\n\n\n\n9.2.2 Highlight Modalities\nWe conduct our study on four highlight modalities across two severity levels (minor and major errors). Using multiple severity levels aligns with current MT evaluation practices (Freitag et al., 2021; Freitag et al., 2024), as well as previous results indicating that users tend to prefer more granular and informative word-level highlights (Shenoy et al., 2021; Vasconcelos et al., 2025). The highlight modalities we employ are:\nNo Highlight The text is presented as-is, without any highlighted spans. This setting serves as a baseline to estimate the default post-editing quality and productivity using our interface.\nOracle Following the Oracle Post-editing phase, we produce oracle error spans from the editing consensus of human post-editors. We label text spans that were edited by two of the three translators as minor, and those edited by all three translators as major, following the intuition that more critical errors are more likely to be identified by several annotators, while minor changes will show more variance across subjects. This modality serves as a best-case scenario, providing an upper bound for future improvements in word-level QE quality.\nSupervised In this setting, word-level error spans are obtained using xcomet-xxl (Guerreiro et al., 2024), which is a multilingual transformer encoder (Goyal et al., 2021) further trained for joint word- and sentence-level QE prediction. We select xcomet-xxl in light of its broad adoption, open accessibility and state-of-the-art performance in QE across several translation directions (Zerva et al., 2024). For the severity levels, we use the labels predicted by the model, mapping critical labels to the major level.\nUnsupervised In this modality, we exploit the access to the MT model producing the original translations to obtain uncertainty-based highlights. As a preliminary evaluation to select a capable unsupervised word-level QE method, we evaluate two unsupervised QE methods employing token log-probabilities assigned by MT model to predict human post-edits: raw negative log-probabilities (Logprobs), corresponding to the surprisal assigned by the MT model to every generated token, and their variance for 10 steps of Monte Carlo Dropout (Logprobs\\(_\\text{mcd var}\\), Gal and Ghahramani, 2016). We employ surprisal-based metrics, following previous work that shows their effectiveness in predicting translation errors (Fomicheva and Specia, 2019) and human editing time (Lim et al., 2024). We collect scores for the English\\(\\rightarrow\\)Italian and English\\(\\rightarrow\\)Dutch directions of QE4PE Oracle post-edits and our DivEMT dataset (Sarti et al., 2022) to identify the best-performing method, using metric scores extracted from the original models used for translation to predict human post-edits. We use average precision (AP) as a threshold-agnostic performance metric for the tested continuous methods. Oracle highlights obtained from the consensus of three annotators in the first stage of the study are used as reference for QE4PE, while a single set of post-edits is available for DivEMT. The xcomet-xxl model used for Supervised highlights, and the average agreement of individual Oracle editors with the consensus label are also included for comparison.\n\n\n\n\n\n\nMethod\nDivEMT\nQE4PE\n\n\nen→it\nen→nl\nen→it\nen→nl\n\n\nAP\nAU\nAP\nAU\nAP\nAU\nAP\nAU\n\n\n\n\nLogprobs\n0.18\n0.18\n0.19\n0.19\n0.10\n0.09\n0.09\n0.09\n\n\nLogprobs \\(_\\text{mcd var}\\) (Unsup.)\n0.41\n0.41\n0.42\n0.42\n0.23\n0.23\n0.31\n0.31\n\n\nxcomet-xxl (Sup.)\n\n\n\n\n0.16\n0.23\n0.19\n0.28\n\n\nAvg. Oracle single translator\n-\n-\n-\n-\n0.53\n0.73\n0.55\n0.75\n\n\n\n\n\nTable 9.2: Average Precision (AP) and Area Under the Precision-Recall Curve (AU) between metrics and error spans derived from human post-editing. We use mBART 1-to-50 and NLLB 3B respectively for DivEMT and QE4PE. For DivEMT, a single post-editor is available for computing the agreement, while for QE4PE we use consensus-based Oracle highlights. For QE4PE, we report the average agreement between individual oracle post-editors and their consensus as an agreement upper bound.\n\n\n\nResults in Table 9.2 show a strong performance for the Logprobs\\(_\\text{mcd var}\\) method, even surpassing the accuracy of the supervised xcomet model across both datasets.4 Hence, we select it for the Unsupervised highlight modality, setting value thresholds for minor/major errors to match the respective highlighted word proportions in the Supervised modality to ensure a fair comparison of their effectiveness in the post-editing task.\n\n\n9.2.3 Data and MT model\nMT Model On the one hand, the MT model must achieve high translation quality in the selected languages to ensure our experimental setup applies to state-of-the-art proprietary systems. Still, the MT model should be open-source and have a manageable size to ensure reproducible findings and enable the computation of uncertainty for the unsupervised setting. All considered, we use NLLB 3.3B (NLLB Team et al., 2024), a widely used MT model achieving industry-level performances across 200 languages (Moslem et al., 2023).\nData selection We begin by selecting two translation directions, English\\(\\rightarrow\\)Italian and English\\(\\rightarrow\\)Dutch, according to the availability of professional translators from our industrial partners. We intentionally focus on out-of-English translations as they are generally more challenging for modern MT models (Kocmi et al., 2023). We aim to identify documents that are manageable for professional translators without domain-specific expertise but still prove challenging for our MT model to ensure a sufficient amount of error spans across modalities. Since original references for our selected translation direction were not available, we do not have a direct mean to compare MT quality in the two languages. However, according to our human MQM assessment in Section 9.3.3 (Table 9.7), NLLB produces a comparable amount of errors across Dutch and Italian translations, suggesting similar quality.\n\n\n\n\n\n\nTask\nDomain\n# Docs\n# Seg.\n# Words\n\n\n\n\nPre\nSocial\n4\n23\n539\n\n\nBiomed.\n2\n15\n348\n\n\nMain\nSocial\n30\n160\n3375\n\n\nBiomed.\n21\n165\n3384\n\n\nPost\nSocial\n6\n34\n841\n\n\nBiomed.\n2\n16\n257\n\n\nTotal\n64\n413\n8744\n\n\n\n\n\nTable 9.3: Statistics for QE4PE data.\n\n\n\nWe begin by translating 3,672 multi-segment English documents from the WMT23 General and Biomedical MT shared tasks (Kocmi et al., 2023; Neves et al., 2023) and MT test suites to Dutch and Italian. Our choice for these specialized domains, as opposed to e.g. generic news articles, is driven by the real-world needs of the translation industry for domain-specific post-editing support (Eschbach-Dymanus et al., 2024; Li et al., 2025). Moreover, focusing on domains that are considerably more challenging for MT systems than news, as shown by recent WMT campaigns (Neves et al., 2024), ensures a sufficient amount of MT errors to support a sound comparison of word-level QE methods. Then, xcomet-xxl is used to produce a first set of segment-level QE scores and word-level error spans for all segments. To make the study tractable, we further narrow down the selection of documents according to several heuristics to ensure a realistic editing experience and a balanced occurrence of error spans (details in Section C.2.1). This procedure yields 351 documents, from which we manually select a subset of 64 documents (413 segments, 8,744 source words per post-editor) across two domains:\n\nSocial media posts, including Mastodon posts from the WMT23 General Task (Kocmi et al., 2023) English\\(\\leftrightarrow\\)German evaluation and Reddit comments from the Robustness Challenge Set for Machine Translation (RoCS-MT, Bawden and Sagot, 2023), displaying atypical language use, such as slang or acronymization.\nBiomedical abstracts extracted from PubMed from the WMT23 Biomedical Translation Task (Neves et al., 2023), including domain-specific terminology.\n\nTable 9.3 present statistics for the Pre, Main and Post editing stages, and Table 9.4 shows an example of highlights and edits. While including multiple domains in the same task can render our post-editing setup less realistic, we deem it essential to test the cross-domain validity of our findings.\n\n\n\n\n\n\nSourceen\nSo why is it that people jump through extra hoops to install Google Maps?\n\n\nNo High.\nQuindi perché le persone devono fare un salto in più per installare Google Maps?\n\n\nOracle\nQuindi perché le persone devono fare un salto in più per installare Google Maps?\n\n\nSup.\nQuindi perché le persone devono fare un salto in più per installare Google Maps?\n\n\nUnsup.\nQuindi perché le persone devono fare un salto in più per installare Google Maps?\n\n\nPE\\(_{\\text{No High.}}\\)\nQuindi perché le persone devono fare un passaggio in più per installare Google Maps?\n\n\nPE\\(_{\\text{Oracle}}\\)\nAllora, perché le persone fanno un passaggio in più per installare Google Maps?\n\n\nPE\\(_{\\text{Sup.}}\\)\nQuindi perché le persone fanno passaggi in più per installare Google Maps?\n\n\nPE\\(_{\\text{Unsup.}}\\)\nQuindi perché le persone fanno i salti mortali per installare Google Maps?\n\n\n\n\n\nTable 9.4: en\\(\\rightarrow\\)it example from the QE4PE dataset, showing minor/major word highlights and a single post-edit per modality, with modified words highlighted.\n\n\n\nCritical Errors Before producing highlights, we manually introduce 13 critical errors into main task segments to assess post-editing thoroughness. Errors are produced, for example, by negating statements, inverting the polarity of adjectives, inverting numbers, and corrupting acronyms. We replicate the errors in both translation directions to enable direct comparison. Most of these errors were correctly identified across all three highlight modalities. Table 9.5 presents some examples of critical errors in the dataset.\n\n\n\n\n\n\nRemove negation (13-6)\n\n\nEnglish\nNo significant differences were found with respect to principal diagnoses […]\n\n\nDutch\nEr werden geen significante verschillen → significante verschillen gevonden met betrekking tot de belangrijkste diagnoses […]\n\n\nTitle literal translation (16-3)\n\n\nEnglish\nThe Last of Us is an easy and canonical example of dad-ification. […]\n\n\nItalian\nThe Last of Us → L’ultimo di noi è un esempio facile e canonico di dad-ification. […]\n\n\nWrong term (48-5)\n\n\nEnglish\n[…], , except for alkaline phosphatase.\n\n\nItalian\n[…], ad eccezione della fosfatasi alcalina → chinasi proteica.\n\n\n\n\n\nTable 9.5: Examples of original \\(\\rightarrow\\) manually inserted critical errors with document-segment ID from Table 9.8.\n\n\n\n\n\n9.2.4 Participants\nFor both directions, the professional translation companies Translated and Global Textware recruited three translators for the Oracle post-editing stage, the core set of 12 translators working on Pre, Main and Post tasks, and six more translators for the QA stage, for a total of 21 translators per direction. All translators were freelancers with native proficiency in their target language and self-assessed proficiency of at least C1 in English. Almost all translators had more than two years of professional translation experience and regularly post-edited MT outputs (details in Table 9.9).\n\n\n9.2.5 Editing Interface\nWe develop a custom interface, which we name Groningen Translation Environment (GroTE, Figure 9.2), to support editing over texts with word-level highlights. While the MMPE tool used by Shenoy et al. (2021) provide extensive multimodal functionalities (Herbig et al., 2020), we aim for a bare-bones setup to avoid confounders in the evaluation. GroTE is a web interface based on Gradio (Abid et al., 2019) and hosted on the Hugging Face Spaces to enable multi-user data collection online. Upon loading a document, source texts and MT outputs for all segments are presented in two columns, following standard industry practices. For modalities with highlights, the interface provides an informative message and supports removing all highlights from a segment via a button, with highlights on words disappearing automatically upon editing, as in (Shenoy et al., 2021). The interface supports real-time logging of user actions, allowing for the analysis of the editing process. In particular, we log the start and end times for each edited document, the accessing and exiting of segment textboxes, highlight removals, and individual keystrokes during editing.\n\n\n\n\n\n\nFigure 9.2: An example of the QE4PE GroTE setup for two segments in an English\\(\\rightarrow\\)Italian document.\n\n\n\nGroTE intentionally lacks standard features such as translation memories, glossaries, and spellchecking to ensure equal familiarity among translators, ultimately controlling for editor proficiency with these tools, as done in previous studies (Shenoy et al., 2021; Sarti et al., 2022). While most translators noted the lack of advanced features in our usability assessment, the majority also found the interface easy to set up, access, and use (Table 9.9).",
    "crumbs": [
      "Interpretability in Human Translation Workflows",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Word-level Quality Estimation for Machine Translation Post-editing</span>"
    ]
  },
  {
    "objectID": "chapters/chap-9-qe4pe.html#sec-qe4pe-analysis",
    "href": "chapters/chap-9-qe4pe.html#sec-qe4pe-analysis",
    "title": "9  Word-level Quality Estimation for Machine Translation Post-editing",
    "section": "9.3 Analysis",
    "text": "9.3 Analysis\n\n9.3.1 Productivity\nWe obtain segment- and document-level edit times and compute editing productivity as the number of processed source characters divided by the sum of all document-level edit times, measured in characters per minute. To account for potential breaks taken by post-editors during editing, we filter out pauses between logged actions longer than 5 minutes. We note that this procedure does not significantly impact the overall ranking of translators, while ensuring a more robust evaluation of editing time.\nDo Highlights Make Post-editors Faster? Figure 9.3 shows translators’ productivity across stages, with every dot corresponding to the productivity of a single individual. We observe that no highlight modality consistently leads to faster editing across all speed groups and that the ordering of Pre-task speed groups is maintained in the following stages despite the different highlight modalities. These results suggest that individual variability in editing speed is more critical than highlight modality in predicting editing speed. However, the fastest English\\(\\rightarrow\\)Dutch translators achieved outstanding productivity, i.e. \\(&gt;2\\) standard deviations above the overall mean (entries with \\(&gt;300\\) char/min, → in Figure 9.3) almost exclusively in No Highlight, and, Oracle modalities, suggesting that lower-quality highlights hinder editing speed.\n\n\n\n\n\n\nFigure 9.3: Productivity of post-editors across QE4PE stages (Pre, Main, Post). The → marks entries with speed \\(&gt;300\\) characters per minute and ❌ marks missing data. Each row corresponds to the same three translators across all stages.\n\n\n\nWe validate these observations by fitting a negative binomial mixed-effect model on segment-level editing times (model details in Table C.10). Excluding random factors such as translator and segment identity from the model results in a significant drop in explained variance, confirming the inherent variability of editing times (\\(R^2 = 0.93 \\rightarrow 0.41\\)). Model coefficients indicate that the output length of MT and the proportion of highlighted characters are the primary factors driving an increase in editing times, possibly reflecting an increase in cognitive effort required to process additional information. We find that highlights have a significant impact on increasing the editing speed of English\\(\\rightarrow\\)Italian translators \\((p &lt; 0.001)\\), but a minimal impact for English\\(\\rightarrow\\)Dutch. Comparing the productivity of the same translator editing with and without highlights (Main vs Post), two-thirds of the translators editing with highlights were up to two times slower on biomedical texts. However, the same proportion of translators was up to three times faster on social media texts across both directions.\nIn summary, we find that highlight modalities are not predictive of edit times on their own, but translation direction and domain play an important role in determining the effect of highlights on editing productivity. We attribute these results to two main factors, which will remain central in the analysis of the following sections: (1) the different propensity of translators to act upon highlighted issues in the two tested directions, and (2) the different nature of errors highlighted across domains.\n\n\n9.3.2 Highlights and Edits\nWe then examine how highlights are distributed across modalities and how they influence the editing choices of human post-editors.\nAgreement Across Modalities First, we quantify how different modalities agree in terms of highlights’ distribution and editing. We find that highlight overlaps across modalities range from 15% to 39% when comparing highlight modalities in a pairwise fashion, with the highest overlap for English\\(\\rightarrow\\)Italian social media and English\\(\\rightarrow\\)Dutch biomedical texts.5 Despite the relatively low agreement on highlights, we find an average agreement of 73% for post-edited characters across modalities. This suggests that edits are generally uniform regardless of highlight modalities and are not necessarily restricted to highlighted spans.6\n\n\n\n\n\n\n\nBase Freq.\nMeasured\nProjected\n\n\n\n\\(P(H)\\)\n\\(P(E)\\)\n\\(P(E|H)\\)\n\\(\\Lambda_E\\)\n\\(P(H|E)\\)\n\\(\\Lambda_H\\)\n\\(\\vec{P}(E|H)\\)\n\\(\\vec{\\Lambda}_E\\)\n\\(\\vec{P}(H|E)\\)\n\\(\\vec{\\Lambda}_H\\)\n\n\n\n\nen→it\n\n\nNo High.\n-\n0.05\n-\n-\n-\n-\n-\n-\n-\n-\n\n\nRandom\n0.16\n-\n-\n-\n-\n-\n0.06\n1.20\n0.18\n1.20\n\n\nOracle\n0.15\n0.12\n0.37\n4.62\n0.45\n4.1\n0.18₋₀.₁₉\n6.00₊₁.₃₈\n0.55₊₀.₁₀\n4.23₊₀.₁₄\n\n\nUnsup.\n0.16\n0.13\n0.25\n2.27\n0.21\n2.2\n0.11₋₀.₁₄\n2.75₊₀.₄₈\n0.37₊₀.₁₆\n2.47₊₀.₂₆\n\n\nSup.\n0.12\n0.16\n0.28\n2.00\n0.22\n2.0\n0.14₋₀.₁₄\n3.50₊₁.₅₀\n0.35₊₀.₁₃\n3.18₊₁.₁₈\n\n\nen→nl\n\n\nNo High.\n-\n0.14\n-\n-\n-\n-\n-\n-\n-\n-\n\n\nRandom\n0.17\n-\n-\n-\n-\n-\n0.16\n1.14\n0.19\n1.19\n\n\nOracle\n0.20\n0.10\n0.26\n4.33\n0.53\n3.12\n0.28₊₀.₀₂\n2.55₋₁.₇₈\n0.40₋₀.₁₃\n2.35₋₀.₇₇\n\n\nUnsup.\n0.20\n0.11\n0.20\n2.50\n0.36\n2.00\n0.22₊₀.₀₂\n1.83₋₀.₆₇\n0.31₋₀.₀₅\n1.72₋₀.₂₈\n\n\nSup.\n0.12\n0.09\n0.24\n3.43\n0.33\n3.30\n0.28₊₀.₀₄\n2.33₋₁.₁₀\n0.24₋₀.₀₉\n2.40₋₀.₉₀\n\n\n\n\n\n\n\n\nTable 9.6: Highlighting (\\(H\\)) and editing (\\(E\\)) average statistics across directions and highlight modalities. Measured: actual edits performed in the specified modality. Projected: using modality highlights over No Highlight edits to account for editing biases (Section 9.3.2). Random highlights matching average word frequencies are used as random baseline, and Projected increases\\(_\\uparrow\\)/decreases\\(_\\downarrow\\) compared to Measured counterparts are shown. Significant Oracle gains over all other modalities are underlined (\\(p&lt;0.05\\) with Bonferroni correction).\n\n\n\nDo Highlights Accurately Identify Potential Issues? Table 9.6 (Base Freq.) shows raw highlight and edit frequencies across modalities. We observe different trends across the two language pairs: for English\\(\\rightarrow\\)Italian, post-editors working with highlights edit more than twice as much as those working with No Highlight, regardless of the highlight modality. On the contrary, for English\\(\\rightarrow\\)Dutch they edit 33% less in the same setting. These results suggest a different attitude towards acting upon highlighted potential issues across the two translation directions, with English\\(\\rightarrow\\)Italian translators appearing to be more inclined to perform more edits when highlights are present. We introduce four metrics to quantify highlights-edits overlap:\n\n\\(P(E|H)\\) and \\(P(H|E)\\), reflecting highlights’ precision and recall in predicting edits, respectively.\n\\(\\Lambda_E\\,{\\stackrel{\\text{\\tiny def}}{=}}\\,P(E|H)/P(E|\\neg H)\\) shows how much more likely an edit is to fall within rather than outside highlighted characters.\n\\(\\Lambda_H\\,{\\stackrel{\\text{\\tiny def}}{=}}\\,P(H|E)/P(H|\\neg E)\\) shows how much more likely it is for a highlight to mark edited rather than unmodified spans.\n\nIntuitively, character-level recall \\(P(H|E)\\) should be more indicative of highlight quality compared to precision \\(P(E|H)\\), provided that word-level highlights can be useful even when not minimal. For example, if the fully-highlighted word traduttore is changed to its feminine version traduttrice, \\(P(H|E) = 1\\) (edit correctly and fully predicted) but \\(P(E|H) = 0.3\\) since word stem characters are left unchanged. Table 9.6 (Measured) shows metric values across the three highlight modalities7. As expected, Oracle highlights obtain the best performance in terms of precision and recall, with \\(P(H|E)\\), in particular, being significantly higher than the other two modalities across both directions.\nSurprisingly, we find no significant precision and recall differences between Oracle and Unsupervised highlights, despite the word-level QE training of xcomet used in the former modality. Moreover, they support the potential of unsupervised, model internals-based techniques to complement or substitute more expensive supervised approaches. Still, likelihood ratios \\(\\Lambda_E, \\Lambda_H \\gg  1\\) for all modalities and directions indicate that highlights are 2-4 times more likely to precisely and comprehensively encompass edits than non-highlighted texts. This suggests that even imperfect highlights that do not reach Oracle-level quality might effectively direct editing efforts toward potential issues. We validate these observations by fitting a zero-inflated negative binomial mixed-effects model to predict segment-level edit rates. Results confirm a significantly higher edit rate for English\\(\\rightarrow\\)Italian highlighted modalities and the social media domain with \\(p&lt;0.001\\) (features and significances shown in Appendix Table C.11). We find a significant zero inflation associated with translator identity, suggesting the choice of leaving MT outputs unedited is highly subjective.\nDo Highlights Influence Editing Choices? Since we found in Section 9.3.1 that the proportion of highlighted characters affects the editing rate of translators, we question whether the relatively high \\(P(E|H)\\) and \\(P(H|E)\\) values might be artificially inflated by translators’ eagerness to intervene on highlighted spans. In other words, do highlights identify actual issues, or do they condition translators to edit when they otherwise would not? To answer this, we propose to project highlights from a selected modality—in which highlights were shown during editing—onto the edits performed by the No Highlight translators on the same segments. The resulting difference between measured and projected metrics can then be used as an estimate of the impact of highlight presentation on their resulting accuracy.\nTo further ensure the soundness of our analysis, we use a set of projected Random highlights as a lower bound for highlight performance. To make the comparison fair, Random highlights are created by randomly highlighting words in MT outputs to match the average word-level highlight frequency across all highlighted modalities, given the current domain and translation direction. Table 9.6 (Projected) shows results for the three highlighted modalities. First, all projected metrics remain consistently above the Random baseline, suggesting a higher-than-chance ability to identify errors even for worst-performing highlight modalities. Projected precision scores \\(P(E|H)\\) depend on edit frequency, and hence see a major decrease for English\\(\\rightarrow\\)Italian, where the No Highlight edit rate \\(P(E)\\) is much lower. However, the increase in \\(\\Lambda_E\\) across all English\\(\\rightarrow\\)Italian modalities confirms that, despite the lower edit proportion, highlighted texts remain notably more likely to be edited than non-highlighted ones. Conversely, the lower \\(\\Lambda_E\\), \\(P(H|E)\\) and \\(\\Lambda_H\\) for English\\(\\rightarrow\\)Dutch show that edits become much less skewed towards highlighted spans in this direction when accounting for presentation bias.\nOverall, while the presence of highlights makes English\\(\\rightarrow\\)Italian translators more likely to intervene in MT outputs, their location in the MT output often pinpoints issues that would be edited regardless of the highlighting. English\\(\\rightarrow\\)Dutch translators, on the contrary, intervene at roughly the same rate regardless of highlight presence, but their edits are focused mainly on highlighted spans when they are present. This difference is consistent across all subjects in both directions, despite the identical setup and comparable MT and QE quality across languages. This suggests that cultural factors may play a non-trivial role in determining the usability and influence of QE methods regardless of span accuracy, a phenomenon previously observed in human-AI interaction studies Ge et al. (2024).\n\n\n9.3.3 Quality Assessment\nWe continue our assessment by inspecting the quality of MT and post-edited outputs along three dimensions. First, we use xcomet segment-level QE ratings as an automatic approximation of quality and compare them to human-annotated quality scores collected in the last phase of our study. For efficiency, these are obtained for the 0-100 Direct Assessment scale commonly used in QE evaluation (Specia et al., 2020), but following an initial step of MQM error annotation to condition scoring on found errors, as prescribed by the ESA protocol (Kocmi et al., 2024b). Then, MQM error span annotations are used to analyze the distribution of error categories. Finally, we manually assess critical errors, which were inserted to quantify the effect of highlighting modalities on unambiguous issues.\nDo Highlights Influence Post-Editing Quality? In this stage, we focus particularly on edited quality improvements, i.e. how post-editing the same MT outputs under different highlight conditions influences the resulting quality of translations. We operationalize this assessment using human ratings and automatic metrics to score MT and post-edited translations, using their difference as the effective quality gain after the post-editing stage. Scores for this metric are generally positive—that is, human post-editing improves quality—and are bounded by the maximal achievable quality gain given the initial MT quality. Figure 9.4 shows median improvement values across quality bins defined from the distribution of initial MT quality scores (shown in histograms), in which all post-edited versions of each MT output appear as separate observations. Positive median scores confirm that post-edits generally lead to improvements in quality across all tested settings. However, we observe different trends across the two metrics: across both domains, xcomet greatly underestimates the human-assessed ESA quality improvement, especially for biomedical texts, where it shows negligible improvement regardless of the initial MT quality. These results echo recent findings cautioning users against the poor performance of trained MT metrics for unseen domains and high-quality translations (Agrawal et al., 2024; Zouhar et al., 2024). Focusing on the more reliable ESA scores, we observe large quality improvements from post-editing, as indicated by near-maximal quality gains across most bins and highlight modalities. While No Highlight seems to underperform other modalities in the social media domain, the lack of more notable differences in gains across highlight modalities suggests that highlights’ quality impact might not be evident in terms of segment-level quality, motivating our next steps in the quality analysis.\n\n\n\n\n\n\nFigure 9.4: Median quality improvement for post-edited segments at various initial MT quality levels across domains and highlight modalities. Quality scores are estimated using xcomet segment-level QE (top) and professional ESA annotations (bottom). Histograms show example counts across quality bins for the two metrics. Dotted lines show upper bounds for quality improvements given starting MT quality.\n\n\n\nWe also find no clear relationship between translator speed and improved edited quality, suggesting that higher productivity does not come at a cost for faster translators (Figure 9.5). This finding confirms that neglecting errors is not the cause of the different editing patterns observed in previous sections.\n\n\n\n\n\n\nFigure 9.5: Median ESA quality improvement following post-editing for segments at various initial MT quality levels across translators’ speed groups, showing no clear quality trends across editors’ productivity levels.\n\n\n\nWhich Error Types Do Highlights Identify? Table 9.7 shows a breakdown of MQM annotations for MT and all highlight modalities using the Accuracy, Style and Linguistic macro-categories of MQM errors.8 At this granularity, differences across modalities become visible, with overall error counts showing a clear relation to \\(\\Lambda_E\\) from Table 9.6, i.e. Oracle being remarkably better for English\\(\\rightarrow\\)Italian, with milder and more uniform trends in English\\(\\rightarrow\\)Dutch. At least for English\\(\\rightarrow\\)Italian, these results confirm that an observable quality improvement from editing with highlights is present in the best-case Oracle scenario. By contrast, for English\\(\\rightarrow\\)Dutch, the Unsupervised method is found to outperform even the Oracle setting in reducing the amount of errors, while it fares relatively poorly for English\\(\\rightarrow\\)Italian. We also observe a distinct distribution of Accuracy and Style errors, with the formers being more prevalent in biomedical texts, and the latters appearing more frequently in translated social media posts (Figure C.8). We posit that differences in error types across domains might explain the opposite productivity trends observed in Section 9.3.1. While highlighted accuracy errors might lead to time-consuming terminology verification in biomedical texts, style errors might be corrected more quickly and naturally in the social media domain.\n\n\n\n\n\n\nLanguage\nMQM Category\nMT\nNo Highlight\nOracle\nUnsupervised\nSupervised\n\n\nMaj.\nMin.\nMaj.\nMin.\nMaj.\nMin.\nMaj.\nMin.\nMaj.\nMin.\n\n\n\n\nItalian\nAccuracy - Addition\n0\n1\n0\n0\n0\n0\n0\n0\n1\n1\n\n\nAccuracy - Mistranslation\n21\n22\n10\n12\n4\n8\n24\n17\n17\n17\n\n\nAccuracy - Inconsistency\n2\n4\n1\n3\n2\n2\n1\n3\n0\n2\n\n\nAccuracy - Omission\n2\n0\n0\n0\n0\n1\n4\n1\n1\n2\n\n\nAccuracy - Untranslated\n1\n4\n1\n2\n0\n1\n1\n1\n3\n2\n\n\nStyle - Inconsistent Style\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nStyle - Readability\n17\n25\n5\n30\n0\n12\n4\n34\n1\n29\n\n\nStyle - Wrong Register\n0\n8\n0\n3\n0\n3\n1\n1\n3\n2\n\n\nLinguistic - Grammar\n6\n15\n2\n16\n0\n5\n3\n12\n2\n12\n\n\nLinguistic - Punctuation\n1\n13\n0\n9\n0\n3\n1\n6\n0\n3\n\n\nLinguistic - Spelling\n5\n3\n0\n4\n0\n3\n3\n2\n0\n1\n\n\nTotal\n55\n95\n19\n79\n6\n38\n42\n77\n28\n71\n\n\nDutch\nAccuracy - Addition\n0\n1\n0\n2\n0\n3\n0\n2\n0\n1\n\n\nAccuracy - Mistranslation\n25\n34\n18\n25\n23\n27\n12\n31\n16\n29\n\n\nAccuracy - Inconsistency\n0\n0\n0\n2\n0\n2\n0\n2\n0\n5\n\n\nAccuracy - Omission\n3\n1\n1\n1\n2\n1\n1\n1\n4\n2\n\n\nAccuracy - Untranslated\n4\n4\n1\n1\n1\n4\n1\n3\n0\n2\n\n\nStyle - Inconsistent Style\n2\n0\n0\n5\n1\n7\n0\n2\n0\n9\n\n\nStyle - Readability\n1\n27\n1\n20\n0\n13\n2\n15\n6\n41\n\n\nStyle - Wrong Register\n0\n2\n0\n3\n0\n3\n0\n1\n1\n0\n\n\nLinguistic - Grammar\n3\n19\n2\n14\n3\n23\n2\n6\n3\n12\n\n\nLinguistic - Punctuation\n0\n6\n0\n3\n0\n4\n0\n2\n0\n3\n\n\nLinguistic - Spelling\n1\n1\n1\n1\n2\n1\n0\n1\n0\n0\n\n\nTotal\n39\n95\n24\n77\n32\n88\n18\n66\n30\n104\n\n\n\n\n\nTable 9.7: MQM error counts averaged across \\(n = 3\\) translators per highlight modality for every translation direction. A description of MQM categories is available in Table 9.1.\n\n\n\nDo Highlights Detect Critical Errors? We examine whether the critical errors we inserted were detected by different modalities, finding that while most modalities fare decently with more than 62% of critical errors highlighted, Unsupervised is the only setting for which all errors are correctly highlighted across both directions (Table 9.8). Then, critical errors are manually verified in all outputs, finding that 16-20% more critical errors are edited in highlighted modalities compared to No Highlight. Hence, highlights might lead to narrow but tangible quality improvements that can go undetected in coarse quality assessments, and finer-grained evaluations might be needed to quantify future improvements in word-level QE quality.\n\n\n\n\n\n\n# Doc.-Seg.\nError Type\nHas Highlight\n% Post-edited\n\n\nOracle\nUnsup.\nSup.\nNo High.\nOracle\nUnsup.\nSup.\n\n\n\n\n1-8\nWrong number\nNld Only\nBoth\nBoth\n67\n83\n83\n83\n\n\n13-6\nRemove negation\nIta Only\nBoth\nBoth\n50\n33\n33\n50\n\n\n16-3\nTitle literal translation\nBoth\nBoth\nBoth\n83\n100\n100\n100\n\n\n20-1\nWrong acronym\nNld Only\nBoth\nIta Only\n0\n33\n33\n33\n\n\n20-7\nWrong acronym (1)\nNeither\nBoth\nNeither\n0\n58\n50\n25\n\n\n20-7\nWrong acronym (2)\nNld Only\nBoth\nIta Only\n0\n58\n50\n25\n\n\n22-1\nName literal translation\nBoth\nBoth\nBoth\n50\n50\n83\n67\n\n\n23-4\nAddition\nNld Only\nBoth\nNeither\n100\n100\n83\n50\n\n\n31-2\nWrong acronym\nNld Only\nBoth\nNeither\n17\n33\n17\n33\n\n\n34-7\nNumbers swapped\nNld Only\nBoth\nNld Only\n17\n50\n33\n67\n\n\n37-4\nVerb polarity inverted\nBoth\nBoth\nBoth\n67\n83\n67\n83\n\n\n43-5\nWrong name\nBoth\nBoth\nBoth\n50\n83\n67\n83\n\n\n48-5\nWrong term\nNld Only\nBoth\nNld Only\n67\n50\n83\n83\n\n\n\nTotal (%)\n65\n100\n62\n44\n63\n60\n60\n\n\n\n\n\nTable 9.8: Highlighting and post-editing statistics for manual critical errors (Section 9.2.3). Labels in Has Highlight columns indicate whether the error was highlighted in Both, only one (Ita / Nld) only or Neither directions. Total scores represent the percentage of detected errors (13 errors, 6 editors per highlight modality).\n\n\n\n\n\n9.3.4 Usability\nIn the post-task questionnaire answers (Table 9.9), most translators stated that the MT outputs had an average-to-high quality and that the provided texts were challenging to translate. Highlights were generally found to be reasonably accurate, but they were generally not found helpful to improve either productivity or quality (including Oracle ones). Interestingly, despite the convincing gains for critical errors measured in the last section, most translators stated that highlights did not influence their editing and did not help them identify errors that would have otherwise been missed. Specifically, this suggests that translators may not readily perceive potential quality improvements and may consider them secondary to the extra cognitive load elicited by highlighted spans. When asked to comment about highlights, several translators called them “more of an eye distraction, as they often weren’t actual mistakes” and “not quite accurate enough to rely on them as a suggestion”. Some translators also stated that missed errors led them to “disregarding the highlights to focus on checking each sentence”. Despite their high quality, only one editor working with Oracle highlights found highlights helpful in “making the editing process faster and somehow easier”. Taken together, these comments convincingly suggest a negative perception of the quality and usefulness of highlights, indicating that improvement in QE accuracy may not be sufficient to enhance QE usefulness in editors’ eyes.\n\n\n\nHover bars to see avg. response values.\n\n\n\n\n\n\n\n\n\nQuestion\nItalian\nDutch\n\n\n\n\nMT outputs were generally of high quality.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProvided texts were challenging to translate.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHighlights ...\n\n\n\n\n... were generally accurate in detecting potential issues.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n... were generally useful during editing.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n... improved my editing productivity.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n... improved the quality of my translations.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n... required additional editing effort on my part.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n... influenced my choices regarding edits.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n... helped identify errors I'd have otherwise missed.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 9.9: Post-task questionnaire responses. Bars represent responses ranging from 1–Strongly disagree (no bar) to 5–Strongly agree (full bar), averaged across \\(n=3\\) translators per language for No Highlight, Oracle, Unsupervised, and Supervised. Dotted line mark avg. judgments of 3–Neither agree nor disagree.",
    "crumbs": [
      "Interpretability in Human Translation Workflows",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Word-level Quality Estimation for Machine Translation Post-editing</span>"
    ]
  },
  {
    "objectID": "chapters/chap-9-qe4pe.html#sec-qe4pe-limitations",
    "href": "chapters/chap-9-qe4pe.html#sec-qe4pe-limitations",
    "title": "9  Word-level Quality Estimation for Machine Translation Post-editing",
    "section": "9.4 Limitations",
    "text": "9.4 Limitations\nOur study presents certain limitations that warrant consideration when interpreting its findings and guiding future research.\nFirstly, while we included two domains and translation directions to improve the generalizability of our findings, our results suggest that language and domain play a crucial role in determining the effectiveness of word-level QE for human post-editing. While we observed mild gains from word-level QE on our tested mid-resourced translation directions (English\\(\\rightarrow\\)Italian and English\\(\\rightarrow\\)Dutch), we expect limited, if any, benefit of such approaches in low-resource languages and domains for which MT systems and QE methods are likely to underperform (Sarti et al., 2022; Zouhar et al., 2024). Furthermore, the domains tested in our study (biomedical and social media posts) provided concrete challenges in the form of specialized terminology and idiomatic expressions, respectively, which are known to hinder the quality of MT outputs (Neves et al., 2024; Bawden and Sagot, 2023). While future work should ensure that our findings can be extended to other domains and languages, the limited benefits brought by the tested word-level QE methods in challenging settings suggest a limited usefulness for higher-resource languages and more standard domains, such as news or Wikipedia texts.\nSecondly, we acknowledge that several design choices in our evaluation setup, rather than about the QE methods themselves, may have influenced our results. These include, for instance, the specific procedure for discretizing continuous scores from the Unsupervised method into error spans, and the method of obtaining Oracle highlights via majority voting among post-editors. While we believe these choices are justified within the context of our study, their impact on the outcomes cannot be entirely discounted. Future studies might benefit from a more fine-grained assessment of how such low-level decisions influence the perceived accuracy and usability of word-level QE.\nFinally, subjective factors such as the translators’ inherent propensity to edit, their prior opinions on the role of MT in post-editing, and their individual editing styles inevitably influenced both quantitative and qualitative assessments in this study. We attempted to mitigate these effects by ensuring a uniform and controlled evaluation setup for all 42 professional translators and by employing averaged judgments for translators working on the same highlight modality. However, we acknowledge that post-editor subjectivity might limit the reproducibility of our findings.",
    "crumbs": [
      "Interpretability in Human Translation Workflows",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Word-level Quality Estimation for Machine Translation Post-editing</span>"
    ]
  },
  {
    "objectID": "chapters/chap-9-qe4pe.html#sec-qe4pe-broader-impact",
    "href": "chapters/chap-9-qe4pe.html#sec-qe4pe-broader-impact",
    "title": "9  Word-level Quality Estimation for Machine Translation Post-editing",
    "section": "9.5 Broader Impact and Ethical Considerations",
    "text": "9.5 Broader Impact and Ethical Considerations\nOur study explicitly centers the experience of professional translators, responding to recent calls for user-centered evaluation of translation technologies. By prioritizing translators’ perspectives and productivity, we aim to contribute to methods that complement rather than replace human expertise. Our findings highlight a gap between user perception and measured quality improvements, suggesting that future efforts should focus primarily on improving the usability of these methods in editing interfaces. In particular, new assistive approaches for post-editing should not only strive to increase productivity but also reduce the cognitive burden associated with post-editing work. This insight is crucial for designing more user-centered quality estimation tools that genuinely support human work. Ultimately, our results suggest that subjective norms across different domains and cultures play an important role in determining the effectiveness of proposed methodologies, underscoring the importance of accounting for human factors when designing such evaluations. All participants in this study were professional translators who provided informed consent. The research protocol ensured anonymity and voluntary participation, with translators recruited and remunerated through professional translation providers. The study’s open data release further promotes transparency, enabling other researchers to reproduce and build upon our findings.",
    "crumbs": [
      "Interpretability in Human Translation Workflows",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Word-level Quality Estimation for Machine Translation Post-editing</span>"
    ]
  },
  {
    "objectID": "chapters/chap-9-qe4pe.html#sec-qe4pe-conclusion",
    "href": "chapters/chap-9-qe4pe.html#sec-qe4pe-conclusion",
    "title": "9  Word-level Quality Estimation for Machine Translation Post-editing",
    "section": "9.6 Conclusion",
    "text": "9.6 Conclusion\nOur QE4PE study evaluated the impact of various error-span highlighting modalities, including automatic and human-made ones, on the productivity and quality of human post-editing in a realistic professional setting. Our findings highlight the importance of domain, language and editors’ speed in determining the effect of highlights on productivity and quality, underscoring the need for broad evaluations encompassing diverse settings. The limited gains of human-made highlights over automatic QE and their indistinguishable perception from editors’ assessment indicate that further gains in the accuracy of these techniques might not be the determining factor in improving their integration into post-editing workflows. In particular, future work might explore other directions to further assess and improve the usability of word-level QE highlights, for example, by studying their impact on non-professional translators and language learners, or by combining them with edit suggestions to justify the presence of error spans.\nWhile our QE4PE study included a preliminary assessment of unsupervised methods for word-level QE, the next chapter expands our investigation by conducting a broad evaluation of unsupervised word-level QE methods for post-editing, including a comparison with state-of-the-art supervised approaches. In light of our results showing the user-dependent effectiveness of quality estimation, we also inspect how the presence of multiple human annotations influences the quality assessment of identified error spans.\n\n\n\n\nAbubakar Abid, Ali Abdalla, Ali Abid, Dawood Khan, Abdulrahman Alfozan, and James Y. Zou. 2019. Gradio: Hassle-free sharing and testing of ML models in the wild. ArXiv, abs/1906.02569.\n\n\nSweta Agrawal, António Farinhas, Ricardo Rei, and Andre Martins. 2024. Can automatic metrics assess high-quality translations? In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Proceedings of the 2024 conference on empirical methods in natural language processing, pages 14491–14502, Miami, Florida, USA. Association for Computational Linguistics.\n\n\nChantal Amrhein, Nikita Moghe, and Liane Guillou. 2022. ACES: Translation accuracy challenge sets for evaluating machine translation metrics. In Philipp Koehn, Loïc Barrault, Ondřej Bojar, Fethi Bougares, Rajen Chatterjee, Marta R. Costa-jussà, Christian Federmann, Mark Fishel, Alexander Fraser, Markus Freitag, Yvette Graham, Roman Grundkiewicz, Paco Guzman, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes, Tom Kocmi, André Martins, Makoto Morishita, et al., editors, Proceedings of the seventh conference on machine translation (WMT), pages 479–513, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics.\n\n\nChantal Amrhein, Nikita Moghe, and Liane Guillou. 2023. ACES: Translation accuracy challenge sets at WMT 2023. In Philipp Koehn, Barry Haddow, Tom Kocmi, and Christof Monz, editors, Proceedings of the eighth conference on machine translation, pages 695–712, Singapore. Association for Computational Linguistics.\n\n\nRachel Bawden and Benoît Sagot. 2023. RoCS-MT: Robustness challenge set for machine translation. In Philipp Koehn, Barry Haddow, Tom Kocmi, and Christof Monz, editors, Proceedings of the eighth conference on machine translation, pages 198–216, Singapore. Association for Computational Linguistics.\n\n\nSugyeong Eo, Chanjun Park, Hyeonseok Moon, Jaehyung Seo, and Heuiseok Lim. 2022. Word-level quality estimation for korean-english neural machine translation. IEEE Access, 10:44964–44973.\n\n\nJohannes Eschbach-Dymanus, Frank Essenberger, Bianka Buschbeck, and Miriam Exel. 2024. Exploring the effectiveness of LLM domain adaptation for business IT machine translation. In Carolina Scarton, Charlotte Prescott, Chris Bayliss, Chris Oakley, Joanna Wright, Stuart Wrigley, Xingyi Song, Edward Gow-Smith, Rachel Bawden, Víctor M Sánchez-Cartagena, Patrick Cadwell, Ekaterina Lapshinova-Koltunski, Vera Cabarrão, Konstantinos Chatzitheodorou, Mary Nurminen, Diptesh Kanojia, and Helena Moniz, editors, Proceedings of the 25th annual conference of the european association for machine translation (volume 1), pages 610–622, Sheffield, UK. European Association for Machine Translation (EAMT).\n\n\nMarina Fomicheva and Lucia Specia. 2019. Taking MT evaluation metrics to extremes: Beyond correlation with human judgments. Computational Linguistics, 45(3):515–558.\n\n\nMarkus Freitag, George Foster, David Grangier, Viresh Ratnakar, Qijun Tan, and Wolfgang Macherey. 2021. Experts, errors, and context: A large-scale study of human evaluation for machine translation. Transactions of the Association for Computational Linguistics, 9:1460–1474.\n\n\nMarkus Freitag, Nitika Mathur, Daniel Deutsch, Chi-Kiu Lo, Eleftherios Avramidis, Ricardo Rei, Brian Thompson, Frederic Blain, Tom Kocmi, Jiayi Wang, David Ifeoluwa Adelani, Marianna Buchicchio, Chrysoula Zerva, and Alon Lavie. 2024. Are LLMs breaking MT metrics? Results of the WMT24 metrics shared task. In Barry Haddow, Tom Kocmi, Philipp Koehn, and Christof Monz, editors, Proceedings of the ninth conference on machine translation, pages 47–81, Miami, Florida, USA. Association for Computational Linguistics.\n\n\nYarin Gal and Zoubin Ghahramani. 2016. Dropout as a bayesian approximation: Representing model uncertainty in deep learning. In Maria Florina Balcan and Kilian Q. Weinberger, editors, Proceedings of the 33rd international conference on machine learning, volume 48, pages 1050–1059, New York, NY, USA. Proceedings of Machine Learning Research (PLMR).\n\n\nXiao Ge, Chunchen Xu, Daigo Misaki, Hazel Rose Markus, and Jeanne L Tsai. 2024. How culture shapes what people want from AI. In Proceedings of the 2024 CHI conference on human factors in computing systems, New York, NY, USA. Association for Computing Machinery.\n\n\nNaman Goyal, Jingfei Du, Myle Ott, Giri Anantharaman, and Alexis Conneau. 2021. Larger-scale transformers for multilingual masked language modeling. In Anna Rogers, Iacer Calixto, Ivan Vulić, Naomi Saphra, Nora Kassner, Oana-Maria Camburu, Trapit Bansal, and Vered Shwartz, editors, Proceedings of the 6th workshop on representation learning for NLP (RepL4NLP-2021), pages 29–33, Online. Association for Computational Linguistics.\n\n\nAna Guerberof-Arenas and Joss Moorkens. 2023. Ethics and machine translation: The end user perspective. In Towards responsible machine translation: Ethical and legal considerations in machine translation, pages 113–133. Springer International Publishing, Cham.\n\n\nNuno M. Guerreiro, Ricardo Rei, Daan van Stigt, Luisa Coheur, Pierre Colombo, and André F. T. Martins. 2024. Xcomet: Transparent machine translation evaluation through fine-grained error detection. Transactions of the Association for Computational Linguistics, 12:979–995.\n\n\nNico Herbig, Tim Düwel, Santanu Pal, Kalliopi Meladaki, Mahsa Monshizadeh, Antonio Krüger, and Josef van Genabith. 2020. MMPE: A Multi-Modal Interface for Post-Editing Machine Translation. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault, editors, Proceedings of the 58th annual meeting of the association for computational linguistics, pages 1691–1702, Online. Association for Computational Linguistics.\n\n\nTom Kocmi, Eleftherios Avramidis, Rachel Bawden, Ondřej Bojar, Anton Dvorkovich, Christian Federmann, Mark Fishel, Markus Freitag, Thamme Gowda, Roman Grundkiewicz, Barry Haddow, Marzena Karpinska, Philipp Koehn, Benjamin Marie, Christof Monz, Kenton Murray, Masaaki Nagata, Martin Popel, Maja Popović, et al. 2024a. Findings of the WMT24 general machine translation shared task: The LLM era is here but MT is not solved yet. In Barry Haddow, Tom Kocmi, Philipp Koehn, and Christof Monz, editors, Proceedings of the ninth conference on machine translation, pages 1–46, Miami, Florida, USA. Association for Computational Linguistics.\n\n\nTom Kocmi, Eleftherios Avramidis, Rachel Bawden, Ondřej Bojar, Anton Dvorkovich, Christian Federmann, Mark Fishel, Markus Freitag, Thamme Gowda, Roman Grundkiewicz, Barry Haddow, Philipp Koehn, Benjamin Marie, Christof Monz, Makoto Morishita, Kenton Murray, Masaaki Nagata, Toshiaki Nakazawa, Martin Popel, et al. 2023. Findings of the 2023 conference on machine translation (WMT23): LLMs are here but not quite there yet. In Philipp Koehn, Barry Haddow, Tom Kocmi, and Christof Monz, editors, Proceedings of the eighth conference on machine translation, pages 1–42, Singapore. Association for Computational Linguistics.\n\n\nTom Kocmi, Vilém Zouhar, Eleftherios Avramidis, Roman Grundkiewicz, Marzena Karpinska, Maja Popović, Mrinmaya Sachan, and Mariya Shmatova. 2024b. Error span annotation: A balanced approach for human evaluation of machine translation. In Barry Haddow, Tom Kocmi, Philipp Koehn, and Christof Monz, editors, Proceedings of the ninth conference on machine translation, pages 1440–1453, Miami, Florida, USA. Association for Computational Linguistics.\n\n\nHaijun Li, Tianqi Shi, Zifu Shang, Yuxuan Han, Xueyu Zhao, Hao Wang, Yu Qian, Zhiqiang Qian, Linlong Xu, Minghao Wu, Chenyang Lyu, Longyue Wang, Gongbo Tang, Weihua Luo, Zhao Xu, and Kaifu Zhang. 2025. TransBench: Benchmarking machine translation for industrial-scale applications. Arxiv.\n\n\nZheng Wei Lim, Ekaterina Vylomova, Charles Kemp, and Trevor Cohn. 2024. Predicting human translation difficulty with neural machine translation. Transactions of the Association for Computational Linguistics, 12:1479–1496.\n\n\nZhongtao Liu, Parker Riley, Daniel Deutsch, Alison Lui, Mengmeng Niu, Apurva Shah, and Markus Freitag. 2024. Beyond human-only: Evaluating human-machine collaboration for collecting high-quality translation data. In Barry Haddow, Tom Kocmi, Philipp Koehn, and Christof Monz, editors, Proceedings of the ninth conference on machine translation, pages 1095–1106, Miami, Florida, USA. Association for Computational Linguistics.\n\n\nArle Richard Lommel, Aljoscha Burchardt, and Hans Uszkoreit. 2013. Multidimensional quality metrics: A flexible system for assessing translation quality. In Proceedings of translating and the computer 35, London, UK. Aslib.\n\n\nYasmin Moslem, Rejwanul Haque, John D. Kelleher, and Andy Way. 2023. Adaptive machine translation with large language models. In Mary Nurminen, Judith Brenner, Maarit Koponen, Sirkku Latomaa, Mikhail Mikhailov, Frederike Schierl, Tharindu Ranasinghe, Eva Vanmassenhove, Sergi Alvarez Vidal, Nora Aranberri, Mara Nunziatini, Carla Parra Escartín, Mikel Forcada, Maja Popovic, Carolina Scarton, and Helena Moniz, editors, Proceedings of the 24th annual conference of the european association for machine translation, pages 227–237, Tampere, Finland. European Association for Machine Translation.\n\n\nMariana Neves, Cristian Grozea, Philippe Thomas, Roland Roller, Rachel Bawden, Aurélie Névéol, Steffen Castle, Vanessa Bonato, Giorgio Maria Di Nunzio, Federica Vezzani, Maika Vicente Navarro, Lana Yeganova, and Antonio Jimeno Yepes. 2024. Findings of the WMT 2024 biomedical translation shared task: Test sets on abstract level. In Barry Haddow, Tom Kocmi, Philipp Koehn, and Christof Monz, editors, Proceedings of the ninth conference on machine translation, pages 124–138, Miami, Florida, USA. Association for Computational Linguistics.\n\n\nMariana Neves, Antonio Jimeno Yepes, Aurélie Névéol, Rachel Bawden, Giorgio Maria Di Nunzio, Roland Roller, Philippe Thomas, Federica Vezzani, Maika Vicente Navarro, Lana Yeganova, Dina Wiemann, and Cristian Grozea. 2023. Findings of the WMT 2023 biomedical translation shared task: Evaluation of ChatGPT 3.5 as a comparison system. In Philipp Koehn, Barry Haddow, Tom Kocmi, and Christof Monz, editors, Proceedings of the eighth conference on machine translation, pages 43–54, Singapore. Association for Computational Linguistics.\n\n\nNLLB Team, Marta R. Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula, Loic Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti, et al. 2024. Scaling neural machine translation to 200 languages. Nature, 630(8018):841–846.\n\n\nRicardo Rei, Ana C Farinha, Chrysoula Zerva, Daan van Stigt, Craig Stewart, Pedro Ramos, Taisiya Glushkova, André F. T. Martins, and Alon Lavie. 2021. Are references really needed? Unbabel-IST 2021 submission for the metrics shared task. In Loic Barrault, Ondrej Bojar, Fethi Bougares, Rajen Chatterjee, Marta R. Costa-jussa, Christian Federmann, Mark Fishel, Alexander Fraser, Markus Freitag, Yvette Graham, Roman Grundkiewicz, Paco Guzman, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes, Philipp Koehn, Tom Kocmi, Andre Martins, Makoto Morishita, et al., editors, Proceedings of the sixth conference on machine translation, pages 1030–1040, Online. Association for Computational Linguistics.\n\n\nGabriele Sarti, Arianna Bisazza, Ana Guerberof-Arenas, and Antonio Toral. 2022. DivEMT: Neural machine translation post-editing effort across typologically diverse languages. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Proceedings of the 2022 conference on empirical methods in natural language processing, pages 7795–7816, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\n\n\nGabriele Sarti, Vilém Zouhar, Grzegorz Chrupała, Ana Guerberof-Arenas, Malvina Nissim, and Arianna Bisazza. 2025. QE4PE: Word-level quality estimation for human post-editing. Arxiv Preprint.\n\n\nBeatrice Savoldi, Alan Ramponi, Matteo Negri, and Luisa Bentivogli. 2025. Translation in the hands of many: Centering lay users in machine translation interactions.\n\n\nRaksha Shenoy, Nico Herbig, Antonio Krüger, and Josef van Genabith. 2021. Investigating the helpfulness of word-level quality estimation for post-editing machine translation output. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Proceedings of the 2021 conference on empirical methods in natural language processing, pages 10173–10185, Online; Punta Cana, Dominican Republic. Association for Computational Linguistics.\n\n\nLucia Specia, Frédéric Blain, Marina Fomicheva, Erick Fonseca, Vishrav Chaudhary, Francisco Guzmán, and André F. T. Martins. 2020. Findings of the WMT 2020 shared task on quality estimation. In Loïc Barrault, Ondřej Bojar, Fethi Bougares, Rajen Chatterjee, Marta R. Costa-jussà, Christian Federmann, Mark Fishel, Alexander Fraser, Yvette Graham, Paco Guzman, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes, Philipp Koehn, André Martins, Makoto Morishita, Christof Monz, Masaaki Nagata, Toshiaki Nakazawa, et al., editors, Proceedings of the fifth conference on machine translation, pages 743–764, Online. Association for Computational Linguistics.\n\n\nAleš Tamchyna. 2021. Deploying MT quality estimation on a large scale: Lessons learned and open questions. In Janice Campbell, Ben Huyck, Stephen Larocca, Jay Marciano, Konstantin Savenkov, and Alex Yanishevsky, editors, Proceedings of machine translation summit XVIII: Users and providers track, pages 291–305, Virtual. Association for Machine Translation in the Americas.\n\n\nHelena Vasconcelos, Gagan Bansal, Adam Fourney, Q. Vera Liao, and Jennifer Wortman Vaughan. 2025. Generation probabilities are not enough: Uncertainty highlighting in AI code completions. ACM Trans. Comput.-Hum. Interact., 32(1).\n\n\nChrysoula Zerva, Frederic Blain, José G. C. De Souza, Diptesh Kanojia, Sourabh Deoghare, Nuno M. Guerreiro, Giuseppe Attanasio, Ricardo Rei, Constantin Orasan, Matteo Negri, Marco Turchi, Rajen Chatterjee, Pushpak Bhattacharyya, Markus Freitag, and André Martins. 2024. Findings of the quality estimation shared task at WMT 2024: Are LLMs closing the gap in QE? In Barry Haddow, Tom Kocmi, Philipp Koehn, and Christof Monz, editors, Proceedings of the ninth conference on machine translation, pages 82–109, Miami, Florida, USA. Association for Computational Linguistics.\n\n\nVilém Zouhar, Shuoyang Ding, Anna Currey, Tatyana Badeka, Jenyuan Wang, and Brian Thompson. 2024. Fine-tuned machine translation metrics struggle in unseen domains. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd annual meeting of the association for computational linguistics (volume 2: Short papers), pages 488–500, Bangkok, Thailand. Association for Computational Linguistics.",
    "crumbs": [
      "Interpretability in Human Translation Workflows",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Word-level Quality Estimation for Machine Translation Post-editing</span>"
    ]
  },
  {
    "objectID": "chapters/chap-9-qe4pe.html#footnotes",
    "href": "chapters/chap-9-qe4pe.html#footnotes",
    "title": "9  Word-level Quality Estimation for Machine Translation Post-editing",
    "section": "",
    "text": "Dataset: gsarti/qe4pe; Interface: gsarti/grote; Code: https://github.com/gsarti/qe4pe↩︎\nWe do not disclose the highlight modality to translators to avoid biasing their judgment in the evaluation.↩︎\nSee Table C.15 for annotation examples.↩︎\nHighlights are extended from tokens to words to match the granularity of other modalities.↩︎\nScores are normalized to account for highlight frequencies across modalities. Agreement is shown in Table C.12.↩︎\nEditing agreement is shown in Figure C.6.↩︎\nBreakdowns by domain and speed shown in Table C.14 and Table C.13↩︎\nPer-domain breakdown in Figure C.8.↩︎",
    "crumbs": [
      "Interpretability in Human Translation Workflows",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Word-level Quality Estimation for Machine Translation Post-editing</span>"
    ]
  },
  {
    "objectID": "chapters/chap-10-unsup-wqe.html",
    "href": "chapters/chap-10-unsup-wqe.html",
    "title": "10  Unsupervised MT Error Detection and Human Disagreement",
    "section": "",
    "text": "10.1 Introduction\nWord-level error spans are widely used in machine translation evaluation to obtain robust and fine-grained estimates of translation quality (Lommel et al., 2013; Freitag et al., 2021a; Freitag et al., 2021b; Kocmi et al., 2024b). Due to the cost of manual annotation, word-level quality estimation (WQE) was proposed for assisting in annotating error spans over MT outputs (Zouhar et al., 2025). Modern WQE approaches generally rely on costly inference with large language models or ad-hoc training with large amounts of human-annotated texts (Fernandes et al., 2023; Kocmi and Federmann, 2023; Guerreiro et al., 2024), making them impractical for less resourced settings (Zouhar et al., 2024).\nTo improve the efficiency of MT quality assessment, several works explored the use of signals derived from the internals of neural MT systems (Fomicheva et al., 2020; Fomicheva et al., 2021; Leiter et al., 2024), for identifying problems in MT outputs, such as hallucinations (Guerreiro et al., 2023a; Guerreiro et al., 2023b; Dale et al., 2023a; Dale et al., 2023b; Himmi et al., 2024). However, previous works have focused on sentence-level metrics for overall translation quality and do not evaluate performance on multiple label sets due to high annotation costs (Fomicheva et al., 2022; Zerva et al., 2024).\nIn this chapter, we conduct a more comprehensive evaluation spanning 10 unsupervised metrics derived from models’ inner representations and predictive distributions to identify translation errors at the word level. We test three open-source multilingual MT models and LLMs of varying sizes across 12 translation directions, including typologically diverse languages and challenging textual domains. Importantly, we focus on texts with multiple human annotations to measure the impact of individual annotator preferences on metric performance, setting a “human-level” baseline for the WQE task.\nWe address the following research questions:\nWe conclude with recommendations for improving the evaluation and usage of future WQE systems.",
    "crumbs": [
      "Interpretability in Human Translation Workflows",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Unsupervised MT Error Detection and Human Disagreement</span>"
    ]
  },
  {
    "objectID": "chapters/chap-10-unsup-wqe.html#sec-unsup-wqe-intro",
    "href": "chapters/chap-10-unsup-wqe.html#sec-unsup-wqe-intro",
    "title": "10  Unsupervised MT Error Detection and Human Disagreement",
    "section": "",
    "text": "Figure 10.1: Example of German\\(\\rightarrow\\)English translation with two sets of human word-level error span annotations and two examples of continuous and binary WQE metrics.\n\n\n\n\n\n\nHow accurate are unsupervised WQE metrics in detecting MT errors compared to trained metrics and human annotators?\nAre popular supervised WQE metrics well-calibrated?\nAre the relative performances of WQE metrics affected by the variability in human error annotations?",
    "crumbs": [
      "Interpretability in Human Translation Workflows",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Unsupervised MT Error Detection and Human Disagreement</span>"
    ]
  },
  {
    "objectID": "chapters/chap-10-unsup-wqe.html#sec-unsup-wqe-related-work",
    "href": "chapters/chap-10-unsup-wqe.html#sec-unsup-wqe-related-work",
    "title": "10  Unsupervised MT Error Detection and Human Disagreement",
    "section": "10.2 Related Work",
    "text": "10.2 Related Work\nActionable Insights from Interpretability Advances in interpretability research have elucidated multiple mechanisms underlying decision-making, knowledge representation, and biases in LMs (Ferrando et al., 2024). However, a better understanding of model’s inner workings often did not translate to tangible gains in model design and other practical applications, which remain rarely explored (Mosbach et al., 2024). Some examples in this direction include using targeted machine unlearning methods for safety-critical scenarios (Barez et al., 2025), or the use of attribution for trustworthy context citations in LM generations (Cohen-Wang et al., 2024; Sarti et al., 2024; Qi^* et al., 2024). In this study, unsupervised metrics extracted from an MT model during generation are employed to detect errors in models’ generated outputs, following the unsupervised QE paradigm introduced in Section 2.6. This can be seen as a variant of out-of-distribution detection in signal processing research (Hendrycks and Gimpel, 2017).\nUncertainty Estimation for Language Models The estimation of uncertainty in language models has garnered increasing attention (Baan et al., 2023), particularly in the context of generation tasks for which the set of plausible responses is large (Giulianelli et al., 2023). Predictive uncertainty is typically decomposed into its aleatoric and epistemic components, representing respectively the irreducible variability in the modeled phenomena, and the improvable confidence in model predictions (Kiureghian and Ditlevsen, 2009). Popular methods for uncertainty estimation involve the calibration of predictive probabilities to reflect aleatoric uncertainty (Jiang et al., 2020; Ulmer et al., 2022; Zhao et al., 2023; Chen et al., 2023), and conformal sets prediction (Zerva and Martins, 2024; Ravfogel et al., 2023). In this study, we utilize uncertainty signals from the predictive distribution of MT models and their internal processing to efficiently predict the resulting generation quality at a fine-grained, token-level scale.\nHuman Label Variation Human label variation is a type of uncertainty that arises from the inherent variability in human judgments (Plank et al., 2014; Plank, 2022), which can be hard to disentangle from actual annotation mistakes (Snow et al., 2008; Weber-Genzel et al., 2024). The use of multiple references was recently recommended to ensure a sound evaluation of generative LMs, reflecting human-plausible levels of variability (Giulianelli et al., 2023), contrary to standard practices that employ a single set of “gold” labels. In our analysis of QE4PE data, which contains multiple edits, we adopt a perspectivist approach1 to ensure a robust assessment of WQE metrics by accounting for annotators’ disagreement (Uma et al., 2021).",
    "crumbs": [
      "Interpretability in Human Translation Workflows",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Unsupervised MT Error Detection and Human Disagreement</span>"
    ]
  },
  {
    "objectID": "chapters/chap-10-unsup-wqe.html#sec-unsup-wqe-data",
    "href": "chapters/chap-10-unsup-wqe.html#sec-unsup-wqe-data",
    "title": "10  Unsupervised MT Error Detection and Human Disagreement",
    "section": "10.3 Models and Datasets",
    "text": "10.3 Models and Datasets\nWe use datasets containing error annotations or post-edits on the outputs of open-source models to extract unsupervised WQE metrics using real model outputs, thereby avoiding potential confounders. We select the following datasets, summarized in Table 10.1:\nDivEMT We reuse the DivEMT dataset, introduced in Chapter 8, including out-of-English machine translations towards six typologically diverse target languages (English\\(\\rightarrow\\)Arabic,Italian,Dutch,Turkish,Ukrainian,Vietnamese) produced by Google Translate and mBART-50 1-to-many for a subset of Wiki texts from the FLORES dataset (Goyal et al., 2022), with edits made by professional translators. In this study, we evaluate unsupervised metrics on the mBART-50 1-to-many model, converting the human post-edits into token-level labels to perform a cross-lingual comparison over a fixed set of examples.\nWMT24 The WMT24 dataset is taken from the General Machine Translation Shared Task at WMT 2024 (Kocmi et al., 2024a). It contains evaluation of several machine translation systems across English\\(\\rightarrow\\){Czech, Hindi, Japanese, Chinese, Russian} (634 segments per language) and Czech\\(\\rightarrow\\)Ukrainian (1954 segments). The human evaluation was conducted using the Error Span Annotation protocol (ESA, Kocmi et al. (2024b)), which involves human annotators highlighting erroneous spans in the translation and marking them as either minor or major errors. This dataset covers the news, social, and speech (with automatic speech recognition) domains. We adopt the official prompting setup from the WMT24 campaign, using the Aya23 model alongside the provided prompt and three in-context translation examples per language to ensure uniformity with previous results.2 Aya23 is a large language model introduced by Aryabumi et al. (2024) to improve the multilingual capabilities of the original Aya model (Üstün et al., 2024) on a selected set of 23 languages. The model was included in the WMT24 evaluation by Kocmi et al. (2024a), resulting in the best translation performance among the tested open-source models. The model is a decoder-only transformer model with 40 layers, a model dimension of 8196 and 64 attention heads per layer. Using WMT24 allows us to extend our evaluation to a state-of-the-art LLM, given the popularity of such systems in MT (Kocmi et al., 2023).\nQE4PE The QE4PE dataset, introduced in Chapter 9, was created to measure the effect of word-level error highlights when included in real-world human post-editing workflows. The QE4PE data provides granular behavioral metrics to evaluate the speed and quality of post-editing of 12 annotators for En\\(\\rightarrow\\)It and En\\(\\rightarrow\\)Nl across two challenging textual domains (social posts and biomedical abstracts) and four error span highlighting modalities, including the unsupervised Surprisal MCDvar method and the supervised xcomet-xxl we also test in this study. Provided that the presence of error span highlights was found to influence the editing choices of human editors, we limit our evaluation to the six human annotators per language that post-edited sentences without any highlights (3 for the Oracle Post-edit task to produce initial human-based highlights, and 3 for the No Highlight modality in the main task). This prevents us from biasing our evaluation of WQE metrics in favor of the metrics that influenced editing choices. As for DivEMT, we use the post-edits over translations—in this case, those of the NLLB 3.3B model (NLLB Team et al., 2024)—to produce token-level error spans, enabling an evaluation of WQE metrics across multiple annotation sets.\n\n\n\n\n\n\n\nDivEMT\nWMT24\nQE4PE\n\n\n\n\nLanguages\nen→ar,it, nl,tr,uk,vi\nen→ja,zh,hi,cs,ru cs→uk\nen→it,nl\n\n\nErrors type\nPost-edit\nAnnotation\nPost-edit\n\n\nLabel sets\n1\n1\n6\n\n\nDomains\nWiki\nMultiple\nSocial, Biomed\n\n\nMT Model\nmBART-50\nAya23\nNLLB\n\n\n# Segments\n2580\n5124\n3888\n\n\n\n\n\nTable 10.1: Summary of tested datasets. Error spans are obtained from explicit error annotations or post-edited spans.",
    "crumbs": [
      "Interpretability in Human Translation Workflows",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Unsupervised MT Error Detection and Human Disagreement</span>"
    ]
  },
  {
    "objectID": "chapters/chap-10-unsup-wqe.html#sec-unsup-wqe-metrics",
    "href": "chapters/chap-10-unsup-wqe.html#sec-unsup-wqe-metrics",
    "title": "10  Unsupervised MT Error Detection and Human Disagreement",
    "section": "10.4 Evaluated Metrics",
    "text": "10.4 Evaluated Metrics\nThe following metrics were evaluated using the Inseq library introduced in Chapter 3.\nPredictive Distribution Metrics We use the Surprisal of the predicted token \\(t^{*}\\), as negative log-probability \\(-\\log p(t^{*}_i|t_{&lt;i})\\), and the Entropy \\(H\\) of the output distribution \\(P_N\\) over vocabulary \\(\\mathcal{V}\\), \\(-\\sum_{i=1}^{|\\mathcal{V}|} p(t_i|t_{&lt;i}) \\log_2 p(t_i|t_{&lt;i})\\), as simple metrics to quantify pointwise and full prediction uncertainty (Fomicheva et al., 2020). For surprisal, we also compute its expectation (MCD\\(_\\text{avg}\\)) and variance (MCD\\(_\\text{var}\\)) with \\(n=10\\) steps of Monte Carlo Dropout (MCD, Gal and Ghahramani, 2016) to obtain a robust estimate and a measure of epistemic uncertainty in predictions, respectively. Intuitively, epistemic uncertainty reflects models’ lack of knowledge rather than data ambiguity.3 We employ the mean of the negative log probabilities as a robust estimate of surprisal:\n\\[\\text{Surprisal MCD}_{\\text{avg}} = \\hat y_{\\text{MCD}} = \\frac{1}{T} \\sum_{t=1}^{T} - \\log p(x | \\Theta_t)\\]\nMoreover, we estimate predictive uncertainty by calculating the variance of predictive probabilities under the same setup:\n\\[\\text{Surprisal MCD}_{\\text{var}} = \\frac{1}{T} \\sum_{t=1}^{T} \\big(- \\log p(x | \\Theta_t) - \\hat y_{\\text{MCD}} \\big)\\]\nVocabulary Projections We use the Logit Lens method (LL, nostalgebraist, 2020), introduced in Section 2.1.3, to extract probability distributions \\(P_0, \\dots, P_{N-1}\\) over \\(V\\) from intermediate activations at every layer \\(l_0, \\dots, l_{N-1}\\) of the decoder. We use the surprisal for the final prediction at every layer (LL-Surprisal) to assess the presence of layers with high sensitivity to incorrect predictions. For the NLLB and mBART-50 models, we also apply a final layer normalization before the projection, following the model architecture. For the Aya model, we instead scale logits by \\(0.0625\\) (the default logit_scale defined in the model configuration). Following the residual stream view of the transformer model (Elhage et al., 2021), the resulting logits offer insight into the model’s predictive confidence at that specific depth of processing. Then, we compute the KL divergence between every layer distribution and the final distribution \\(P_N\\), e.g. \\(\\text{KL}(P_{N-1}\\|P_N)\\), to highlight trends in the shift in predictive probability produced by the application of remaining layers (LL KL-Div). Finally, we adapt the approach of Baldock et al. (2021) and use the number of the first layer for which the final prediction corresponds to the top logit as a metric of model confidence, \\(l \\;\\text{s.t.}\\;\\arg \\max P_l = t^{*}\\) and \\(\\arg \\max P_i \\neq t^{*} \\;\\forall i&lt;l\\) (LL Pred. Depth).\nContext mixing We employ simple estimates of context relevance using attention weights produced during the transformer attention operation. More specifically, for every attention head at every layer of the decoder module, we extract a score for every token in the preceding context. We then use the entropy of the distribution of attention weights4 over previous context as a simple measure of information locality during inference (Ferrando et al., 2022; Mohebbi et al., 2023). Following Fomicheva et al. (2020), we experiment with using the mean and the maximum entropy across all attention heads of all layers as separate metrics (Attn. Entropyavg/max). Finally, we evaluate the Between Layer OOD method by Jelenić et al. (2024), employing gradients to estimate layer transformation smoothness for OOD detection (BLOOD).\nSupervised baselines We also test the state-of-the-art supervised WQE model xcomet (Guerreiro et al., 2024), introduced in Section 2.6. In this chapter, we focus on their word-level error span prediction capabilities in a quality estimation setup, where the model classifies every input token according to MQM severity levels {ok, minor, major, critical} with a learned linear layer.5 Contrary to the continuous metrics from the previous section, binary labels from xcomet cannot be easily calibrated to match subjective annotation propensity. Hence, we propose to adapt the xcomet metric to use the sum of probability for all error types as a token-level continuous confidence metric, \\(s(t^{*}) = p(\\text{minor}) + p(\\text{major}) + p(\\text{critical})\\), which we dub xcometconf.\nHuman Editors For QE4PE, we report the min/mean/max agreement between each annotator’s edited spans and those of the other five editors as a less subjective “human-level” quality measure.",
    "crumbs": [
      "Interpretability in Human Translation Workflows",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Unsupervised MT Error Detection and Human Disagreement</span>"
    ]
  },
  {
    "objectID": "chapters/chap-10-unsup-wqe.html#sec-unsup-wqe-experiments",
    "href": "chapters/chap-10-unsup-wqe.html#sec-unsup-wqe-experiments",
    "title": "10  Unsupervised MT Error Detection and Human Disagreement",
    "section": "10.5 Experiments",
    "text": "10.5 Experiments\n\n10.5.1 Setup\nToken-level Evaluation Error spans used as labels in our evaluation are defined at the character level, while metric scores depend on the tokenization employed by either the MT model (for unsupervised metrics) or xcomet (for supervised metrics). To facilitate comparison, we label tokens as part of an error span if at least one character contained within them was marked as an error or edited by an annotator. Table 10.2 and Table 10.3 provide examples of various segmentations for the same MT output.\n\n\n\nHover highlighted spans to see error annotations.\n\n\n\n\nSourceen\nSo why is it that people jump through extra hoops to install Google Maps?\n\n\nMTit (NLLB)\nQuindi perché le persone devono fare un salto in più per installare Google Maps?\n\n\n\n\n\n\nAnnotator \\(t1\\)\nQuindi perché le persone devono fare un passaggio in più per installare Google Maps?\n\n\nAnnotator \\(t2\\)\nQuindi perché le persone fanno i salti mortali per installare Google Maps?\n\n\nAnnotator \\(t3\\)\nQuindi perché le persone effettuano dei passaggi ulteriori e superflui per installare Google Maps?\n\n\nAnnotator \\(t4\\)\nAllora perché le persone fanno un passaggio in più per installare Google Maps?\n\n\nAnnotator \\(t5\\)\nE allora mi chiedo: perché gli utenti iPhone si affannano tanto per installare Google Maps?\n\n\nAnnotator \\(t6\\)\nQuindi perché le persone fanno di tutto per installare Google Maps?\n\n\nEdit Counts (Figure 10.3)\nQuindi perché le persone devono fare un salto in più per installare Google Maps?\n\n\n\n\n\n\nxcomet-xl\nQuindi perché le persone devono fare un salto in più per installare Google Maps?\n\n\nxcomet-xxl\nQuindi perché le persone devono fare un salto in più per installare Google Maps?\n\n\nxcomet-xl\\(_{\\text{conf}}\\)\nQuindi perché le persone devono fare un salto in più per install are Google Maps ?\n\n\nxcomet-xxl\\(_{\\text{conf}}\\)\nQuindi perché le persone devono fare un salto in più per install are Google Maps ?\n\n\nSurprisal MCD\\(_{\\text{var}}\\)\nQuindi perché le persone devono fare un sal to in più per installare Google Maps ?\n\n\n\n\n\nTable 10.2: Annotated example from the En\\(\\rightarrow\\)It portion of the QE4PE dataset. Top: Annotator edits with highlighted final text and replaced text on top, with count-based aggregation showing inter-annotator agreement. Bottom: Word-level annotations for best-performing metrics discussed in the study.\n\n\n\n\n\n\nHover highlighted spans to see error annotations.\n\n\n\n\nSourceen\nSo the challenges in this are already showing themselves. I'm likely going to have a VERY difficult time getting a medical clearance due to the FAA's stance on certain medications.\n\n\nMTcs (Aya23)\nTakže problémy s tím se již projevují. Pravděpodobně budu mít PŘESNĚ obtížný čas dostat lékařské potvrzení kvůli postoji FAA k některým lékům.\n\n\n\n\n\n\nAnnotator\nTakže problémy s tím se již projevují. Pravděpodobně budu mít PŘESNĚ obtížný čas dostat lékařské potvrzení kvůli postoji FAA k některým lékům.\n\n\n\n\n\n\nxcomet-xl\nTakže problémy s tím se již projevují. Pravděpodobně budu mít PŘESNĚ obtížný čas dostat lékařské potvrzení kvůli postoji FAA k některým lékům\n\n\nxcomet-xxl\nTakže problémy s tím se již projevují. Pravděpodobně budu mít PŘESNĚ obtížný čas dostat lékařské potvrzení kvůli postoji FAA k některým lékům.\n\n\nxcomet-xl\\(_{\\text{conf}}\\)\nTakže problémy s tím se již projevují . Pravděpodobně budu mít PŘESNĚ obtížný čas dostat lékařské potvrzení kvůli postoji FAA k některým lékům .\n\n\nxcomet-xxl\\(_{\\text{conf}}\\)\nTakže problémy s tím se již projevují . Pravděpodobně budu PŘESNĚ obtížný čas dostat lékařské potvrzení kvůli postoji FAA k některým lékům .\n\n\nOut. Entropy\nTakže problémy s tím se již projevují . Pravděpodobně budu mít PŘESNĚ obtížný čas dostat lékařské potvrzení kvůli postoji FAA k některým lékům .\n\n\n\n\n\nTable 10.3: Annotated example from the En\\(\\rightarrow\\)Cs portion of the WMT24 dataset. Top: Annotator edits with highlighted Error Span Annotation of minor and major errors. Bottom: Word-level annotations for best-performing metrics discussed in the study.\n\n\n\nConstraining generation Evaluating metrics at the word level can be challenging due to the need for perfect uniformity between model generations and annotated spans. For this reason, we extract unsupervised metrics during generation while force-decoding the annotated outputs from the MT model to ensure perfect adherence with annotated error spans. In general, such an approach could introduce a problematic confounder in the evaluation, as observed results may be the product of constraining a model towards an unnatural generation, rather than reflecting the underlying phenomena. However, in this study, we carefully ensure that the generation setup matches exactly the one of previous works where the annotated translations were produced, using the same MT model and the same inputs.6 Hence, the constraining process serves as a simple assurance of conformity in light of potential discrepancies introduced by different decoding strategies, and does not affect the soundness of our method.\n\n\n10.5.2 Results\nHow Accurate are Unsupervised WQE Metrics? Table 10.4 reports the average metrics performance across all translation directions across tested datasets.7 We report Average Precision (AP) as a general measure of metric quality across the full score range, and we estimate calibrated metric performance as the best F1 score (F1*) across all thresholds for binarizing continuous metric scores into pos./neg. labels matching human annotation.8 Our results show that, despite high variability in error span prevalence across different models, languages and annotators, metric rankings remain generally consistent, suggesting the presence of robust relations between various signals sourced from models’ inner workings and translation errors.\n\n\n\n\n\n\n\nMethod\nDivEMT\nWMT24\nQE4PE\n\n\n\n\nAP\nF1\\(^{*}\\)\nAP\nF1\\(^{*}\\)\nAP\nF1\\(^{*}\\)\n\n\n\n\n\nRandom\n.34\n.50\n.05\n.09\n.17\n.27\n\n\nunsupervised\nSurprisal\n.43\n.53\n.08\n.13\n.23\n.32\n\n\nOut. Entropy\n.46\n.51\n.10\n.16\n.23\n.31\n\n\nSurprisal mcd\\(_{\\text{avg}}\\)\n.43\n.53\n-\n-\n.24\n.33\n\n\nSurprisal mcd\\(_{\\text{var}}\\)\n.47\n.54\n-\n-\n.26\n.34\n\n\nLL Surprisal\\(_{\\text{best}}\\)\n.42\n.53\n.09\n.15\n.23\n.32\n\n\nLL KL-Div\\(_{\\text{best}}\\)\n.43\n.51\n.07\n.12\n.20\n.29\n\n\nLL Pred. Depth\n.39\n.51\n.06\n.12\n.20\n.29\n\n\nAtt. Entropy\\(_{\\text{avg}}\\)\n.37\n.50\n.05\n.09\n.18\n.28\n\n\nAtt. Entropy\\(_{\\text{max}}\\)\n.34\n.50\n.05\n.09\n.16\n.28\n\n\nblood\\(_{\\text{best}}\\)\n.34\n.50\n-\n-\n.17\n.28\n\n\nsupervised\nxcomet-xl\n.42\n.45\n.09\n.19\n.23\n.34\n\n\nxcomet-xl\\(_{\\text{conf}}\\)\n.54\n.55\n.15\n.23\n.32\n.37\n\n\nxcomet-xxl\n.43\n.41\n.09\n.20\n.22\n.31\n\n\nxcomet-xxl\\(_{\\text{conf}}\\)\n.56\n.55\n.16\n.24\n.33\n.37\n\n\nhuman\nHum. Editors\\(_{\\text{min}}\\)\n-\n-\n-\n-\n.24\n.34\n\n\nHum. Editors\\(_{\\text{avg}}\\)\n-\n-\n-\n-\n.28\n.41\n\n\nHum. Editors\\(_{\\text{max}}\\)\n-\n-\n-\n-\n.32\n.47\n\n\n\n\n\nTable 10.4: Average Precision (AP) and Optimal F1 (F1*) for metrics across tested datasets. Results are averaged across all languages and annotators, with best unsupervised and overall best results highlighted.\n\n\n\nAmong unsupervised metrics, we find those based on the output distribution to be most effective at identifying error spans, in line with previous segment-level QE results (Fomicheva et al., 2020). Notably, the Surprisal MCDvar shows strong performances in line with the default xcomet models. For the multi-label QE4PE dataset, we find that the best supervised metrics score on par with the average human annotator consensus (Hum. Editorsavg), while unsupervised metrics generally obtain lower performances.\nConfidence Weighting Enables xcomet Calibration From Table 10.4 results, default xcomet metrics underperform compared to the best unsupervised techniques, a surprising result given their ad-hoc tuning. On the contrary, simple continuous scores derived from xcomet (xcometconf) consistently reach better results across all tested sets. Figure 10.2 shows the precision-recall tradeoff for these metrics on the EN\\(\\rightarrow\\)IT subset of the DivEMT dataset.9 In their default form, commonly used for evaluation via the unbabel-comet library, xcomet metrics consistently outperform Surprisal MCDvar in terms of precision (51-60%, compared to 34% optimal precision for MCDvar), but identify only 32-26% of tokens annotated as errors, resulting in lower AP.\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n \n\n\n\n\nFigure 10.2: Precision-Recall tradeoff for binary and confidence-weighted xcomet variants and the Surprisal MCDvar metric for DivEMT EN\\(\\rightarrow\\)IT.\n\n\n\nThe low recall of these metrics may be problematic in WQE applications, where omitting an error could result in oversights by human post-editors, who may trust the comprehensiveness of WQE predictions. On the contrary, the confidence-weighted xcometconf shows strong performances across the whole recall range, resulting in consistent improvements in both F1* and AP Table 10.4. Concretely, these results confirm that default xcomet performance does not reflect the full capacity of the metric, and operating with granular confidence scores can be beneficial when calibration is possible.\nMetrics Performance for Multiple Annotations While our evaluation so far employed human error span annotations as binary labels, we set out to assess how more granular labeling schemes impact metrics’ performance. Given \\(L\\) sets of binary labels (up to 6 per language for QE4PE), we assign a score \\(s \\in \\{1,\\dots,L\\}\\) to every MT token using the number of annotators that marked it as an error, resulting in edit counts reflecting human agreement rate, as shown in Table 10.2.\nFigure 10.3 presents the correlation of various metrics as the number of annotators available increases, with median values and confidence bounds obtained from edit counts across all combinations of \\(L\\) label sets.10 The increasing trend in correlations across all reported metrics indicates that these methods effectively reflect the aleatoric uncertainty in error span labels, i.e., the disagreement between various annotators. In particular, the Surprisal MCDvar metric sees a steeper correlation increase than other well-performing metrics, surpassing default xcomet supervised approaches for higher correlation bins. This suggests the epistemic uncertainty derived from noisy model predictions might be a promising way to anticipate the aleatoric uncertainty across human annotators for WQE. We observe that 95% confidence intervals for high-scoring metrics largely overlap when a single set of labels is used, indicating that rankings of metric performance are subject to change depending on the subjective choices of the annotator. While this poses a problem when attempting a robust evaluation of WQE metrics, we remark that including multiple annotations largely mitigates this issue. As a result, we recommend explicitly accounting for human label variation by including multiple error annotations in future WQE evaluations to ensure generalizable findings.\n\n\n\n\n\n\nFigure 10.3: Spearman correlation between WQE metric scores and human edit counts across multiple annotation sets for QE4PE EN\\(\\rightarrow\\)IT (left) and EN\\(\\rightarrow\\)NL (right).",
    "crumbs": [
      "Interpretability in Human Translation Workflows",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Unsupervised MT Error Detection and Human Disagreement</span>"
    ]
  },
  {
    "objectID": "chapters/chap-10-unsup-wqe.html#sec-unsup-wqe-limitations",
    "href": "chapters/chap-10-unsup-wqe.html#sec-unsup-wqe-limitations",
    "title": "10  Unsupervised MT Error Detection and Human Disagreement",
    "section": "10.6 Limitations",
    "text": "10.6 Limitations\nOur findings are accompanied by several limitations. Firstly, our choice of tested datasets was limited by the availability of annotated outputs generated by open-source MT models. While several other datasets matching these criteria exist (Fomicheva et al., 2022; Yang et al., 2023; Dale et al., 2023b), we restricted our assessment to a sufficient subset to ensure diversity across languages and tested models to support our findings. To facilitate comparison with other datasets, our evaluation for WMT24 treats available error spans as binary labels and does not directly account for error severity in human-annotated spans. Our choice of unsupervised metrics was primarily driven by previous work on uncertainty quantification in MT, and ease of implementation for popular methods in mechanistic interpretability literature (Ferrando et al., 2024). However, our choices in the latter category were limited, as most methods are nowadays developed and tested specifically for decoder-only transformer models. Finally, despite their strong performance, we found that unsupervised methods based on MCD require substantial computational resources, and as such, we were unable to evaluate them on Aya23 35B. While our primary focus was to establish baseline performances across various popular methods, future work should leverage the latest insights from more advanced techniques, such as those requiring the tuning of vocabulary projections (Belrose et al., 2023; Yom Din et al., 2024) or the identification of “confidence neurons” that modulate predictive entropy (Stolfo et al., 2024).",
    "crumbs": [
      "Interpretability in Human Translation Workflows",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Unsupervised MT Error Detection and Human Disagreement</span>"
    ]
  },
  {
    "objectID": "chapters/chap-10-unsup-wqe.html#sec-unsup-wqe-conclusion",
    "href": "chapters/chap-10-unsup-wqe.html#sec-unsup-wqe-conclusion",
    "title": "10  Unsupervised MT Error Detection and Human Disagreement",
    "section": "10.7 Conclusion",
    "text": "10.7 Conclusion\nWe conducted a comprehensive evaluation of supervised and unsupervised WQE metrics across multiple languages and annotation sets. Our results show that, while unsupervised metrics generally lag behind state-of-the-art supervised systems, some uncertainty quantification methods based on the predictive distribution show promising correlation with human label variation. Moreover, we find that popular supervised WQE metrics generally have low levels of recall and can benefit from confidence weighting when calibration is possible. Finally, individual annotator preferences are key confounders in WQE evaluations and can be mitigated by using multiple annotation sets.\nWe offer the following practical recommendations for evaluating WQE systems:\n\nUse agreement between multiple human annotations to control the effect of subjective preferences and rank WQE metrics robustly.\nEmploy an in-distribution calibration set of error spans before testing to ensure fair metric comparisons, and favor evaluations accounting for precision-recall tradeoffs to ensure their usability across various confidence levels.\nPrevious work showed the effectiveness of visualization reflecting prediction confidence (Vasconcelos et al., 2025), such as highlights for various error severity levels (Sarti et al., 2025a). Consider using continuous WQE metrics in real-world applications such as WQE-augmented post-editing to convey fine-grained confidence variations.\n\nThis final assessment concludes our investigation into the potential of model processing signals for enhancing the downstream verification of machine-translated content, converting interpretability methods commonly used for model analysis into practical tools for improving decision-making in real-world human-AI interaction settings.\n\n\n\n\nViraat Aryabumi, John Dang, Dwarak Talupuru, Saurabh Dash, David Cairuz, Hangyu Lin, Bharat Venkitesh, Madeline Smith, Jon Ander Campos, Yi Chern Tan, Kelly Marchisio, Max Bartolo, Sebastian Ruder, Acyr Locatelli, Julia Kreutzer, Nick Frosst, Aidan Gomez, Phil Blunsom, Marzieh Fadaee, et al. 2024. Aya 23: Open weight releases to further multilingual progress.\n\n\nJoris Baan, Nico Daheim, Evgenia Ilia, Dennis Ulmer, Haau-Sing Li, Raquel Fernández, Barbara Plank, Rico Sennrich, Chrysoula Zerva, and Wilker Aziz. 2023. Uncertainty in natural language generation: From theory to applications.\n\n\nRobert Baldock, Hartmut Maennel, and Behnam Neyshabur. 2021. Deep learning through the lens of example difficulty. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P. S. Liang, and J. Wortman Vaughan, editors, Advances in neural information processing systems, volume 34, pages 10876–10889. Curran Associates, Inc.\n\n\nFazl Barez, Tingchen Fu, Ameya Prabhu, Stephen Casper, Amartya Sanyal, Adel Bibi, Aidan O’Gara, Robert Kirk, Ben Bucknall, Tim Fist, Luke Ong, Philip Torr, Kwok-Yan Lam, Robert Trager, David Krueger, Sören Mindermann, José Hernandez-Orallo, Mor Geva, and Yarin Gal. 2025. Open problems in machine unlearning for AI safety.\n\n\nNora Belrose, Zach Furman, Logan Smith, Danny Halawi, Igor Ostrovsky, Lev McKinney, Stella Biderman, and Jacob Steinhardt. 2023. Eliciting latent predictions from transformers with the tuned lens. ArXiv, abs/2303.08112.\n\n\nYangyi Chen, Lifan Yuan, Ganqu Cui, Zhiyuan Liu, and Heng Ji. 2023. A close look into the calibration of pre-trained language models. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st annual meeting of the association for computational linguistics (volume 1: Long papers), pages 1343–1367, Toronto, Canada. Association for Computational Linguistics.\n\n\nBenjamin Cohen-Wang, Harshay Shah, Kristian Georgiev, and Aleksander Mądry. 2024. ContextCite: Attributing model generation to context. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, Advances in neural information processing systems, volume 37, pages 95764–95807. Curran Associates, Inc.\n\n\nDavid Dale, Elena Voita, Loic Barrault, and Marta R. Costa-jussà. 2023a. Detecting and mitigating hallucinations in machine translation: Model internal workings alone do well, sentence similarity Even better. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st annual meeting of the association for computational linguistics (volume 1: Long papers), pages 36–50, Toronto, Canada. Association for Computational Linguistics.\n\n\nDavid Dale, Elena Voita, Janice Lam, Prangthip Hansanti, Christophe Ropers, Elahe Kalbassi, Cynthia Gao, Loic Barrault, and Marta Costa-jussà. 2023b. HalOmi: A manually annotated benchmark for multilingual hallucination and omission detection in machine translation. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 conference on empirical methods in natural language processing, pages 638–653, Singapore. Association for Computational Linguistics.\n\n\nNelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, et al. 2021. A mathematical framework for transformer circuits. Transformer Circuits Thread. https://transformer-circuits.pub/2021/framework/index.html.\n\n\nPatrick Fernandes, Daniel Deutsch, Mara Finkelstein, Parker Riley, André Martins, Graham Neubig, Ankush Garg, Jonathan Clark, Markus Freitag, and Orhan Firat. 2023. The devil is in the errors: Leveraging large language models for fine-grained machine translation evaluation. In Philipp Koehn, Barry Haddow, Tom Kocmi, and Christof Monz, editors, Proceedings of the eighth conference on machine translation, pages 1066–1083, Singapore. Association for Computational Linguistics.\n\n\nJavier Ferrando, Gerard I. Gállego, and Marta R. Costa-jussà. 2022. Measuring the mixing of contextual information in the transformer. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Proceedings of the 2022 conference on empirical methods in natural language processing, pages 8698–8714, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\n\n\nJavier Ferrando, Gabriele Sarti, Arianna Bisazza, and Marta R. Costa-jussà. 2024. A primer on the inner workings of transformer-based language models. Arxiv Preprint.\n\n\nMarina Fomicheva, Piyawat Lertvittayakumjorn, Wei Zhao, Steffen Eger, and Yang Gao. 2021. The Eval4NLP shared task on explainable quality estimation: Overview and results. In Yang Gao, Steffen Eger, Wei Zhao, Piyawat Lertvittayakumjorn, and Marina Fomicheva, editors, Proceedings of the 2nd workshop on evaluation and comparison of NLP systems, pages 165–178, Punta Cana, Dominican Republic. Association for Computational Linguistics.\n\n\nMarina Fomicheva, Shuo Sun, Erick Fonseca, Chrysoula Zerva, Frédéric Blain, Vishrav Chaudhary, Francisco Guzmán, Nina Lopatina, Lucia Specia, and André F. T. Martins. 2022. MLQE-PE: A multilingual quality estimation and post-editing dataset. In Nicoletta Calzolari, Frédéric Béchet, Philippe Blache, Khalid Choukri, Christopher Cieri, Thierry Declerck, Sara Goggi, Hitoshi Isahara, Bente Maegaard, Joseph Mariani, Hélène Mazo, Jan Odijk, and Stelios Piperidis, editors, Proceedings of the thirteenth language resources and evaluation conference, pages 4963–4974, Marseille, France. European Language Resources Association.\n\n\nMarina Fomicheva, Shuo Sun, Lisa Yankovskaya, Frédéric Blain, Francisco Guzmán, Mark Fishel, Nikolaos Aletras, Vishrav Chaudhary, and Lucia Specia. 2020. Unsupervised quality estimation for neural machine translation. Transactions of the Association for Computational Linguistics, 8:539–555.\n\n\nMarkus Freitag, George Foster, David Grangier, Viresh Ratnakar, Qijun Tan, and Wolfgang Macherey. 2021a. Experts, errors, and context: A large-scale study of human evaluation for machine translation. Transactions of the Association for Computational Linguistics, 9:1460–1474.\n\n\nMarkus Freitag, Ricardo Rei, Nitika Mathur, Chi-kiu Lo, Craig Stewart, George Foster, Alon Lavie, and Ondřej Bojar. 2021b. Results of the WMT21 metrics shared task: Evaluating metrics with expert-based human evaluations on TED and news domain. In Loic Barrault, Ondrej Bojar, Fethi Bougares, Rajen Chatterjee, Marta R. Costa-jussa, Christian Federmann, Mark Fishel, Alexander Fraser, Markus Freitag, Yvette Graham, Roman Grundkiewicz, Paco Guzman, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes, Philipp Koehn, Tom Kocmi, Andre Martins, Makoto Morishita, et al., editors, Proceedings of the sixth conference on machine translation, pages 733–774, Online. Association for Computational Linguistics.\n\n\nYarin Gal and Zoubin Ghahramani. 2016. Dropout as a bayesian approximation: Representing model uncertainty in deep learning. In Maria Florina Balcan and Kilian Q. Weinberger, editors, Proceedings of the 33rd international conference on machine learning, volume 48, pages 1050–1059, New York, NY, USA. Proceedings of Machine Learning Research (PLMR).\n\n\nMario Giulianelli, Joris Baan, Wilker Aziz, Raquel Fernández, and Barbara Plank. 2023. What comes next? Evaluating uncertainty in neural text generators against human production variability. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 conference on empirical methods in natural language processing, pages 14349–14371, Singapore. Association for Computational Linguistics.\n\n\nNaman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-Jen Chen, Guillaume Wenzek, Da Ju, Sanjana Krishnan, Marc’Aurelio Ranzato, Francisco Guzmán, and Angela Fan. 2022. The Flores-101 evaluation benchmark for low-resource and multilingual machine translation. Transactions of the Association for Computational Linguistics, 10:522–538.\n\n\nNuno M. Guerreiro, Pierre Colombo, Pablo Piantanida, and André Martins. 2023a. Optimal transport for unsupervised hallucination detection in neural machine translation. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st annual meeting of the association for computational linguistics (volume 1: Long papers), pages 13766–13784, Toronto, Canada. Association for Computational Linguistics.\n\n\nNuno M. Guerreiro, Ricardo Rei, Daan van Stigt, Luisa Coheur, Pierre Colombo, and André F. T. Martins. 2024. Xcomet: Transparent machine translation evaluation through fine-grained error detection. Transactions of the Association for Computational Linguistics, 12:979–995.\n\n\nNuno M. Guerreiro, Elena Voita, and André Martins. 2023b. Looking for a needle in a haystack: A comprehensive study of hallucinations in neural machine translation. In Andreas Vlachos and Isabelle Augenstein, editors, Proceedings of the 17th conference of the european chapter of the association for computational linguistics, pages 1059–1075, Dubrovnik, Croatia. Association for Computational Linguistics.\n\n\nDan Hendrycks and Kevin Gimpel. 2017. A baseline for detecting misclassified and out-of-distribution examples in neural networks. In International conference on learning representations (ICLR 2017).\n\n\nAnas Himmi, Guillaume Staerman, Marine Picot, Pierre Colombo, and Nuno M Guerreiro. 2024. Enhanced hallucination detection in neural machine translation through simple detector aggregation. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Proceedings of the 2024 conference on empirical methods in natural language processing, pages 18573–18583, Miami, Florida, USA. Association for Computational Linguistics.\n\n\nFran Jelenić, Josip Jukić, Martin Tutek, Mate Puljiz, and Jan Snajder. 2024. Out-of-distribution detection by leveraging between-layer transformation smoothness. In The twelfth international conference on learning representations.\n\n\nZhengbao Jiang, Frank F. Xu, Jun Araki, and Graham Neubig. 2020. How can we know what language models know? Transactions of the Association for Computational Linguistics, 8:423–438.\n\n\nArmen Der Kiureghian and Ove Ditlevsen. 2009. Aleatory or epistemic? Does it matter? Structural Safety, 31(2):105–112. Risk Acceptance and Risk Communication.\n\n\nTom Kocmi, Eleftherios Avramidis, Rachel Bawden, Ondřej Bojar, Anton Dvorkovich, Christian Federmann, Mark Fishel, Markus Freitag, Thamme Gowda, Roman Grundkiewicz, Barry Haddow, Marzena Karpinska, Philipp Koehn, Benjamin Marie, Christof Monz, Kenton Murray, Masaaki Nagata, Martin Popel, Maja Popović, et al. 2024a. Findings of the WMT24 general machine translation shared task: The LLM era is here but MT is not solved yet. In Barry Haddow, Tom Kocmi, Philipp Koehn, and Christof Monz, editors, Proceedings of the ninth conference on machine translation, pages 1–46, Miami, Florida, USA. Association for Computational Linguistics.\n\n\nTom Kocmi, Eleftherios Avramidis, Rachel Bawden, Ondřej Bojar, Anton Dvorkovich, Christian Federmann, Mark Fishel, Markus Freitag, Thamme Gowda, Roman Grundkiewicz, Barry Haddow, Philipp Koehn, Benjamin Marie, Christof Monz, Makoto Morishita, Kenton Murray, Masaaki Nagata, Toshiaki Nakazawa, Martin Popel, et al. 2023. Findings of the 2023 conference on machine translation (WMT23): LLMs are here but not quite there yet. In Philipp Koehn, Barry Haddow, Tom Kocmi, and Christof Monz, editors, Proceedings of the eighth conference on machine translation, pages 1–42, Singapore. Association for Computational Linguistics.\n\n\nTom Kocmi and Christian Federmann. 2023. Large language models are state-of-the-art evaluators of translation quality. In Mary Nurminen, Judith Brenner, Maarit Koponen, Sirkku Latomaa, Mikhail Mikhailov, Frederike Schierl, Tharindu Ranasinghe, Eva Vanmassenhove, Sergi Alvarez Vidal, Nora Aranberri, Mara Nunziatini, Carla Parra Escartín, Mikel Forcada, Maja Popovic, Carolina Scarton, and Helena Moniz, editors, Proceedings of the 24th annual conference of the european association for machine translation, pages 193–203, Tampere, Finland. European Association for Machine Translation.\n\n\nTom Kocmi, Vilém Zouhar, Eleftherios Avramidis, Roman Grundkiewicz, Marzena Karpinska, Maja Popović, Mrinmaya Sachan, and Mariya Shmatova. 2024b. Error span annotation: A balanced approach for human evaluation of machine translation. In Barry Haddow, Tom Kocmi, Philipp Koehn, and Christof Monz, editors, Proceedings of the ninth conference on machine translation, pages 1440–1453, Miami, Florida, USA. Association for Computational Linguistics.\n\n\nChristoph Leiter, Piyawat Lertvittayakumjorn, Marina Fomicheva, Wei Zhao, Yang Gao, and Steffen Eger. 2024. Towards explainable evaluation metrics for machine translation. Journal of Machine Learning Research, 25(75):1–49.\n\n\nArle Richard Lommel, Aljoscha Burchardt, and Hans Uszkoreit. 2013. Multidimensional quality metrics: A flexible system for assessing translation quality. In Proceedings of translating and the computer 35, London, UK. Aslib.\n\n\nHosein Mohebbi, Willem Zuidema, Grzegorz Chrupała, and Afra Alishahi. 2023. Quantifying context mixing in transformers. In Andreas Vlachos and Isabelle Augenstein, editors, Proceedings of the 17th conference of the european chapter of the association for computational linguistics, pages 3378–3400, Dubrovnik, Croatia. Association for Computational Linguistics.\n\n\nMarius Mosbach, Vagrant Gautam, Tomás Vergara Browne, Dietrich Klakow, and Mor Geva. 2024. From insights to actions: The impact of interpretability and analysis research on NLP. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Proceedings of the 2024 conference on empirical methods in natural language processing, pages 3078–3105, Miami, Florida, USA. Association for Computational Linguistics.\n\n\nNLLB Team, Marta R. Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula, Loic Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti, et al. 2024. Scaling neural machine translation to 200 languages. Nature, 630(8018):841–846.\n\n\nnostalgebraist. 2020. Interpreting GPT: The logit lens. AI Alignment Forum.\n\n\nBarbara Plank. 2022. The “problem” of human label variation: On ground truth in data, modeling and evaluation. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Proceedings of the 2022 conference on empirical methods in natural language processing, pages 10671–10682, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\n\n\nBarbara Plank, Dirk Hovy, and Anders Søgaard. 2014. Linguistically debatable or just plain wrong? In Kristina Toutanova and Hua Wu, editors, Proceedings of the 52nd annual meeting of the association for computational linguistics (volume 2: Short papers), pages 507–511, Baltimore, Maryland. Association for Computational Linguistics.\n\n\nJirui Qi^*, Gabriele Sarti^*, Raquel Fernández, and Arianna Bisazza. 2024. Model internals-based answer attribution for trustworthy retrieval-augmented generation. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Proceedings of the 2024 conference on empirical methods in natural language processing, pages 6037–6053, Miami, Florida, USA. Association for Computational Linguistics.\n\n\nShauli Ravfogel, Yoav Goldberg, and Jacob Goldberger. 2023. Conformal nucleus sampling. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Findings of the association for computational linguistics: ACL 2023, pages 27–34, Toronto, Canada. Association for Computational Linguistics.\n\n\nGabriele Sarti, Grzegorz Chrupała, Malvina Nissim, and Arianna Bisazza. 2024. Quantifying the plausibility of context reliance in neural machine translation. In The twelfth international conference on learning representations (ICLR 2024), Vienna, Austria. OpenReview.\n\n\nGabriele Sarti, Vilém Zouhar, Grzegorz Chrupała, Ana Guerberof-Arenas, Malvina Nissim, and Arianna Bisazza. 2025a. QE4PE: Word-level quality estimation for human post-editing. Arxiv Preprint.\n\n\nGabriele Sarti, Vilém Zouhar, Malvina Nissim, and Arianna Bisazza. 2025b. Unsupervised word-level quality estimation for machine translation through the lens of annotators (dis)agreement. Arxiv Preprint.\n\n\nRion Snow, Brendan O’Connor, Daniel Jurafsky, and Andrew Ng. 2008. Cheap and fast – but is it good? Evaluating non-expert annotations for natural language tasks. In Mirella Lapata and Hwee Tou Ng, editors, Proceedings of the 2008 conference on empirical methods in natural language processing, pages 254–263, Honolulu, Hawaii. Association for Computational Linguistics.\n\n\nAlessandro Stolfo, Ben Wu, Wes Gurnee, Yonatan Belinkov, Xingyi Song, Mrinmaya Sachan, and Neel Nanda. 2024. Confidence regulation neurons in language models. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, Advances in neural information processing systems, volume 37, pages 125019–125049. Curran Associates, Inc.\n\n\nDennis Ulmer, Jes Frellsen, and Christian Hardmeier. 2022. Exploring predictive uncertainty and calibration in NLP: A study on the impact of method & data scarcity. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Findings of the association for computational linguistics: EMNLP 2022, pages 2707–2735, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\n\n\nAlexandra N Uma, Tommaso Fornaciari, Dirk Hovy, Silviu Paun, Barbara Plank, and Massimo Poesio. 2021. Learning from disagreement: A survey. Journal of Artificial Intelligence Research, 72:1385–1470.\n\n\nAhmet Üstün, Viraat Aryabumi, Zheng Yong, Wei-Yin Ko, Daniel D’souza, Gbemileke Onilude, Neel Bhandari, Shivalika Singh, Hui-Lee Ooi, Amr Kayid, Freddie Vargus, Phil Blunsom, Shayne Longpre, Niklas Muennighoff, Marzieh Fadaee, Julia Kreutzer, and Sara Hooker. 2024. Aya model: An instruction finetuned open-access multilingual language model. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd annual meeting of the association for computational linguistics (volume 1: Long papers), pages 15894–15939, Bangkok, Thailand. Association for Computational Linguistics.\n\n\nHelena Vasconcelos, Gagan Bansal, Adam Fourney, Q. Vera Liao, and Jennifer Wortman Vaughan. 2025. Generation probabilities are not enough: Uncertainty highlighting in AI code completions. ACM Trans. Comput.-Hum. Interact., 32(1).\n\n\nLeon Weber-Genzel, Siyao Peng, Marie-Catherine De Marneffe, and Barbara Plank. 2024. VariErr NLI: Separating annotation error from human label variation. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd annual meeting of the association for computational linguistics (volume 1: Long papers), pages 2256–2269, Bangkok, Thailand. Association for Computational Linguistics.\n\n\nZhen Yang, Fandong Meng, Yuanmeng Yan, and Jie Zhou. 2023. Rethinking the word-level quality estimation for machine translation from human judgement. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Findings of the association for computational linguistics: ACL 2023, pages 2012–2025, Toronto, Canada. Association for Computational Linguistics.\n\n\nAlexander Yom Din, Taelin Karidi, Leshem Choshen, and Mor Geva. 2024. Jump to conclusions: Short-cutting transformers with linear transformations. In Nicoletta Calzolari, Min-Yen Kan, Veronique Hoste, Alessandro Lenci, Sakriani Sakti, and Nianwen Xue, editors, Proceedings of the 2024 joint international conference on computational linguistics, language resources and evaluation (LREC-COLING 2024), pages 9615–9625, Torino, Italia. ELRA; ICCL.\n\n\nChrysoula Zerva, Frederic Blain, José G. C. De Souza, Diptesh Kanojia, Sourabh Deoghare, Nuno M. Guerreiro, Giuseppe Attanasio, Ricardo Rei, Constantin Orasan, Matteo Negri, Marco Turchi, Rajen Chatterjee, Pushpak Bhattacharyya, Markus Freitag, and André Martins. 2024. Findings of the quality estimation shared task at WMT 2024: Are LLMs closing the gap in QE? In Barry Haddow, Tom Kocmi, Philipp Koehn, and Christof Monz, editors, Proceedings of the ninth conference on machine translation, pages 82–109, Miami, Florida, USA. Association for Computational Linguistics.\n\n\nChrysoula Zerva and André F. T. Martins. 2024. Conformalizing machine translation evaluation. Transactions of the Association for Computational Linguistics, 12:1460–1478.\n\n\nYao Zhao, Mikhail Khalman, Rishabh Joshi, Shashi Narayan, Mohammad Saleh, and Peter J Liu. 2023. Calibrating sequence likelihood improves conditional language generation. In The eleventh international conference on learning representations.\n\n\nVilém Zouhar, Shuoyang Ding, Anna Currey, Tatyana Badeka, Jenyuan Wang, and Brian Thompson. 2024. Fine-tuned machine translation metrics struggle in unseen domains. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd annual meeting of the association for computational linguistics (volume 2: Short papers), pages 488–500, Bangkok, Thailand. Association for Computational Linguistics.\n\n\nVilém Zouhar, Tom Kocmi, and Mrinmaya Sachan. 2025. AI-assisted human evaluation of machine translation. In Luis Chiruzzo, Alan Ritter, and Lu Wang, editors, Proceedings of the 2025 conference of the nations of the americas chapter of the association for computational linguistics: Human language technologies (volume 1: Long papers), pages 4936–4950, Albuquerque, New Mexico. Association for Computational Linguistics.",
    "crumbs": [
      "Interpretability in Human Translation Workflows",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Unsupervised MT Error Detection and Human Disagreement</span>"
    ]
  },
  {
    "objectID": "chapters/chap-10-unsup-wqe.html#footnotes",
    "href": "chapters/chap-10-unsup-wqe.html#footnotes",
    "title": "10  Unsupervised MT Error Detection and Human Disagreement",
    "section": "",
    "text": "https://pdai.info/↩︎\nhttps://github.com/wmt-conference/wmt-collect-translations↩︎\nMCD is tested only on encoder-decoder models since Aya layers do not include dropout. The MCD\\(_\\text{var}\\) setting corresponds to the Unsupervised setting from Chapter 9.↩︎\nFor encoder-decoder model, self-attention and cross-attention weights are concatenated and renormalized.↩︎\nThe default xcomet metric was used with the unbabel-comet library (v2.2.6).↩︎\nGeneration parameters such as sampling temperature are not relevant in this setting, provided that they only alter the selection of the following output token, which we do via force-decoding.↩︎\nFull breakdown available in Table C.16, Table C.17, Table C.18, Table C.19.↩︎\nRandom baseline AP values match the proportion of tokens marked as errors, which can vary greatly.↩︎\nResults for all datasets in Figure C.11, Figure C.12, Figure C.13, Figure C.14.↩︎\n\\(x\\)=1 corresponds to binary labels from previous sections.↩︎",
    "crumbs": [
      "Interpretability in Human Translation Workflows",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Unsupervised MT Error Detection and Human Disagreement</span>"
    ]
  },
  {
    "objectID": "chapters/chap-11-conclusion.html",
    "href": "chapters/chap-11-conclusion.html",
    "title": "11  Conclusion",
    "section": "",
    "text": "11.1 Research Questions Revisited\nLanguage models have evolved from narrow, task-specific tools to general-purpose architectures that convert knowledge into actionable insights across hundreds of languages. Interpretability research has shed light on how these systems process language, pioneering novel analysis methods to investigate their predictive behaviors and inner mechanisms. Today’s challenge is to translate these insights into practical tools and techniques that help debug models, control their behaviors, and ultimately improve their trustworthiness and usability in the eyes of users. This dissertation has sought to tackle this challenge, developing frameworks that serve the users of language models and machine translation systems at various levels: from everyday users who need factual answers from chatbots, to developers customizing model outputs, to professional editors refining machine translations.\nIn this final chapter, we begin by revisiting the research questions posed in Chapter 1 and answering them in relation to our findings. We then conclude by charting a path forward, discussing how actionable interpretability research can shape the next generation of transparent, controllable AI systems.\nThe development and deployment of the Inseq toolkit (Chapter 3) and its subsequent integration with the PECoRe framework (Chapter 4) have provided important insights into this question. From a conceptual standpoint, the main principle to facilitate their widespread adoption is a progressive disclosure of complexity, which is necessary to benefit users at all levels of expertise. This human-computer interaction concept proved essential for bridging the gap between two distinct user groups: interpretability researchers with deep technical expertise, and domain experts who understand the practical implications but may lack programming skills. We achieved this balance through three key strategies: first, we unified access to popular models and methods through interfaces compatible with mainstream frameworks. Second, we provided both cutting-edge techniques and extensible baselines with sensible defaults. Third, we created compelling visualizations and post-processing functions that surface key insights without overwhelming users.\nOn the technical front, supporting model quantization, efficient batching, and distributed inference proved to be challenging yet essential. As language models become increasingly computationally demanding, these optimizations ensure that our tools remain accessible across diverse domains and computational budgets. Our Inseq toolkit successfully innovates across these dimensions, providing simple interfaces for common use cases while maintaining access to advanced features. Its widespread adoption across machine translation, summarization, question answering, and conversational AI validates these design choices and demonstrates their broad applicability.\nOur PECoRe framework (Chapter 4) demonstrated that we can faithfully quantify context usage in language models and machine translation systems through a two-step process: first, by identifying context-sensitive tokens using contrastive information-theoretic metrics, and then attributing their generation to specific contextual cues through contrastive input attribution. This data-driven verification process replaces traditional heuristic-based analyses, enabling model debugging at scale.\nOur findings exposed critical weaknesses in context-aware MT systems. In particular, we traced gender agreement failures back to incorrect anaphora resolution and found formatting anomalies triggered by spurious examples in context. When we extended this analysis to retrieval-augmented generation with Mirage (Chapter 5), we found that attribution based on model internals could accurately cite relevant retrieved passages. Our proposed procedure avoids the pitfalls of post-hoc rationalizations using surface-level similarity between generated and retrieved contents, grounding instead the citation process in actual context usage for improved trustworthiness.\nThe comparative analysis of Chapter 7 established interpretability-based steering as a viable alternative to prompting for controllable machine translation. Our contrastive SAE steering framework matched prompting’s personalization accuracy—which already outperformed traditional fine-tuned MT systems in Chapter 6 —while offering distinct advantages in terms of efficiency and transparency. Remarkably, our framework successfully captured individual translators’ stylistic signatures using only learned sparse latent representations, succeeding even in the challenging domain of literary translation.\nMoreover, our probing analyses revealed that steering and prompting converge on similar mechanistic solutions, resulting in comparable underlying representations. However, steering methods offer crucial advantages: while in-context demonstrations can fail unpredictably based on prompting choices such as example ordering, steering provides direct control through an interpretable concept space with tunable steering intensity.\nThe DivEMT study in Chapter 8 provided a nuanced answer to this question. While access to MT generally improved translator productivity, its contribution varied dramatically by language pair. In our results, typological similarity emerged as a significant factor: languages closely related to the source language, English, such as Dutch and Italian, exhibited substantial post-editing productivity gains, whereas distant pairs, like English-Arabic and English-Vietnamese, showed minimal improvement even after controlling for the resourcedness of the training data.\nNotably, we found traditional MT quality metrics to correlate poorly with actual productivity benefits across languages. This disconnect challenges the fundamental assumptions that the outcome of better-scoring systems should require less editing, underscoring the need for user-centered assessment that goes beyond technical quality measures.\nOur QE4PE study revealed a multifaceted impact of error highlights on the workflow of professional translators. We identified potential error cues that influence both translators’ productivity and editing behavior in different ways, with effects that depend heavily on textual domains and translation direction. Interestingly, Italian translators responded to highlights by editing more broadly across entire texts, whereas Dutch translators focused their edits primarily on highlighted spans. These results suggest different approaches to the post-editing task, hinting at the influence of cultural factors at play.\nIn our error assessment, highlights led to a 15-20% reduction in critical errors compared to standard post-editing, as translators caught mistakes they might otherwise miss. However, overall quality metrics showed no improvement, indicating that coarse-grained quality metrics employed in MT evaluation might fail to capture these targeted benefits. Perhaps most surprisingly, we found no meaningful differences in terms of speed or quality between editors working with human-made highlights, supervised models, or unsupervised uncertainty metrics. This suggests that the technical accuracy of quality estimation, which is typically the focus of evaluation campaigns, matters less than understanding how to integrate these tools into translators’ workflows effectively.\nOur systematic evaluation in Chapter 10 demonstrated that unsupervised methods employing model internals can match supervised approaches in detecting translation errors across multiple models, datasets, and languages. The variance of token log-probabilities estimated with Monte Carlo Dropout (MCD) proved particularly robust for predicting error spans, outperforming methods based on vocabulary projections, attention weights, and other internal signals.\nWe found that the limitations of supervised metrics stem from their low recall, with predictions often missing the actual error distribution in test sets. Proper calibration of these metrics’ confidence dramatically improved their performance, bringing them close to inter-annotator agreement levels among professional translators. Crucially, we found that metric rankings can shift substantially when few annotations are present, depending on individual annotators’ subjective judgments. This underscores the necessity of multiple annotation sets and careful calibration for fair quality estimation assessment.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "chapters/chap-11-conclusion.html#research-questions-revisited",
    "href": "chapters/chap-11-conclusion.html#research-questions-revisited",
    "title": "11  Conclusion",
    "section": "",
    "text": "❓ Research Question 1 (RQ1)\n\n\n\nWhat are the conceptual and technical requirements for interpretability software tools enabling scalable and reproducible analyses into the inner workings of generative language models?\n\n\n\n\n\n\n\n\n\n\n❓ Research Question 2 (RQ2)\n\n\n\nHow do language models and machine translation systems exploit contextual information during generation, and how can we quantify this usage in a faithful manner?\n\n\n\n\n\n\n\n\n\n\n❓ Research Question 3 (RQ3)\n\n\n\nAre interpretability-based steering methods viable approaches for controllable machine translation? How do they compare with prompting-based methods in terms of their performance and their impact on models’ internal mechanisms?\n\n\n\n\n\n\n\n\n\n\n❓ Research Question 4 (RQ4)\n\n\n\nDoes MT contribute positively to the productivity of professional translators across different languages? Which factors influence its effectiveness?\n\n\n\n\n\n\n\n\n\n\n❓ Research Question 5 (RQ5)\n\n\n\nHow do word-level error highlights impact the productivity and editing choices of professional translators and the quality of resulting translations?\n\n\n\n\n\n\n\n\n\n\n❓ Research Question 6 (RQ6)\n\n\n\nCan unsupervised error span detection methods reliably identify problems in machine-translated outputs? How does human label variation affect their performance, compared to traditional supervised approaches?",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "chapters/chap-11-conclusion.html#outlook-and-future-directions",
    "href": "chapters/chap-11-conclusion.html#outlook-and-future-directions",
    "title": "11  Conclusion",
    "section": "11.2 Outlook and Future Directions",
    "text": "11.2 Outlook and Future Directions\nThe themes and findings of this dissertation open several promising avenues toward the role of actionable interpretability insights for trustworthy NLP systems.\nA core premise of this thesis—that downstream applications serve as invaluable testbeds for interpretability methods—resonates with current debates within the interpretability research community. As Marks (2025) argues, if interpretability methods enable use cases unattainable by other approaches, they provide evidence of genuine, significant insights. Our work validates this perspective: interpretability-based methods excel at answer attribution, controlled generation and error detection, providing more faithful and auditable results than the supervised models typically employed for those tasks. Our PECoRe framework, for example, can debug issues in context usage that would be hard to detect through simple behavioral evaluations.\nThe final experimental chapters of this thesis take this paradigm a step further, evaluating interpretability techniques not only by their accuracy on realistic tasks but also by their downstream impact on user decision-making, productivity, and satisfaction. While the focus of the NLP interpretability community in recent years has gravitated towards the low-level technical depths of mechanistic interpretability (Saphra and Wiegreffe, 2024), the emerging field of human-centered explainable AI (HCXAI)—which has for now mainly engaged the human-computer interaction community1—is taking the lead in developing sociotechnical frameworks for model explanations centered around users’ needs and experiences. The intersection between these areas remains frustratingly small: few mechanistic studies conduct downstream human evaluations, and most human-centered work fails to integrate the best state-of-the-art interpretability methods due to a lack of experience or resources. Work aimed at bridging this gap will remain essential to ensure that interpretability advances remain both technically sound and practically relevant.\nDespite its success, modern interpretability research faces a serious threat: the growing inaccessibility of frontier systems, which play a key role as prime “subjects” of interpretability studies. A recent survey of 184 recent interpretability works reveals a widening disparity between the capabilities of state-of-the-art systems and those of systems generally evaluated in interpretability studies (Fiotto-Kaufman et al., 2025). This gap, driven by engineering barriers and proprietary API restrictions, threatens the validity of insights derived from simpler, less capable models. Addressing this issue will require robust shared infrastructure for interpretability research, simplifying access to state-of-the-art systems and fostering a more inclusive research environment. Our proposed Inseq library was developed with this in mind, supporting methods such as quantized, batched and distributed inference to reduce the computational load of interpretability analyses. More recently, the NNSight library (Fiotto-Kaufman et al., 2025) represents the most significant step in this direction, providing researchers with fine-grained access to model internals through remote execution, abstracting away the complexity and costs associated with local hardware setups. Beyond tools, the computational cost of current interpretability methods is a barrier to their widespread adoption, particularly in production environments where faster predictions might be favored over more precise or trustworthy results. Future technical research should prioritize the development of more efficient techniques, exploring approximation methods, caching strategies, or ad-hoc kernels, while preserving faithfulness to the model’s inner workings. The CAT method from Chapter 3, which approximates patching with contrastive gradient attribution, exemplifies the many possibilities in this direction.\nPerhaps most importantly, interpretability research can pave the way for more effective human-AI collaboration. Our translator studies show that model insights have the potential to transform professional workflows, but also that presentation matters as much, if not more, than accuracy. Despite that, the presentation of interpretability insights is often overlooked by current work. The translation domain presents unique challenges in this area, with human professionals operating in similar settings but across entirely different languages and cultural contexts, requiring tailored approaches. User-centric interfaces that let domain experts explore model behaviors quickly and intuitively will be essential for addressing these challenges.\nAs language model adoption accelerates, the demand for transparency and usability tools will only intensify, and interpretability researchers are in a crucial position to address these requests. The methods, insights, and perspectives presented in this dissertation demonstrate the potential of interpretability in machine translation, while highlighting the critical importance of continued research at the intersection of interpretability, multilingual NLP, and human-computer interaction. By making these systems more transparent, controllable, and aligned with human needs, we move toward a future where language technologies do not operate as opaque oracles, but rather as trusted partners helping us tackle the complex challenges ahead.\n\n\n\n\nJaden Fried Fiotto-Kaufman, Alexander Russell Loftus, Eric Todd, Jannik Brinkmann, Koyena Pal, Dmitrii Troitskii, Michael Ripa, Adam Belfki, Can Rager, Caden Juang, Aaron Mueller, Samuel Marks, Arnab Sen Sharma, Francesca Lucchetti, Nikhil Prakash, Carla E. Brodley, Arjun Guha, Jonathan Bell, Byron C Wallace, et al. 2025. NNsight and NDIF: Democratizing access to open-weight foundation model internals. In The thirteenth international conference on learning representations.\n\n\nSamuel Marks. 2025. Downstream applications as validation of interpretability. LessWrong Post.\n\n\nNaomi Saphra and Sarah Wiegreffe. 2024. Mechanistic? In Yonatan Belinkov, Najoung Kim, Jaap Jumelet, Hosein Mohebbi, Aaron Mueller, and Hanjie Chen, editors, Proceedings of the 7th BlackboxNLP workshop: Analyzing and interpreting neural networks for NLP, pages 480–498, Miami, Florida, US. Association for Computational Linguistics.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "chapters/chap-11-conclusion.html#footnotes",
    "href": "chapters/chap-11-conclusion.html#footnotes",
    "title": "11  Conclusion",
    "section": "",
    "text": "The main workshop in this area is organized by the ACM SIGCHI interest group.↩︎",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "chapters/references.html",
    "href": "chapters/references.html",
    "title": "Bibliography",
    "section": "",
    "text": "Abubakar Abid, Ali Abdalla, Ali Abid, Dawood Khan, Abdulrahman Alfozan,\nand James Y. Zou. 2019. Gradio: Hassle-free sharing and\ntesting of ML models in the wild. ArXiv,\nabs/1906.02569.\n\n\nSamira Abnar and Willem Zuidema. 2020. Quantifying\nattention flow in transformers. In Dan Jurafsky, Joyce Chai, Natalie\nSchluter, and Joel Tetreault, editors, Proceedings of the 58th\nannual meeting of the association for computational linguistics,\npages 4190–4197, Online. Association for Computational Linguistics.\n\n\nReduan Achtibat, Sayed Mohammad Vakilzadeh Hatefi, Maximilian Dreyer,\nAakriti Jain, Thomas Wiegand, Sebastian Lapuschkin, and Wojciech Samek.\n2024. AttnLRP: Attention-aware layer-wise relevance propagation for\ntransformers. In Proceedings of the 41st international conference on\nmachine learning, Vienna, Austria. JMLR.org.\n\n\nJulius Adebayo, Justin Gilmer, Michael Muelly, Ian Goodfellow, Moritz\nHardt, and Been Kim. 2018. Sanity\nchecks for saliency maps. In S. Bengio, H. Wallach, H. Larochelle,\nK. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in\nneural information processing systems, volume 31, pages 9505–9515,\nMontréal, Canada. Curran Associates, Inc.\n\n\nJulius Adebayo, Michael Muelly, Harold Abelson, and Been Kim. 2022. Post hoc explanations\nmay be ineffective for detecting unknown spurious correlation. In\nInternational conference on learning representations.\n\n\nJulius Adebayo, Michael Muelly, Ilaria Liccardi, and Been Kim. 2020.\nDebugging tests for model explanations. In Proceedings of the 34th\ninternational conference on neural information processing systems,\nRed Hook, NY, USA. Curran Associates Inc.\n\n\nChirag Agarwal, Sree Harsha Tanneru, and Himabindu Lakkaraju. 2024. Faithfulness vs. Plausibility:\nOn the (un)reliability of explanations from large language models.\nArxiv.\n\n\nSweta Agrawal, António Farinhas, Ricardo Rei, and Andre Martins. 2024.\nCan automatic\nmetrics assess high-quality translations? In Yaser Al-Onaizan, Mohit\nBansal, and Yun-Nung Chen, editors, Proceedings of the 2024\nconference on empirical methods in natural language processing,\npages 14491–14502, Miami, Florida, USA. Association for Computational\nLinguistics.\n\n\nSweta Agrawal, Chunting Zhou, Mike Lewis, Luke Zettlemoyer, and Marjan\nGhazvininejad. 2023. In-context\nexamples selection for machine translation. In Anna Rogers, Jordan\nBoyd-Graber, and Naoaki Okazaki, editors, Findings of the\nassociation for computational linguistics: ACL 2023, pages\n8857–8873, Toronto, Canada. Association for Computational Linguistics.\n\n\nRoee Aharoni, Melvin Johnson, and Orhan Firat. 2019. Massively multilingual\nneural machine translation. In Jill Burstein, Christy Doran, and\nThamar Solorio, editors, Proceedings of the 2019 conference of the\nnorth American chapter of the association for computational\nlinguistics: Human language technologies, volume 1 (long and short\npapers), pages 3874–3884, Minneapolis, Minnesota. Association for\nComputational Linguistics.\n\n\nArafat Ahsan, Vandan Mujadia, and Dipti Misra Sharma. 2021. Assessing post-editing\neffort in the English-Hindi direction. In\nSivaji Bandyopadhyay, Sobha Lalitha Devi, and Pushpak Bhattacharyya,\neditors, Proceedings of the 18th international conference on natural\nlanguage processing (ICON), pages 44–53, National Institute of\nTechnology Silchar, Silchar, India. NLP Association of India (NLPAI).\n\n\nJ Alammar. 2021. Ecco: An open source\nlibrary for the explainability of transformer language models. In\nHeng Ji, Jong C. Park, and Rui Xia, editors, Proceedings of the 59th\nannual meeting of the association for computational linguistics and the\n11th international joint conference on natural language processing:\nSystem demonstrations, pages 249–257, Online. Association for\nComputational Linguistics.\n\n\nSimone Alghisi, Massimo Rizzoli, Gabriel Roccabruna, Seyed Mahed\nMousavi, and Giuseppe Riccardi. 2024. Should we fine-tune\nor RAG? Evaluating different techniques to adapt\nLLMs for dialogue. In Saad Mahamood, Nguyen Le Minh,\nand Daphne Ippolito, editors, Proceedings of the 17th international\nnatural language generation conference, pages 180–197, Tokyo,\nJapan. Association for Computational Linguistics.\n\n\nDuarte Miguel Alves, José Pombal, Nuno M Guerreiro, Pedro Henrique\nMartins, João Alves, Amin Farajian, Ben Peters, Ricardo Rei, Patrick\nFernandes, Sweta Agrawal, Pierre Colombo, José G. C. de Souza, and Andre\nMartins. 2024. Tower: An open\nmultilingual large language model for translation-related tasks. In\nFirst conference on language modeling.\n\n\nChantal Amrhein, Nikita Moghe, and Liane Guillou. 2022. ACES:\nTranslation accuracy challenge sets for evaluating machine translation\nmetrics. In Philipp Koehn, Loïc Barrault, Ondřej Bojar, Fethi\nBougares, Rajen Chatterjee, Marta R. Costa-jussà, Christian Federmann,\nMark Fishel, Alexander Fraser, Markus Freitag, Yvette Graham, Roman\nGrundkiewicz, Paco Guzman, Barry Haddow, Matthias Huck, Antonio Jimeno\nYepes, Tom Kocmi, André Martins, Makoto Morishita, et al., editors,\nProceedings of the seventh conference on machine translation\n(WMT), pages 479–513, Abu Dhabi, United Arab Emirates (Hybrid).\nAssociation for Computational Linguistics.\n\n\nChantal Amrhein, Nikita Moghe, and Liane Guillou. 2023. ACES:\nTranslation accuracy challenge sets at WMT 2023. In\nPhilipp Koehn, Barry Haddow, Tom Kocmi, and Christof Monz, editors,\nProceedings of the eighth conference on machine translation,\npages 695–712, Singapore. Association for Computational Linguistics.\n\n\nChantal Amrhein and Rico Sennrich. 2021. How suitable\nare subword segmentation strategies for translating non-concatenative\nmorphology? In Marie-Francine Moens, Xuanjing Huang, Lucia Specia,\nand Scott Wen-tau Yih, editors, Findings of the association for\ncomputational linguistics: EMNLP 2021, pages 689–705, Punta Cana,\nDominican Republic. Association for Computational Linguistics.\n\n\nChantal Amrhein and Rico Sennrich. 2022. Identifying\nweaknesses in machine translation metrics through minimum\nBayes risk decoding: A case study for\nCOMET. In Yulan He, Heng Ji, Sujian Li, Yang Liu, and\nChua-Hui Chang, editors, Proceedings of the 2nd conference of the\nasia-pacific chapter of the association for computational linguistics\nand the 12th international joint conference on natural language\nprocessing (volume 1: Long papers), pages 1125–1141, Online only.\nAssociation for Computational Linguistics.\n\n\nRohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin,\nAlexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng\nChen, et al. 2023. Palm 2\ntechnical report. Arxiv.\n\n\nAndy Arditi, Oscar Obeso, Aaquib Syed, Daniel Paleka, Nina Panickssery,\nWes Gurnee, and Neel Nanda. 2024. Refusal\nin language models is mediated by a single direction. In A.\nGloberson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C.\nZhang, editors, Advances in neural information processing\nsystems, volume 37, pages 136037–136083, Red Hook, NY, USA. Curran\nAssociates, Inc.\n\n\nSanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, and Andrej Risteski.\n2018. Linear algebraic\nstructure of word senses, with applications to polysemy.\nTransactions of the Association for Computational Linguistics,\n6:483–495.\n\n\nViraat Aryabumi, John Dang, Dwarak Talupuru, Saurabh Dash, David Cairuz,\nHangyu Lin, Bharat Venkitesh, Madeline Smith, Jon Ander Campos, Yi Chern\nTan, Kelly Marchisio, Max Bartolo, Sebastian Ruder, Acyr Locatelli,\nJulia Kreutzer, Nick Frosst, Aidan Gomez, Phil Blunsom, Marzieh Fadaee,\net al. 2024. Aya 23: Open\nweight releases to further multilingual progress.\n\n\nAkari Asai, Xinyan Yu, Jungo Kasai, and Hanna Hajishirzi. 2021. One\nquestion answering model for many languages with cross-lingual dense\npassage retrieval. Advances in Neural Information Processing\nSystems, 34:7547–7560.\n\n\nPepa Atanasova, Oana-Maria Camburu, Christina Lioma, Thomas Lukasiewicz,\nJakob Grue Simonsen, and Isabelle Augenstein. 2023. Faithfulness tests\nfor natural language explanations. In Anna Rogers, Jordan\nBoyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st\nannual meeting of the association for computational linguistics (volume\n2: Short papers), pages 283–294, Toronto, Canada. Association for\nComputational Linguistics.\n\n\nPepa Atanasova, Jakob Grue Simonsen, Christina Lioma, and Isabelle\nAugenstein. 2020. A diagnostic\nstudy of explainability techniques for text classification. In\nBonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors,\nProceedings of the 2020 conference on empirical methods in natural\nlanguage processing (EMNLP), pages 3256–3274, Online. Association\nfor Computational Linguistics.\n\n\nGiuseppe Attanasio, Eliana Pastor, Chiara Di Bonaventura, and Debora\nNozza. 2023. Ferret: A framework\nfor benchmarking explainers on transformers. In Danilo Croce and\nLuca Soldaini, editors, Proceedings of the 17th conference of the\neuropean chapter of the association for computational linguistics:\nSystem demonstrations, pages 256–266, Dubrovnik, Croatia.\nAssociation for Computational Linguistics.\n\n\nWilker Aziz, Sheila Castilho, and Lucia Specia. 2012. PET: A tool for\npost-editing and assessing machine translation. In Nicoletta\nCalzolari, Khalid Choukri, Thierry Declerck, Mehmet Uğur Doğan, Bente\nMaegaard, Joseph Mariani, Asuncion Moreno, Jan Odijk, and Stelios\nPiperidis, editors, Proceedings of the eighth international\nconference on language resources and evaluation\n(LREC‘12), pages 3982–3987, Istanbul, Turkey. European\nLanguage Resources Association (ELRA).\n\n\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. 2016. Layer normalization.\nArxiv Preprint.\n\n\nJoris Baan, Nico Daheim, Evgenia Ilia, Dennis Ulmer, Haau-Sing Li,\nRaquel Fernández, Barbara Plank, Rico Sennrich, Chrysoula Zerva, and\nWilker Aziz. 2023. Uncertainty in natural language\ngeneration: From theory to applications.\n\n\nAlexander AND Montavon Bach Sebastian AND Binder. 2015. On pixel-wise\nexplanations for non-linear classifier decisions by layer-wise relevance\npropagation. PLOS ONE, 10(7):1–46.\n\n\nDavid Baehrens, Timon Schroeter, Stefan Harmeling, Motoaki Kawanabe,\nKatja Hansen, and Klaus-Robert Müller. 2010. How to explain\nindividual classification decisions. J. Mach. Learn. Res.,\n11:1803–1831.\n\n\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by\njointly learning to align and translate. In Yoshua Bengio and Yann\nLeCun, editors, Proceedings of the 3rd international conference on\nlearning representations (ICLR), San Diego, CA, USA.\n\n\nRobert Baldock, Hartmut Maennel, and Behnam Neyshabur. 2021. Deep\nlearning through the lens of example difficulty. In M. Ranzato, A.\nBeygelzimer, Y. Dauphin, P. S. Liang, and J. Wortman Vaughan, editors,\nAdvances in neural information processing systems, volume 34,\npages 10876–10889. Curran Associates, Inc.\n\n\nSatanjeev Banerjee and Alon Lavie. 2005. METEOR: An\nautomatic metric for MT evaluation with improved\ncorrelation with human judgments. In Jade Goldstein, Alon Lavie,\nChin-Yew Lin, and Clare Voss, editors, Proceedings of the\nACL workshop on intrinsic and extrinsic evaluation measures\nfor machine translation and/or summarization, pages 65–72, Ann\nArbor, Michigan. Association for Computational Linguistics.\n\n\nFazl Barez, Tingchen Fu, Ameya Prabhu, Stephen Casper, Amartya Sanyal,\nAdel Bibi, Aidan O’Gara, Robert Kirk, Ben Bucknall, Tim Fist, Luke Ong,\nPhilip Torr, Kwok-Yan Lam, Robert Trager, David Krueger, Sören\nMindermann, José Hernandez-Orallo, Mor Geva, and Yarin Gal. 2025. Open problems in machine\nunlearning for AI safety.\n\n\nLoic Barrault, Ondrej Bojar, Fethi Bougares, Rajen Chatterjee, Marta R.\nCosta-jussa, Christian Federmann, Mark Fishel, Alexander Fraser, Markus\nFreitag, Yvette Graham, Roman Grundkiewicz, Paco Guzman, Barry Haddow,\nMatthias Huck, Antonio Jimeno Yepes, Philipp Koehn, Tom Kocmi, Andre\nMartins, Makoto Morishita, et al., editors. 2021. Proceedings of the sixth\nconference on machine translation. Association for\nComputational Linguistics, Online.\n\n\nJasmijn Bastings, Sebastian Ebert, Polina Zablotskaia, Anders Sandholm,\nand Katja Filippova. 2022. “Will\nyou find these shortcuts?” A protocol for evaluating the\nfaithfulness of input salience methods for text classification. In\nYoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors,\nProceedings of the 2022 conference on empirical methods in natural\nlanguage processing, pages 976–991, Abu Dhabi, United Arab\nEmirates. Association for Computational Linguistics.\n\n\nJasmijn Bastings and Katja Filippova. 2020. The elephant in\nthe interpretability room: Why use attention as explanation when we have\nsaliency methods? In Afra Alishahi, Yonatan Belinkov, Grzegorz\nChrupała, Dieuwke Hupkes, Yuval Pinter, and Hassan Sajjad, editors,\nProceedings of the third BlackboxNLP workshop on analyzing and\ninterpreting neural networks for NLP, pages 149–155, Online.\nAssociation for Computational Linguistics.\n\n\nRachel Bawden and Benoît Sagot. 2023. RoCS-MT:\nRobustness challenge set for machine translation. In Philipp Koehn,\nBarry Haddow, Tom Kocmi, and Christof Monz, editors, Proceedings of\nthe eighth conference on machine translation, pages 198–216,\nSingapore. Association for Computational Linguistics.\n\n\nRachel Bawden, Rico Sennrich, Alexandra Birch, and Barry Haddow. 2018.\nEvaluating discourse\nphenomena in neural machine translation. In Marilyn Walker, Heng Ji,\nand Amanda Stent, editors, Proceedings of the 2018 conference of the\nnorth American chapter of the association for computational\nlinguistics: Human language technologies, volume 1 (long papers),\npages 1304–1313, New Orleans, Louisiana. Association for Computational\nLinguistics.\n\n\nYonatan Belinkov. 2022. Probing classifiers:\nPromises, shortcomings, and advances. Computational\nLinguistics, 48(1):207–219.\n\n\nYonatan Belinkov, Nadir Durrani, Fahim Dalvi, Hassan Sajjad, and James\nGlass. 2017. What do\nneural machine translation models learn about morphology? In Regina\nBarzilay and Min-Yen Kan, editors, Proceedings of the 55th annual\nmeeting of the association for computational linguistics (volume 1: Long\npapers), pages 861–872, Vancouver, Canada. Association for\nComputational Linguistics.\n\n\nYonatan Belinkov and James Glass. 2019. Analysis methods in neural\nlanguage processing: A survey. Transactions of the Association\nfor Computational Linguistics, 7:49–72.\n\n\nNora Belrose, Zach Furman, Logan Smith, Danny Halawi, Igor Ostrovsky,\nLev McKinney, Stella Biderman, and Jacob Steinhardt. 2023. Eliciting latent predictions\nfrom transformers with the tuned lens. ArXiv,\nabs/2303.08112.\n\n\nLuisa Bentivogli, Arianna Bisazza, Mauro Cettolo, and Marcello Federico.\n2016. Neural versus\nphrase-based machine translation quality: A case study. In Jian Su,\nKevin Duh, and Xavier Carreras, editors, Proceedings of the 2016\nconference on empirical methods in natural language processing,\npages 257–267, Austin, Texas. Association for Computational Linguistics.\n\n\nNathaniel Berger, Stefan Riezler, Miriam Exel, and Matthias Huck. 2024.\nPost-edits are\npreferences too. In Barry Haddow, Tom Kocmi, Philipp Koehn, and\nChristof Monz, editors, Proceedings of the ninth conference on\nmachine translation, pages 1289–1300, Miami, Florida, USA.\nAssociation for Computational Linguistics.\n\n\nFederico Bianchi, Giuseppe Attanasio, Raphael Pisoni, Silvia Terragni,\nGabriele Sarti, and Dario Balestri. 2023. Contrastive\nlanguage–image pre-training for the Italian\nlanguage. In Federico Boschetti, Gianluca E. Lebani, Bernardo\nMagnini, and Nicole Novielli, editors, Proceedings of the 9th\nitalian conference on computational linguistics (CLiC-it 2023),\npages 78–85, Venice, Italy. CEUR Workshop Proceedings.\n\n\nBigScience Workshop, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie\nPavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra\nLuccioni, François Yvon, et al. 2022. BLOOM: A\n176B-parameter open-access multilingual language model.\nArxiv.\n\n\nBlair Bilodeau, Natasha Jaques, Pang Wei Koh, and Been Kim. 2024. Impossibility theorems\nfor feature attribution. Proceedings of the National Academy of\nSciences, 121(2):e2304406120.\n\n\nAlexandra Birch, Miles Osborne, and Philipp Koehn. 2008. Predicting success in machine\ntranslation. In Mirella Lapata and Hwee Tou Ng, editors,\nProceedings of the 2008 conference on empirical methods in natural\nlanguage processing, pages 745–754, Honolulu, Hawaii. Association\nfor Computational Linguistics.\n\n\nArianna Bisazza, Ahmet Üstün, and Stephan Sportel. 2021. On the difficulty of\ntranslating free-order case-marking languages. Transactions of\nthe Association for Computational Linguistics, 9:1233–1248.\n\n\nSidney Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao,\nLaurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang,\nMichael Pieler, Usvsn Sai Prashanth, Shivanshu Purohit, Laria Reynolds,\nJonathan Tow, Ben Wang, and Samuel Weinbach. 2022. GPT-NeoX-20B:\nAn open-source autoregressive language model. In Angela Fan, Suzana\nIlic, Thomas Wolf, and Matthias Gallé, editors, Proceedings of\nBigScience episode #5 – workshop on challenges\n& perspectives in creating large language models,\npages 95–136, virtual+Dublin. Association for Computational Linguistics.\n\n\nFrederic Blain, Chrysoula Zerva, Ricardo Rei, Nuno M. Guerreiro, Diptesh\nKanojia, José G. C. de Souza, Beatriz Silva, Tânia Vaz, Yan Jingxuan,\nFatemeh Azadi, Constantin Orasan, and André Martins. 2023. Findings of the\nWMT 2023 shared task on quality estimation. In Philipp\nKoehn, Barry Haddow, Tom Kocmi, and Christof Monz, editors,\nProceedings of the eighth conference on machine translation,\npages 629–653, Singapore. Association for Computational Linguistics.\n\n\nJohn Blatz, Erin Fitzgerald, George Foster, Simona Gandrabur, Cyril\nGoutte, Alex Kulesza, Alberto Sanchis, and Nicola Ueffing. 2004. Confidence estimation for\nmachine translation. In COLING 2004: Proceedings of\nthe 20th international conference on computational linguistics,\npages 315–321, Geneva, Switzerland. COLING.\n\n\nSu Lin Blodgett, Solon Barocas, Hal Daumé III, and Hanna Wallach. 2020.\nLanguage\n(technology) is power: A critical survey of\n“bias” in NLP. In Dan\nJurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault, editors,\nProceedings of the 58th annual meeting of the association for\ncomputational linguistics, pages 5454–5476, Online. Association for\nComputational Linguistics.\n\n\nBernd Bohnet, Vinh Q. Tran, Pat Verga, Roee Aharoni, Daniel Andor, Livio\nBaldini Soares, Jacob Eisenstein, Kuzman Ganchev, Jonathan Herzig, Kai\nHui, Tom Kwiatkowski, Ji Ma, Jianmo Ni, Tal Schuster, William W. Cohen,\nMichael Collins, Dipanjan Das, Donald Metzler, Slav Petrov, et al. 2022.\nAttributed question\nanswering: Evaluation and modeling for attributed large language\nmodels. ArXiv.\n\n\nOndřej Bojar, Christian Buck, Chris Callison-Burch, Christian Federmann,\nBarry Haddow, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and\nLucia Specia. 2013. Findings of the 2013\nWorkshop on Statistical Machine\nTranslation. In Ondrej Bojar, Christian Buck, Chris\nCallison-Burch, Barry Haddow, Philipp Koehn, Christof Monz, Matt Post,\nHerve Saint-Amand, Radu Soricut, and Lucia Specia, editors,\nProceedings of the eighth workshop on statistical machine\ntranslation, pages 1–44, Sofia, Bulgaria. Association for\nComputational Linguistics.\n\n\nOndřej Bojar, Rajen Chatterjee, Christian Federmann, Yvette Graham,\nBarry Haddow, Shujian Huang, Matthias Huck, Philipp Koehn, Qun Liu,\nVarvara Logacheva, Christof Monz, Matteo Negri, Matt Post, Raphael\nRubino, Lucia Specia, and Marco Turchi. 2017. Findings of the 2017\nconference on machine translation (WMT17). In Ondřej\nBojar, Christian Buck, Rajen Chatterjee, Christian Federmann, Yvette\nGraham, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes, Philipp\nKoehn, and Julia Kreutzer, editors, Proceedings of the second\nconference on machine translation, pages 169–214, Copenhagen,\nDenmark. Association for Computational Linguistics.\n\n\nSebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza\nRutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste\nLespiau, Bogdan Damoc, Aidan Clark, Diego De Las Casas, Aurelia Guy,\nJacob Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore,\nChris Jones, Albin Cassirer, et al. 2022. Improving\nlanguage models by retrieving from trillions of tokens. In Kamalika\nChaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and\nSivan Sabato, editors, Proceedings of the 39th international\nconference on machine learning, volume 162, pages 2206–2240. PMLR.\n\n\nAlexander Borzunov, Dmitry Baranchuk, Tim Dettmers, Maksim Riabinin,\nYounes Belkada, Artem Chumachenko, Pavel Samygin, and Colin Raffel.\n2023. Petals:\nCollaborative inference and fine-tuning of large models. In Danushka\nBollegala, Ruihong Huang, and Alan Ritter, editors, Proceedings of\nthe 61st annual meeting of the association for computational linguistics\n(volume 3: System demonstrations), pages 558–568, Toronto, Canada.\nAssociation for Computational Linguistics.\n\n\nLynne Bowker. 2002. Computer-aided\ntranslation technology: A practical introduction. University of\nOttawa Press.\n\n\nEleftheria Briakou, Di Lu, Ke Zhang, and Joel Tetreault. 2021. Olá,\nbonjour, salve! XFORMAL: A benchmark for multilingual\nformality style transfer. In Kristina Toutanova, Anna Rumshisky,\nLuke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan\nCotterell, Tanmoy Chakraborty, and Yichao Zhou, editors, Proceedings\nof the 2021 conference of the north american chapter of the association\nfor computational linguistics: Human language technologies, pages\n3199–3216, Online. Association for Computational Linguistics.\n\n\nEleftheria Briakou, Jiaming Luo, Colin Cherry, and Markus Freitag. 2024.\nTranslating\nstep-by-step: Decomposing the translation process for improved\ntranslation quality of long-form texts. In Barry Haddow, Tom Kocmi,\nPhilipp Koehn, and Christof Monz, editors, Proceedings of the ninth\nconference on machine translation, pages 1301–1317, Miami, Florida,\nUSA. Association for Computational Linguistics.\n\n\nTrenton Bricken, Adly Templeton, Joshua Batson, Brian Chen, Adam Jermyn,\nTom Conerly, Nick Turner, Cem Anil, Carson Denison, Amanda Askell,\nRobert Lasenby, Yifan Wu, Shauna Kravec, Nicholas Schiefer, Tim Maxwell,\nNicholas Joseph, Zac Hatfield-Dodds, Alex Tamkin, Karina Nguyen, et al.\n2023. Towards\nmonosemanticity: Decomposing language models with dictionary\nlearning. Transformer Circuits Thread.\n\n\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,\nAmanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger,\nTom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, et al. 2020. Language models are few-shot learners. In\nProceedings of the 34th international conference on neural\ninformation processing systems, Red Hook, NY, USA. Curran\nAssociates Inc.\n\n\nEmanuele Bugliarello, Sabrina J. Mielke, Antonios Anastasopoulos, Ryan\nCotterell, and Naoaki Okazaki. 2020. It‘s easier to\ntranslate out of English than into it:\nMeasuring neural translation difficulty by cross-mutual\ninformation. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel\nTetreault, editors, Proceedings of the 58th annual meeting of the\nassociation for computational linguistics, pages 1640–1649, Online.\nAssociation for Computational Linguistics.\n\n\nAylin Caliskan, Joanna J. Bryson, and Arvind Narayanan. 2017. Semantics derived\nautomatically from language corpora contain human-like biases.\nScience, 356(6334):183–186.\n\n\nChris Callison-Burch, Cameron Fordyce, Philipp Koehn, Christof Monz, and\nJosh Schroeder. 2007. (Meta-) evaluation of machine\ntranslation. In Chris Callison-Burch, Philipp Koehn, Cameron Shaw\nFordyce, and Christof Monz, editors, Proceedings of the second\nworkshop on statistical machine translation, pages 136–158, Prague,\nCzech Republic. Association for Computational Linguistics.\n\n\nSara Candussio, Gaia Saveri, Gabriele Sarti, and Luca Bortolussi. 2025.\nBridging logic and\nlearning: Decoding temporal logic embeddings via transformers. In\nMachine learning and knowledge discovery in databases. Research\ntrack. Springer Nature Switzerland.\n\n\nStephen Casper, Carson Ezell, Charlotte Siegmann, Noam Kolt, Taylor Lynn\nCurtis, Benjamin Bucknall, Andreas Haupt, Kevin Wei, Jérémy Scheurer,\nMarius Hobbhahn, Lee Sharkey, Satyapriya Krishna, Marvin Von Hagen,\nSilas Alberti, Alan Chan, Qinyi Sun, Michael Gerovitch, David Bau, Max\nTegmark, et al. 2024. Black-box access is\ninsufficient for rigorous AI audits. In Proceedings of the 2024\nACM conference on fairness, accountability, and transparency, pages\n2254–2272, New York, NY, USA. Association for Computing Machinery.\n\n\nSheila Castilho, Joss Moorkens, Federico Gaspari, Iacer Calixto, John\nTinsley, and Andy Way. 2017. Is neural machine\ntranslation the new state of the art? The Prague Bulletin of\nMathematical Linguistics, 108(1):109–120.\n\n\nMauro Cettolo, Marcello Federico, Luisa Bentivogli, Jan Niehues,\nSebastian Stüker, Katsuhito Sudoh, Koichiro Yoshino, and Christian\nFedermann. 2017. Overview of the\nIWSLT 2017 evaluation campaign. In Sakriani Sakti and\nMasao Utiyama, editors, Proceedings of the 14th international\nconference on spoken language translation, pages 2–14, Tokyo,\nJapan. International Workshop on Spoken Language Translation.\n\n\nSviatoslav Chalnev, Matthew Siu, and Arthur Conmy. 2024. Improving steering vectors by\ntargeting sparse autoencoder features. Arxiv.\n\n\nYangyi Chen, Lifan Yuan, Ganqu Cui, Zhiyuan Liu, and Heng Ji. 2023. A close look into\nthe calibration of pre-trained language models. In Anna Rogers,\nJordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the\n61st annual meeting of the association for computational linguistics\n(volume 1: Long papers), pages 1343–1367, Toronto, Canada.\nAssociation for Computational Linguistics.\n\n\nWon Ik Cho, Ji Won Kim, Seok Min Kim, and Nam Soo Kim. 2019. On measuring gender bias in\ntranslation of gender-neutral pronouns. In Marta R. Costa-jussà,\nChristian Hardmeier, Will Radford, and Kellie Webster, editors,\nProceedings of the first workshop on gender bias in natural language\nprocessing, pages 173–181, Florence, Italy. Association for\nComputational Linguistics.\n\n\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav\nMishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko,\nJoshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer,\nVinodkumar Prabhakaran, et al. 2023. PaLM: Scaling language\nmodeling with pathways. Journal of Machine Learning\nResearch, 24(240):1–113.\n\n\nGeorge Chrysostomou and Nikolaos Aletras. 2022. An empirical study\non explanations in out-of-domain settings. In Smaranda Muresan,\nPreslav Nakov, and Aline Villavicencio, editors, Proceedings of the\n60th annual meeting of the association for computational linguistics\n(volume 1: Long papers), pages 6920–6938, Dublin, Ireland.\nAssociation for Computational Linguistics.\n\n\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William\nFedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma,\nAlbert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen,\nAakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, et\nal. 2024. Scaling\ninstruction-finetuned language models. Journal of Machine\nLearning Research, 25(70):1–53.\n\n\nKenneth W. Church and Eduard H. Hovy. 1993. Good applications for crummy\nmachine translation. Machine Translation, 8(4):239–258.\n\n\nCristiano Ciaccio, Gabriele Sarti, Alessio Miaschi, and Felice\nDell’Orletta. 2025. Crossword\nspace: Latent manifold learning for italian crosswords and beyond.\nIn Cristina Bosco, Elisabetta Jezek, Marco Polignano, and Manuela\nSanguinetti, editors, Proceedings of the 11th italian conference on\ncomputational linguistics (CLiC-it 2023), Cagliari, Italy. CEUR\nWorkshop Proceedings.\n\n\nKevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D. Manning.\n2019. What does\nBERT look at? An analysis of BERT‘s\nattention. In Tal Linzen, Grzegorz Chrupała, Yonatan Belinkov, and\nDieuwke Hupkes, editors, Proceedings of the 2019 ACL workshop\nBlackboxNLP: Analyzing and interpreting neural networks for NLP,\npages 276–286, Florence, Italy. Association for Computational\nLinguistics.\n\n\nBenjamin Cohen-Wang, Harshay Shah, Kristian Georgiev, and Aleksander\nMądry. 2024. ContextCite:\nAttributing model generation to context. In A. Globerson, L. Mackey,\nD. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors,\nAdvances in neural information processing systems, volume 37,\npages 95764–95807. Curran Associates, Inc.\n\n\nÇağrı Çöltekin and Taraka Rama. 2023. What do complexity\nmeasures measure? Correlating and validating corpus-based measures of\nmorphological complexity. Linguistics Vanguard,\n9(s1):27–43.\n\n\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary,\nGuillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke\nZettlemoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In Dan Jurafsky,\nJoyce Chai, Natalie Schluter, and Joel Tetreault, editors,\nProceedings of the 58th annual meeting of the association for\ncomputational linguistics, pages 8440–8451, Online. Association for\nComputational Linguistics.\n\n\nAlexis Conneau and Guillaume Lample. 2019. Cross-lingual\nlanguage model pretraining. In H. Wallach, H. Larochelle, A.\nBeygelzimer, F. dAlché-Buc, E. Fox, and R. Garnett, editors,\nAdvances in neural information processing systems, volume 32.\nCurran Associates, Inc.\n\n\nAlexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams, Samuel\nBowman, Holger Schwenk, and Veselin Stoyanov. 2018. XNLI:\nEvaluating cross-lingual sentence representations. In Ellen Riloff,\nDavid Chiang, Julia Hockenmaier, and Jun’ichi Tsujii, editors,\nProceedings of the 2018 conference on empirical methods in natural\nlanguage processing, pages 2475–2485, Brussels, Belgium.\nAssociation for Computational Linguistics.\n\n\nSven Coppers, Jan Van den Bergh, Kris Luyten, Karin Coninx, Iulianna Van\nder Lek-Ciudin, Tom Vanallemeersch, and Vincent Vandeghinste. 2018. Intellingo:\nAn intelligible translation environment. In\nProceedings of the 2018 CHI conference on human factors in computing\nsystems, pages 1–13.\n\n\nRyan Cotterell, Sabrina J. Mielke, Jason Eisner, and Brian Roark. 2018.\nAre all languages equally\nhard to language-model? In Marilyn Walker, Heng Ji, and Amanda\nStent, editors, Proceedings of the 2018 conference of the north\nAmerican chapter of the association for computational\nlinguistics: Human language technologies, volume 2 (short papers),\npages 536–541, New Orleans, Louisiana. Association for Computational\nLinguistics.\n\n\nIan Covert, Scott Lundberg, and Su-In Lee. 2021. Explaining by removing: A\nunified framework for model explanation. Journal of Machine\nLearning Research, 22(209):1–90.\n\n\nJonathan Crabbé and Mihaela van der Schaar. 2023. Evaluating the\nrobustness of interpretability methods through explanation invariance\nand equivariance. In Thirty-seventh conference on neural\ninformation processing systems.\n\n\nMenglong Cui, Pengzhi Gao, Wei Liu, Jian Luan, and Bin Wang. 2025. Multilingual\nmachine translation with open large language models at practical scale:\nAn empirical study. In Luis Chiruzzo, Alan Ritter, and Lu Wang,\neditors, Proceedings of the 2025 conference of the nations of the\namericas chapter of the association for computational linguistics: Human\nlanguage technologies (volume 1: Long papers), pages 5420–5443,\nAlbuquerque, New Mexico. Association for Computational Linguistics.\n\n\nAnna Currey, Maria Nadejde, Raghavendra Reddy Pappagari, Mia Mayer,\nStanislas Lauly, Xing Niu, Benjamin Hsu, and Georgiana Dinu. 2022. MT-GenEval:\nA counterfactual and contextual dataset for evaluating gender accuracy\nin machine translation. In Yoav Goldberg, Zornitsa Kozareva, and Yue\nZhang, editors, Proceedings of the 2022 conference on empirical\nmethods in natural language processing, pages 4287–4299, Abu Dhabi,\nUnited Arab Emirates. Association for Computational Linguistics.\n\n\nJoke Daems, Sonia Vandepitte, Robert J. Hartsuiker, and Lieve Macken.\n2017a. Identifying\nthe machine translation error types with the greatest impact on\npost-editing effort. Frontiers in Psychology, 8.\n\n\nJoke Daems, Sonia Vandepitte, Robert Hartsuiker, and Lieve Macken.\n2017b. Translation methods\nand experience: A comparative analysis of human translation and\npost-editing with students and professional translators. Meta :\njournal des traducteurs / Meta: Translators’ Journal,\n62(2):245–270.\n\n\nDamai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu Wei.\n2022. Knowledge\nneurons in pretrained transformers. In Smaranda Muresan, Preslav\nNakov, and Aline Villavicencio, editors, Proceedings of the 60th\nannual meeting of the association for computational linguistics (volume\n1: Long papers), pages 8493–8502, Dublin, Ireland. Association for\nComputational Linguistics.\n\n\nDavid Dale, Elena Voita, Loic Barrault, and Marta R. Costa-jussà. 2023a.\nDetecting and\nmitigating hallucinations in machine translation: Model internal\nworkings alone do well, sentence similarity Even\nbetter. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki,\neditors, Proceedings of the 61st annual meeting of the association\nfor computational linguistics (volume 1: Long papers), pages 36–50,\nToronto, Canada. Association for Computational Linguistics.\n\n\nDavid Dale, Elena Voita, Janice Lam, Prangthip Hansanti, Christophe\nRopers, Elahe Kalbassi, Cynthia Gao, Loic Barrault, and Marta\nCosta-jussà. 2023b. HalOmi:\nA manually annotated benchmark for multilingual hallucination and\nomission detection in machine translation. In Houda Bouamor, Juan\nPino, and Kalika Bali, editors, Proceedings of the 2023 conference\non empirical methods in natural language processing, pages 638–653,\nSingapore. Association for Computational Linguistics.\n\n\nXuan-Quy Dao and Ngoc-Bich Le. 2023. Chatgpt is good but bing chat is\nbetter for vietnamese students. Arxiv.\n\n\nTim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. 2022. GPT3.int8():\n8-bit matrix multiplication for transformers at scale. In S. Koyejo,\nS. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors,\nAdvances in neural information processing systems, volume 35,\npages 30318–30332. Curran Associates, Inc.\n\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.\nBERT:\nPre-training of deep bidirectional transformers for language\nunderstanding. In Jill Burstein, Christy Doran, and Thamar Solorio,\neditors, Proceedings of the 2019 conference of the north\nAmerican chapter of the association for computational\nlinguistics: Human language technologies, volume 1 (long and short\npapers), pages 4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\n\n\nJay DeYoung, Sarthak Jain, Nazneen Fatema Rajani, Eric Lehman, Caiming\nXiong, Richard Socher, and Byron C. Wallace. 2020. ERASER:\nA benchmark to evaluate rationalized NLP\nmodels. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel\nTetreault, editors, Proceedings of the 58th annual meeting of the\nassociation for computational linguistics, pages 4443–4458, Online.\nAssociation for Computational Linguistics.\n\n\nQingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Jingyuan Ma, Rui Li, Heming\nXia, Jingjing Xu, Zhiyong Wu, Baobao Chang, Xu Sun, Lei Li, and Zhifang\nSui. 2024. A\nsurvey on in-context learning. In Yaser Al-Onaizan, Mohit Bansal,\nand Yun-Nung Chen, editors, Proceedings of the 2024 conference on\nempirical methods in natural language processing, pages 1107–1128,\nMiami, Florida, USA. Association for Computational Linguistics.\n\n\nDavid L. Donoho and Michael Elad. 2003. Optimally sparse\nrepresentation in general (nonorthogonal) dictionaries via\n&#x2113;&lt;sup&gt;1&lt;/sup&gt; minimization. Proceedings\nof the National Academy of Sciences, 100(5):2197–2202.\n\n\nFinale Doshi-Velez and Been Kim. 2017. Towards a rigorous science of\ninterpretable machine learning.\n\n\nZi-Yi Dou and Graham Neubig. 2021. Word alignment by\nfine-tuning embeddings on parallel corpora. In Paola Merlo, Jorg\nTiedemann, and Reut Tsarfaty, editors, Proceedings of the 16th\nconference of the european chapter of the association for computational\nlinguistics: Main volume, pages 2112–2128, Online. Association for\nComputational Linguistics.\n\n\nEsin Durmus, He He, and Mona Diab. 2020. FEQA:\nA question answering evaluation framework for faithfulness assessment in\nabstractive summarization. In Dan Jurafsky, Joyce Chai, Natalie\nSchluter, and Joel Tetreault, editors, Proceedings of the 58th\nannual meeting of the association for computational linguistics,\npages 5055–5070, Online. Association for Computational Linguistics.\n\n\nLukas Edman, Gabriele Sarti, Antonio Toral, Gertjan van Noord, and\nArianna Bisazza. 2024. Are character-level\ntranslations worth the wait? Comparing ByT5\nand mT5 for machine translation. Transactions of\nthe Association for Computational Linguistics, 12:392–410.\n\n\nUpol Ehsan, Q. Vera Liao, Michael Muller, Mark O. Riedl, and Justin D.\nWeisz. 2021. Expanding\nexplainability: Towards social transparency in AI systems. In\nProceedings of the 2021 CHI conference on human factors in computing\nsystems, New York, NY, USA. Association for Computing Machinery.\n\n\nUpol Ehsan, Samir Passi, Q. Vera Liao, Larry Chan, I-Hsiang Lee, Michael\nMuller, and Mark O Riedl. 2024. The who in XAI: How AI\nbackground shapes perceptions of AI explanations. In Proceedings\nof the 2024 CHI conference on human factors in computing systems,\nNew York, NY, USA. Association for Computing Machinery.\n\n\nBryan Eikema and Wilker Aziz. 2020. Is\nMAP decoding all you need? The inadequacy of the mode in\nneural machine translation. In Donia Scott, Nuria Bel, and Chengqing\nZong, editors, Proceedings of the 28th international conference on\ncomputational linguistics, pages 4506–4520, Barcelona, Spain\n(Online). International Committee on Computational Linguistics.\n\n\nNelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom\nHenighan, Shauna Kravec, Zac Hatfield-Dodds, Robert Lasenby, Dawn Drain,\nCarol Chen, Roger Grosse, Sam McCandlish, Jared Kaplan, Dario Amodei,\nMartin Wattenberg, and Christopher Olah. 2022. Toy\nmodels of superposition. Transformer Circuits Thread.\n\n\nNelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas\nJoseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly,\nNova DasSarma, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny\nHernandez, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, et\nal. 2021. A mathematical framework for transformer circuits.\nTransformer Circuits Thread.\nhttps://transformer-circuits.pub/2021/framework/index.html.\n\n\nJoseph Enguehard. 2023. Sequential\nintegrated gradients: A simple but effective method for explaining\nlanguage models. In Anna Rogers, Jordan Boyd-Graber, and Naoaki\nOkazaki, editors, Findings of the association for computational\nlinguistics: ACL 2023, pages 7555–7565, Toronto, Canada.\nAssociation for Computational Linguistics.\n\n\nSugyeong Eo, Chanjun Park, Hyeonseok Moon, Jaehyung Seo, and Heuiseok\nLim. 2022. Word-level quality\nestimation for korean-english neural machine translation. IEEE\nAccess, 10:44964–44973.\n\n\nJohannes Eschbach-Dymanus, Frank Essenberger, Bianka Buschbeck, and\nMiriam Exel. 2024. Exploring the\neffectiveness of LLM domain adaptation for business\nIT machine translation. In Carolina Scarton, Charlotte\nPrescott, Chris Bayliss, Chris Oakley, Joanna Wright, Stuart Wrigley,\nXingyi Song, Edward Gow-Smith, Rachel Bawden, Víctor M\nSánchez-Cartagena, Patrick Cadwell, Ekaterina Lapshinova-Koltunski, Vera\nCabarrão, Konstantinos Chatzitheodorou, Mary Nurminen, Diptesh Kanojia,\nand Helena Moniz, editors, Proceedings of the 25th annual conference\nof the european association for machine translation (volume 1),\npages 610–622, Sheffield, UK. European Association for Machine\nTranslation (EAMT).\n\n\nEkaterina Fadeeva, Aleksandr Rubashevskii, Artem Shelmanov, Sergey\nPetrakov, Haonan Li, Hamdy Mubarak, Evgenii Tsymbalov, Gleb Kuzmin,\nAlexander Panchenko, Timothy Baldwin, Preslav Nakov, and Maxim Panov.\n2024. Fact-checking\nthe output of large language models via token-level uncertainty\nquantification. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar,\neditors, Findings of the association for computational linguistics:\nACL 2024, pages 9367–9385, Bangkok, Thailand. Association for\nComputational Linguistics.\n\n\nEkaterina Fadeeva, Roman Vashurin, Akim Tsvigun, Artem Vazhentsev,\nSergey Petrakov, Kirill Fedyanin, Daniil Vasilev, Elizaveta Goncharova,\nAlexander Panchenko, Maxim Panov, Timothy Baldwin, and Artem Shelmanov.\n2023. LM-polygraph:\nUncertainty estimation for language models. In Yansong Feng and Els\nLefever, editors, Proceedings of the 2023 conference on empirical\nmethods in natural language processing: System demonstrations,\npages 446–461, Singapore. Association for Computational Linguistics.\n\n\nAngela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky,\nSiddharth Goyal, Mandeep Baines, Onur Çelebi, Guillaume Wenzek, Vishrav\nChaudhary, Naman Goyal, Tom Birch, Vitaliy Liptchinsky, Sergey Edunov,\nEdouard Grave, Michael Auli, and Armand Joulin. 2021. Beyond english-centric\nmultilingual machine translation. Journal of Machine Learning\nResearch, 22(107):1–48.\n\n\nAngela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston,\nand Michael Auli. 2019. ELI5: Long form\nquestion answering. In Anna Korhonen, David Traum, and Lluís\nMàrquez, editors, Proceedings of the 57th annual meeting of the\nassociation for computational linguistics, pages 3558–3567,\nFlorence, Italy. Association for Computational Linguistics.\n\n\nAnna Farkas and Renáta Németh. 2022. How to measure gender\nbias in machine translation: Real-world oriented machine translators,\nmultiple reference points. Social Sciences & Humanities\nOpen, 5(1):100239.\n\n\nMarcello Federico, Nicola Bertoldi, Marco Trombetti, and Alessandro\nCattelan. 2014. MateCat:\nAn open source CAT tool for MT\npost-editing. In Proceedings of the 11th conference of the\nassociation for machine translation in the americas: tutorials,\nVancouver, Canada. Association for Machine Translation in the Americas.\n\n\nThomas Fel. 2024. Sparks\nof explainability: Recent advancements in explaining large vision\nmodels. PhD thesis, University of Toulouse.\n\n\nNils Feldhus, Robert Schwarzenberg, and Sebastian Möller. 2021. Thermostat: A\nlarge collection of NLP model explanations and analysis\ntools. In Heike Adel and Shuming Shi, editors, Proceedings of\nthe 2021 conference on empirical methods in natural language processing:\nSystem demonstrations, pages 87–95, Online; Punta Cana, Dominican\nRepublic. Association for Computational Linguistics.\n\n\nFangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Arivazhagan, and Wei\nWang. 2022. Language-agnostic\nBERT sentence embedding. In Smaranda Muresan, Preslav\nNakov, and Aline Villavicencio, editors, Proceedings of the 60th\nannual meeting of the association for computational linguistics (volume\n1: Long papers), pages 878–891, Dublin, Ireland. Association for\nComputational Linguistics.\n\n\nShi Feng, Eric Wallace, Alvin Grissom II, Mohit Iyyer, Pedro Rodriguez,\nand Jordan Boyd-Graber. 2018. Pathologies of neural models\nmake interpretations difficult. In Ellen Riloff, David Chiang, Julia\nHockenmaier, and Jun’ichi Tsujii, editors, Proceedings of the 2018\nconference on empirical methods in natural language processing,\npages 3719–3728, Brussels, Belgium. Association for Computational\nLinguistics.\n\n\nPatrick Fernandes, Daniel Deutsch, Mara Finkelstein, Parker Riley, André\nMartins, Graham Neubig, Ankush Garg, Jonathan Clark, Markus Freitag, and\nOrhan Firat. 2023a. The devil is in the\nerrors: Leveraging large language models for fine-grained machine\ntranslation evaluation. In Philipp Koehn, Barry Haddow, Tom Kocmi,\nand Christof Monz, editors, Proceedings of the eighth conference on\nmachine translation, pages 1066–1083, Singapore. Association for\nComputational Linguistics.\n\n\nPatrick Fernandes, António Farinhas, Ricardo Rei, José G. C. de Souza,\nPerez Ogayo, Graham Neubig, and Andre Martins. 2022. Quality-aware\ndecoding for neural machine translation. In Marine Carpuat,\nMarie-Catherine de Marneffe, and Ivan Vladimir Meza Ruiz, editors,\nProceedings of the 2022 conference of the north american chapter of\nthe association for computational linguistics: Human language\ntechnologies, pages 1396–1412, Seattle, United States. Association\nfor Computational Linguistics.\n\n\nPatrick Fernandes, Kayo Yin, Emmy Liu, André Martins, and Graham Neubig.\n2023b. When does\ntranslation require context? A data-driven, multilingual\nexploration. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki,\neditors, Proceedings of the 61st annual meeting of the association\nfor computational linguistics (volume 1: Long papers), pages\n606–626, Toronto, Canada. Association for Computational Linguistics.\n\n\nPatrick Fernandes, Kayo Yin, Graham Neubig, and André F. T. Martins.\n2021. Measuring\nand increasing context usage in context-aware machine translation.\nIn Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors,\nProceedings of the 59th annual meeting of the association for\ncomputational linguistics and the 11th international joint conference on\nnatural language processing (volume 1: Long papers), pages\n6467–6478, Online. Association for Computational Linguistics.\n\n\nJavier Ferrando, Gerard I. Gállego, Belen Alastruey, Carlos Escolano,\nand Marta R. Costa-jussà. 2022a. Towards opening\nthe black box of neural machine translation: Source and target\ninterpretations of the transformer. In Yoav Goldberg, Zornitsa\nKozareva, and Yue Zhang, editors, Proceedings of the 2022 conference\non empirical methods in natural language processing, pages\n8756–8769, Abu Dhabi, United Arab Emirates. Association for\nComputational Linguistics.\n\n\nJavier Ferrando, Gerard I. Gállego, and Marta R. Costa-jussà. 2022b. Measuring the\nmixing of contextual information in the transformer. In Yoav\nGoldberg, Zornitsa Kozareva, and Yue Zhang, editors, Proceedings of\nthe 2022 conference on empirical methods in natural language\nprocessing, pages 8698–8714, Abu Dhabi, United Arab Emirates.\nAssociation for Computational Linguistics.\n\n\nJavier Ferrando, Gerard I. Gállego, Ioannis Tsiamas, and Marta R.\nCosta-jussà. 2023. Explaining how\ntransformers use context to build predictions. In Anna Rogers,\nJordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the\n61st annual meeting of the association for computational linguistics\n(volume 1: Long papers), pages 5486–5513, Toronto, Canada.\nAssociation for Computational Linguistics.\n\n\nJavier Ferrando, Oscar Balcells Obeso, Senthooran Rajamanoharan, and\nNeel Nanda. 2025. Do i know this entity?\nKnowledge awareness and hallucinations in language models. In\nThe thirteenth international conference on learning\nrepresentations.\n\n\nJavier Ferrando, Gabriele Sarti, Arianna Bisazza, and Marta R.\nCosta-jussà. 2024. A primer\non the inner workings of transformer-based language models.\nArxiv Preprint.\n\n\nJaden Fried Fiotto-Kaufman, Alexander Russell Loftus, Eric Todd, Jannik\nBrinkmann, Koyena Pal, Dmitrii Troitskii, Michael Ripa, Adam Belfki, Can\nRager, Caden Juang, Aaron Mueller, Samuel Marks, Arnab Sen Sharma,\nFrancesca Lucchetti, Nikhil Prakash, Carla E. Brodley, Arjun Guha,\nJonathan Bell, Byron C Wallace, et al. 2025. NNsight\nand NDIF: Democratizing access to open-weight foundation\nmodel internals. In The thirteenth international conference on\nlearning representations.\n\n\nLucie Flekova, Jordan Carpenter, Salvatore Giorgi, Lyle Ungar, and\nDaniel Preoţiuc-Pietro. 2016. Analyzing biases in human\nperception of user age and gender from text. In Katrin Erk and Noah\nA. Smith, editors, Proceedings of the 54th annual meeting of the\nassociation for computational linguistics (volume 1: Long papers),\npages 843–854, Berlin, Germany. Association for Computational\nLinguistics.\n\n\nMarina Fomicheva, Piyawat Lertvittayakumjorn, Wei Zhao, Steffen Eger,\nand Yang Gao. 2021. The\nEval4NLP shared task on explainable quality\nestimation: Overview and results. In Yang Gao, Steffen Eger, Wei\nZhao, Piyawat Lertvittayakumjorn, and Marina Fomicheva, editors,\nProceedings of the 2nd workshop on evaluation and comparison of NLP\nsystems, pages 165–178, Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\n\n\nMarina Fomicheva and Lucia Specia. 2019. Taking MT\nevaluation metrics to extremes: Beyond correlation with human\njudgments. Computational Linguistics, 45(3):515–558.\n\n\nMarina Fomicheva, Lucia Specia, and Nikolaos Aletras. 2022a. Translation\nerror detection as rationale extraction. In Smaranda Muresan,\nPreslav Nakov, and Aline Villavicencio, editors, Findings of the\nassociation for computational linguistics: ACL 2022, pages\n4148–4159, Dublin, Ireland. Association for Computational Linguistics.\n\n\nMarina Fomicheva, Shuo Sun, Erick Fonseca, Chrysoula Zerva, Frédéric\nBlain, Vishrav Chaudhary, Francisco Guzmán, Nina Lopatina, Lucia Specia,\nand André F. T. Martins. 2022b. MLQE-PE:\nA multilingual quality estimation and post-editing dataset. In\nNicoletta Calzolari, Frédéric Béchet, Philippe Blache, Khalid Choukri,\nChristopher Cieri, Thierry Declerck, Sara Goggi, Hitoshi Isahara, Bente\nMaegaard, Joseph Mariani, Hélène Mazo, Jan Odijk, and Stelios Piperidis,\neditors, Proceedings of the thirteenth language resources and\nevaluation conference, pages 4963–4974, Marseille, France. European\nLanguage Resources Association.\n\n\nMarina Fomicheva, Shuo Sun, Lisa Yankovskaya, Frédéric Blain, Francisco\nGuzmán, Mark Fishel, Nikolaos Aletras, Vishrav Chaudhary, and Lucia\nSpecia. 2020. Unsupervised quality\nestimation for neural machine translation. Transactions of the\nAssociation for Computational Linguistics, 8:539–555.\n\n\nMarkus Freitag, George Foster, David Grangier, Viresh Ratnakar, Qijun\nTan, and Wolfgang Macherey. 2021a. Experts, errors, and\ncontext: A large-scale study of human evaluation for machine\ntranslation. Transactions of the Association for Computational\nLinguistics, 9:1460–1474.\n\n\nMarkus Freitag, Nitika Mathur, Daniel Deutsch, Chi-Kiu Lo, Eleftherios\nAvramidis, Ricardo Rei, Brian Thompson, Frederic Blain, Tom Kocmi, Jiayi\nWang, David Ifeoluwa Adelani, Marianna Buchicchio, Chrysoula Zerva, and\nAlon Lavie. 2024. Are\nLLMs breaking MT metrics? Results of the\nWMT24 metrics shared task. In Barry Haddow, Tom Kocmi,\nPhilipp Koehn, and Christof Monz, editors, Proceedings of the ninth\nconference on machine translation, pages 47–81, Miami, Florida,\nUSA. Association for Computational Linguistics.\n\n\nMarkus Freitag, Nitika Mathur, Chi-kiu Lo, Eleftherios Avramidis,\nRicardo Rei, Brian Thompson, Tom Kocmi, Frederic Blain, Daniel Deutsch,\nCraig Stewart, Chrysoula Zerva, Sheila Castilho, Alon Lavie, and George\nFoster. 2023. Results of\nWMT23 metrics shared task: Metrics might be guilty but\nreferences are not innocent. In Philipp Koehn, Barry Haddow, Tom\nKocmi, and Christof Monz, editors, Proceedings of the eighth\nconference on machine translation, pages 578–628, Singapore.\nAssociation for Computational Linguistics.\n\n\nMarkus Freitag, Ricardo Rei, Nitika Mathur, Chi-kiu Lo, Craig Stewart,\nEleftherios Avramidis, Tom Kocmi, George Foster, Alon Lavie, and André\nF. T. Martins. 2022. Results of\nWMT22 metrics shared task: Stop using BLEU\n– neural metrics are better and more robust. In Philipp\nKoehn, Loïc Barrault, Ondřej Bojar, Fethi Bougares, Rajen Chatterjee,\nMarta R. Costa-jussà, Christian Federmann, Mark Fishel, Alexander\nFraser, Markus Freitag, Yvette Graham, Roman Grundkiewicz, Paco Guzman,\nBarry Haddow, Matthias Huck, Antonio Jimeno Yepes, Tom Kocmi, André\nMartins, Makoto Morishita, et al., editors, Proceedings of the\nseventh conference on machine translation (WMT), pages 46–68, Abu\nDhabi, United Arab Emirates (Hybrid). Association for Computational\nLinguistics.\n\n\nMarkus Freitag, Ricardo Rei, Nitika Mathur, Chi-kiu Lo, Craig Stewart,\nGeorge Foster, Alon Lavie, and Ondřej Bojar. 2021b. Results of the\nWMT21 metrics shared task: Evaluating metrics with\nexpert-based human evaluations on TED and news domain.\nIn Loic Barrault, Ondrej Bojar, Fethi Bougares, Rajen Chatterjee, Marta\nR. Costa-jussa, Christian Federmann, Mark Fishel, Alexander Fraser,\nMarkus Freitag, Yvette Graham, Roman Grundkiewicz, Paco Guzman, Barry\nHaddow, Matthias Huck, Antonio Jimeno Yepes, Philipp Koehn, Tom Kocmi,\nAndre Martins, Makoto Morishita, et al., editors, Proceedings of the\nsixth conference on machine translation, pages 733–774, Online.\nAssociation for Computational Linguistics.\n\n\nYarin Gal and Zoubin Ghahramani. 2016. Dropout as a\nbayesian approximation: Representing model uncertainty in deep\nlearning. In Maria Florina Balcan and Kilian Q. Weinberger, editors,\nProceedings of the 33rd international conference on machine\nlearning, volume 48, pages 1050–1059, New York, NY, USA.\nProceedings of Machine Learning Research (PLMR).\n\n\nLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe,\nCharles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima,\nShawn Presser, and Connor Leahy. 2021. The pile: An 800GB dataset of\ndiverse text for language modeling. Arxiv.\n\n\nTianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen. 2023a. Enabling large\nlanguage models to generate text with citations. In Houda Bouamor,\nJuan Pino, and Kalika Bali, editors, Proceedings of the 2023\nconference on empirical methods in natural language processing,\npages 6465–6488, Singapore. Association for Computational Linguistics.\n\n\nYunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi\nDai, Jiawei Sun, and Haofen Wang. 2023b. Retrieval-augmented generation\nfor large language models: A survey. ArXiv.\n\n\nIgnacio Garcia. 2009. Beyond translation\nmemory: Computers and the professional translator. The Journal\nof Specialised Translation.\n\n\nXavier Garcia, Noah Constant, Mandy Guo, and Orhan Firat. 2021. Towards universality in\nmultilingual text rewriting. Arxiv.\n\n\nXavier Garcia and Orhan Firat. 2022. Using natural language prompts\nfor machine translation. Arxiv.\n\n\nXiao Ge, Chunchen Xu, Daigo Misaki, Hazel Rose Markus, and Jeanne L\nTsai. 2024. How\nculture shapes what people want from AI. In Proceedings of the\n2024 CHI conference on human factors in computing systems, New\nYork, NY, USA. Association for Computing Machinery.\n\n\nViveta Gene. 2021. The post-editing\nworkflow: Training challenges for LSPs, post-editors and\nacademia. In Ruslan Mitkov, Vilelmini Sosoni, Julie Christine\nGiguère, Elena Murgolo, and Elizabeth Deysel, editors, Proceedings\nof the translation and interpreting technology online conference,\npages 187–198, Held Online. INCOMA Ltd.\n\n\nDaniela Gerz, Ivan Vulić, Edoardo Maria Ponti, Roi Reichart, and Anna\nKorhonen. 2018. On the\nrelation between linguistic typology and (limitations of) multilingual\nlanguage modeling. In Ellen Riloff, David Chiang, Julia Hockenmaier,\nand Jun’ichi Tsujii, editors, Proceedings of the 2018 conference on\nempirical methods in natural language processing, pages 316–327,\nBrussels, Belgium. Association for Computational Linguistics.\n\n\nMor Geva, Avi Caciularu, Kevin Wang, and Yoav Goldberg. 2022. Transformer\nfeed-forward layers build predictions by promoting concepts in the\nvocabulary space. In Yoav Goldberg, Zornitsa Kozareva, and Yue\nZhang, editors, Proceedings of the 2022 conference on empirical\nmethods in natural language processing, pages 30–45, Abu Dhabi,\nUnited Arab Emirates. Association for Computational Linguistics.\n\n\nMor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. 2021. Transformer\nfeed-forward layers are key-value memories. In Marie-Francine Moens,\nXuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors,\nProceedings of the 2021 conference on empirical methods in natural\nlanguage processing, pages 5484–5495, Online; Punta Cana, Dominican\nRepublic. Association for Computational Linguistics.\n\n\nMario Giulianelli, Joris Baan, Wilker Aziz, Raquel Fernández, and\nBarbara Plank. 2023. What comes next?\nEvaluating uncertainty in neural text generators against human\nproduction variability. In Houda Bouamor, Juan Pino, and Kalika\nBali, editors, Proceedings of the 2023 conference on empirical\nmethods in natural language processing, pages 14349–14371,\nSingapore. Association for Computational Linguistics.\n\n\nIan Goodfellow, Yoshua Bengio, and Aaron Courville. 2016. Deep learning. MIT\nPress.\n\n\nNaman Goyal, Jingfei Du, Myle Ott, Giri Anantharaman, and Alexis\nConneau. 2021. Larger-scale\ntransformers for multilingual masked language modeling. In Anna\nRogers, Iacer Calixto, Ivan Vulić, Naomi Saphra, Nora Kassner,\nOana-Maria Camburu, Trapit Bansal, and Vered Shwartz, editors,\nProceedings of the 6th workshop on representation learning for NLP\n(RepL4NLP-2021), pages 29–33, Online. Association for Computational\nLinguistics.\n\n\nNaman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-Jen Chen, Guillaume\nWenzek, Da Ju, Sanjana Krishnan, Marc’Aurelio Ranzato, Francisco Guzmán,\nand Angela Fan. 2022. The\nFlores-101 evaluation benchmark for low-resource and\nmultilingual machine translation. Transactions of the\nAssociation for Computational Linguistics, 10:522–538.\n\n\nTanya Goyal and Greg Durrett. 2021. Annotating and\nmodeling fine-grained factuality in summarization. In Kristina\nToutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz\nBeltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao\nZhou, editors, Proceedings of the 2021 conference of the north\namerican chapter of the association for computational linguistics: Human\nlanguage technologies, pages 1449–1462, Online. Association for\nComputational Linguistics.\n\n\nYvette Graham, Timothy Baldwin, Alistair Moffat, and Justin Zobel. 2013.\nContinuous measurement\nscales in human evaluation of machine translation. In Antonio\nPareja-Lora, Maria Liakata, and Stefanie Dipper, editors,\nProceedings of the 7th linguistic annotation workshop and\ninteroperability with discourse, pages 33–41, Sofia, Bulgaria.\nAssociation for Computational Linguistics.\n\n\nSpence Green, Jeffrey Heer, and Christopher D. Manning. 2013. The efficacy of human\npost-editing for language translation. In Proceedings of the\nSIGCHI Conference on Human\nFactors in Computing\nSystems, pages 439–448, New York, NY, USA. Association\nfor Computing Machinery.\n\n\nAna Guerberof. 2009. Productivity and\nquality in MT post-editing. In Beyond translation\nmemories: New tools for translators workshop, Ottawa, Canada.\n\n\nAna Guerberof-Arenas and Joss Moorkens. 2023. Ethics and machine\ntranslation: The end user perspective. In Towards responsible\nmachine translation: Ethical and legal considerations in machine\ntranslation, pages 113–133. Springer International Publishing,\nCham.\n\n\nAna Guerberof-Arenas and Antonio Toral. 2022. Creativity in translation:\nMachine translation as a constraint for literary texts.\nTranslation Spaces, 11(2):184–212.\n\n\nNuno M. Guerreiro, Pierre Colombo, Pablo Piantanida, and André Martins.\n2023a. Optimal\ntransport for unsupervised hallucination detection in neural machine\ntranslation. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki,\neditors, Proceedings of the 61st annual meeting of the association\nfor computational linguistics (volume 1: Long papers), pages\n13766–13784, Toronto, Canada. Association for Computational Linguistics.\n\n\nNuno M. Guerreiro, Ricardo Rei, Daan van Stigt, Luisa Coheur, Pierre\nColombo, and André F. T. Martins. 2024. Xcomet: Transparent machine\ntranslation evaluation through fine-grained error detection.\nTransactions of the Association for Computational Linguistics,\n12:979–995.\n\n\nNuno M. Guerreiro, Elena Voita, and André Martins. 2023b. Looking for a\nneedle in a haystack: A comprehensive study of hallucinations in neural\nmachine translation. In Andreas Vlachos and Isabelle Augenstein,\neditors, Proceedings of the 17th conference of the european chapter\nof the association for computational linguistics, pages 1059–1075,\nDubrovnik, Croatia. Association for Computational Linguistics.\n\n\nAbhijeet Gupta, Gemma Boleda, Marco Baroni, and Sebastian Padó. 2015. Distributional vectors\nencode referential attributes. In Lluís Màrquez, Chris\nCallison-Burch, and Jian Su, editors, Proceedings of the 2015\nconference on empirical methods in natural language processing,\npages 12–21, Lisbon, Portugal. Association for Computational\nLinguistics.\n\n\nChristian Hadiwinoto. 2017. Book review: Syntax-based\nstatistical machine translation by philip Williams, rico\nSennrich, matt post and philipp Koehn.\nComputational Linguistics, 43(4):893–896.\n\n\nZellig S. Harris. 1954. Distributional\nstructure. Word, 10(2-3):146–162.\n\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for\nimage recognition. In 2016 IEEE conference on computer vision\nand pattern recognition (CVPR), pages 770–778, Los Alamitos, CA,\nUSA. IEEE Computer Society.\n\n\nPengcheng He, Jianfeng Gao, and Weizhu Chen. 2023. DeBERTaV3:\nImproving DeBERTa using ELECTRA-style\npre-training with gradient-disentangled embedding sharing. In\nProceedings of the 11th international conference on learning\nrepresentations.\n\n\nRoee Hendel, Mor Geva, and Amir Globerson. 2023. In-context\nlearning creates task vectors. In Houda Bouamor, Juan Pino, and\nKalika Bali, editors, Findings of the association for computational\nlinguistics: EMNLP 2023, pages 9318–9333, Singapore. Association\nfor Computational Linguistics.\n\n\nDan Hendrycks and Kevin Gimpel. 2017. A baseline for detecting\nmisclassified and out-of-distribution examples in neural networks.\nIn International conference on learning representations (ICLR\n2017).\n\n\nDan Hendrycks and Laura Hiscott. 2025. The\nmisguided quest for mechanistic AI interpretability. Accessed August\n4, 2025.\n\n\nNico Herbig, Tim Düwel, Santanu Pal, Kalliopi Meladaki, Mahsa\nMonshizadeh, Antonio Krüger, and Josef van Genabith. 2020. MMPE:\nA Multi-Modal\nInterface for Post-Editing\nMachine Translation. In Dan Jurafsky,\nJoyce Chai, Natalie Schluter, and Joel Tetreault, editors,\nProceedings of the 58th annual meeting of the association for\ncomputational linguistics, pages 1691–1702, Online. Association for\nComputational Linguistics.\n\n\nAnas Himmi, Guillaume Staerman, Marine Picot, Pierre Colombo, and Nuno M\nGuerreiro. 2024. Enhanced\nhallucination detection in neural machine translation through simple\ndetector aggregation. In Yaser Al-Onaizan, Mohit Bansal, and\nYun-Nung Chen, editors, Proceedings of the 2024 conference on\nempirical methods in natural language processing, pages\n18573–18583, Miami, Florida, USA. Association for Computational\nLinguistics.\n\n\nSepp Hochreiter. 1998. The vanishing gradient\nproblem during learning recurrent neural nets and problem solutions.\nInt. J. Uncertain. Fuzziness Knowl.-Based Syst., 6(2):107–116.\n\n\nSepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory.\nNeural computation, 9(8):1735–1780.\n\n\nAri Holtzman, Peter West, Vered Shwartz, Yejin Choi, and Luke\nZettlemoyer. 2021. Surface form\ncompetition: Why the highest probability answer isn‘t always right.\nIn Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau\nYih, editors, Proceedings of the 2021 conference on empirical\nmethods in natural language processing, pages 7038–7051, Online;\nPunta Cana, Dominican Republic. Association for Computational\nLinguistics.\n\n\nOr Honovich, Roee Aharoni, Jonathan Herzig, Hagai Taitelbaum, Doron\nKukliansy, Vered Cohen, Thomas Scialom, Idan Szpektor, Avinatan\nHassidim, and Yossi Matias. 2022. TRUE:\nRe-evaluating factual consistency evaluation. In Marine Carpuat,\nMarie-Catherine de Marneffe, and Ivan Vladimir Meza Ruiz, editors,\nProceedings of the 2022 conference of the north american chapter of\nthe association for computational linguistics: Human language\ntechnologies, pages 3905–3920, Seattle, United States. Association\nfor Computational Linguistics.\n\n\nJeremy Howard and Sebastian Ruder. 2018. Universal language model\nfine-tuning for text classification. In Iryna Gurevych and Yusuke\nMiyao, editors, Proceedings of the 56th annual meeting of the\nassociation for computational linguistics (volume 1: Long papers),\npages 328–339, Melbourne, Australia. Association for Computational\nLinguistics.\n\n\nJing Huang, Atticus Geiger, Karel D’Oosterlinck, Zhengxuan Wu, and\nChristopher Potts. 2023. Rigorously\nassessing natural language explanations of neurons. In Yonatan\nBelinkov, Sophie Hao, Jaap Jumelet, Najoung Kim, Arya McCarthy, and\nHosein Mohebbi, editors, Proceedings of the 6th BlackboxNLP\nworkshop: Analyzing and interpreting neural networks for NLP, pages\n317–331, Singapore. Association for Computational Linguistics.\n\n\nLianzhe Huang, Shuming Ma, Dongdong Zhang, Furu Wei, and Houfeng Wang.\n2022. Zero-shot\ncross-lingual transfer of prompt-based tuning with a unified\nmultilingual prompt. In Yoav Goldberg, Zornitsa Kozareva, and Yue\nZhang, editors, Proceedings of the 2022 conference on empirical\nmethods in natural language processing, pages 11488–11497, Abu\nDhabi, United Arab Emirates. Association for Computational Linguistics.\n\n\nRobert Huben, Hoagy Cunningham, Logan Riggs Smith, Aidan Ewart, and Lee\nSharkey. 2024. Sparse autoencoders\nfind highly interpretable features in language models. In The\ntwelfth international conference on learning representations.\n\n\nWilliam J. Hutchins. 2001. Machine translation over\nfifty years. Histoire Épistémologie Langage, 23:7–31.\n\n\nKhondoker Ittehadul Islam and Gabriele Sarti. 2025. Reveal-bangla: A dataset for\ncross-lingual multi-step reasoning evaluation. Arxiv\nPreprint.\n\n\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio\nPetroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel,\nand Edouard Grave. 2023. Atlas: Few-shot learning\nwith retrieval augmented language models. Journal of Machine\nLearning Research, 24(251):1–43.\n\n\nAlon Jacovi and Yoav Goldberg. 2020. Towards faithfully\ninterpretable NLP systems: How should we define and\nevaluate faithfulness? In Dan Jurafsky, Joyce Chai, Natalie\nSchluter, and Joel Tetreault, editors, Proceedings of the 58th\nannual meeting of the association for computational linguistics,\npages 4198–4205, Online. Association for Computational Linguistics.\n\n\nSarthak Jain and Byron C. Wallace. 2019. Attention is\nnot Explanation. In Jill Burstein, Christy Doran, and\nThamar Solorio, editors, Proceedings of the 2019 conference of the\nnorth American chapter of the association for computational\nlinguistics: Human language technologies, volume 1 (long and short\npapers), pages 3543–3556, Minneapolis, Minnesota. Association for\nComputational Linguistics.\n\n\nStanisław Jastrzebski, Devansh Arpit, Nicolas Ballas, Vikas Verma, Tong\nChe, and Yoshua Bengio. 2018. Residual connections\nencourage iterative inference. In International conference on\nlearning representations.\n\n\nFran Jelenić, Josip Jukić, Martin Tutek, Mate Puljiz, and Jan Snajder.\n2024. Out-of-distribution\ndetection by leveraging between-layer transformation smoothness. In\nThe twelfth international conference on learning\nrepresentations.\n\n\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,\nDevendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna\nLengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud,\nMarie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas\nWang, Timothée Lacroix, and William El Sayed. 2023. Mistral 7B.\n\n\nZhengbao Jiang, Frank F. Xu, Jun Araki, and Graham Neubig. 2020. How can we know what\nlanguage models know? Transactions of the Association for\nComputational Linguistics, 8:423–438.\n\n\nDi Jin, Zhijing Jin, Zhiting Hu, Olga Vechtomova, and Rada Mihalcea.\n2022. Deep learning for\ntext style transfer: A survey. Computational Linguistics,\n48(1):155–205.\n\n\nLinghao Jin, Jacqueline He, Jonathan May, and Xuezhe Ma. 2023. Challenges in\ncontext-aware neural machine translation. In Houda Bouamor, Juan\nPino, and Kalika Bali, editors, Proceedings of the 2023 conference\non empirical methods in natural language processing, pages\n15246–15263, Singapore. Association for Computational Linguistics.\n\n\nNal Kalchbrenner and Phil Blunsom. 2013. Recurrent continuous\ntranslation models. In David Yarowsky, Timothy Baldwin, Anna\nKorhonen, Karen Livescu, and Steven Bethard, editors, Proceedings of\nthe 2013 conference on empirical methods in natural language\nprocessing, pages 1700–1709, Seattle, Washington, USA. Association\nfor Computational Linguistics.\n\n\nJared Kaplan, Sam McCandlish, T. J. Henighan, Tom B. Brown, Benjamin\nChess, Rewon Child, Scott Gray, Alec Radford, Jeff Wu, and Dario Amodei.\n2020. Scaling laws for neural\nlanguage models. ArXiv.\n\n\nSariya Karimova, Patrick Simianer, and Stefan Riezler. 2018. A user-study on online\nadaptation of neural machine translation to human post-edits.\nMachine Translation, 32(4):309–324.\n\n\nMarzena Karpinska and Mohit Iyyer. 2023. Large language models\neffectively leverage document-level context for literary translation,\nbut critical errors persist. In Philipp Koehn, Barry Haddow, Tom\nKocmi, and Christof Monz, editors, Proceedings of the eighth\nconference on machine translation, pages 419–451, Singapore.\nAssociation for Computational Linguistics.\n\n\nFabio Kepler, Jonay Trénous, Marcos Treviso, Miguel Vera, and André F.\nT. Martins. 2019. OpenKiwi:\nAn open source framework for quality estimation. In Marta R.\nCosta-jussà and Enrique Alfonseca, editors, Proceedings of the 57th\nannual meeting of the association for computational linguistics: System\ndemonstrations, pages 117–122, Florence, Italy. Association for\nComputational Linguistics.\n\n\nYunsu Kim, Petre Petrov, Pavel Petrushkov, Shahram Khadivi, and Hermann\nNey. 2019a. Pivot-based\ntransfer learning for neural machine translation between\nnon-English languages. In Kentaro Inui, Jing Jiang,\nVincent Ng, and Xiaojun Wan, editors, Proceedings of the 2019\nconference on empirical methods in natural language processing and the\n9th international joint conference on natural language processing\n(EMNLP-IJCNLP), pages 866–876, Hong Kong, China. Association for\nComputational Linguistics.\n\n\nYunsu Kim, Duc Thanh Tran, and Hermann Ney. 2019b. When and why is\ndocument-level context useful in neural machine translation? In\nAndrei Popescu-Belis, Sharid Loáiciga, Christian Hardmeier, and Deyi\nXiong, editors, Proceedings of the fourth workshop on discourse in\nmachine translation (DiscoMT 2019), pages 24–34, Hong Kong, China.\nAssociation for Computational Linguistics.\n\n\nArmen Der Kiureghian and Ove Ditlevsen. 2009. Aleatory or\nepistemic? Does it matter? Structural Safety,\n31(2):105–112. Risk Acceptance and Risk Communication.\n\n\nGoro Kobayashi, Tatsuki Kuribayashi, Sho Yokoi, and Kentaro Inui. 2020.\nAttention is\nnot only a weight: Analyzing transformers with vector norms. In\nBonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors,\nProceedings of the 2020 conference on empirical methods in natural\nlanguage processing (EMNLP), pages 7057–7075, Online. Association\nfor Computational Linguistics.\n\n\nGoro Kobayashi, Tatsuki Kuribayashi, Sho Yokoi, and Kentaro Inui. 2021.\nIncorporating\nResidual and Normalization Layers\ninto Analysis of Masked Language\nModels. In Marie-Francine Moens, Xuanjing Huang, Lucia\nSpecia, and Scott Wen-tau Yih, editors, Proceedings of the 2021\nconference on empirical methods in natural language processing,\npages 4547–4568, Online; Punta Cana, Dominican Republic. Association for\nComputational Linguistics.\n\n\nTom Kocmi, Eleftherios Avramidis, Rachel Bawden, Ondřej Bojar, Anton\nDvorkovich, Christian Federmann, Mark Fishel, Markus Freitag, Thamme\nGowda, Roman Grundkiewicz, Barry Haddow, Marzena Karpinska, Philipp\nKoehn, Benjamin Marie, Christof Monz, Kenton Murray, Masaaki Nagata,\nMartin Popel, Maja Popović, et al. 2024a. Findings of the\nWMT24 general machine translation shared task: The\nLLM era is here but MT is not solved yet.\nIn Barry Haddow, Tom Kocmi, Philipp Koehn, and Christof Monz, editors,\nProceedings of the ninth conference on machine translation,\npages 1–46, Miami, Florida, USA. Association for Computational\nLinguistics.\n\n\nTom Kocmi, Eleftherios Avramidis, Rachel Bawden, Ondřej Bojar, Anton\nDvorkovich, Christian Federmann, Mark Fishel, Markus Freitag, Thamme\nGowda, Roman Grundkiewicz, Barry Haddow, Philipp Koehn, Benjamin Marie,\nChristof Monz, Makoto Morishita, Kenton Murray, Masaaki Nagata, Toshiaki\nNakazawa, Martin Popel, et al. 2023. Findings of the 2023\nconference on machine translation (WMT23):\nLLMs are here but not quite there yet. In Philipp\nKoehn, Barry Haddow, Tom Kocmi, and Christof Monz, editors,\nProceedings of the eighth conference on machine translation,\npages 1–42, Singapore. Association for Computational Linguistics.\n\n\nTom Kocmi and Christian Federmann. 2023a. GEMBA-MQM:\nDetecting translation quality error spans with GPT-4.\nIn Philipp Koehn, Barry Haddow, Tom Kocmi, and Christof Monz, editors,\nProceedings of the eighth conference on machine translation,\npages 768–775, Singapore. Association for Computational Linguistics.\n\n\nTom Kocmi and Christian Federmann. 2023b. Large language models\nare state-of-the-art evaluators of translation quality. In Mary\nNurminen, Judith Brenner, Maarit Koponen, Sirkku Latomaa, Mikhail\nMikhailov, Frederike Schierl, Tharindu Ranasinghe, Eva Vanmassenhove,\nSergi Alvarez Vidal, Nora Aranberri, Mara Nunziatini, Carla Parra\nEscartín, Mikel Forcada, Maja Popovic, Carolina Scarton, and Helena\nMoniz, editors, Proceedings of the 24th annual conference of the\neuropean association for machine translation, pages 193–203,\nTampere, Finland. European Association for Machine Translation.\n\n\nTom Kocmi, Vilém Zouhar, Eleftherios Avramidis, Roman Grundkiewicz,\nMarzena Karpinska, Maja Popović, Mrinmaya Sachan, and Mariya Shmatova.\n2024b. Error span\nannotation: A balanced approach for human evaluation of machine\ntranslation. In Barry Haddow, Tom Kocmi, Philipp Koehn, and Christof\nMonz, editors, Proceedings of the ninth conference on machine\ntranslation, pages 1440–1453, Miami, Florida, USA. Association for\nComputational Linguistics.\n\n\nPhilipp Koehn. 2005. Europarl:\nA parallel corpus for statistical machine translation. In\nProceedings of machine translation summit x: papers, pages\n79–86, Phuket, Thailand.\n\n\nPhilipp Koehn, Franz J. Och, and Daniel Marcu. 2003. Statistical phrase-based\ntranslation. In Proceedings of the 2003 human language\ntechnology conference of the north American chapter of the\nassociation for computational linguistics, pages 127–133.\n\n\nArne Köhn. 2015. What‘s\nin an embedding? Analyzing word embeddings through multilingual\nevaluation. In Lluís Màrquez, Chris Callison-Burch, and Jian Su,\neditors, Proceedings of the 2015 conference on empirical methods in\nnatural language processing, pages 2067–2073, Lisbon, Portugal.\nAssociation for Computational Linguistics.\n\n\nNarine Kokhlikyan, Vivek Miglani, Miguel Martin, Edward Wang, Bilal\nAlsallakh, Jonathan Reynolds, Alexander Melnikov, Natalia Kliushkina,\nCarlos Araya, Siqi Yan, and Orion Reblitz-Richardson. 2020. Captum: A unified and generic\nmodel interpretability library for PyTorch. ArXiv.\n\n\nMaarit Koponen, Wilker Aziz, Luciana Ramos, and Lucia Specia. 2012. Post-editing\ntime as a measure of cognitive effort. In Workshop on\npost-editing technology and practice.\n\n\nMaarit Koponen, Umut Sulubacak, Kaisa Vitikainen, and Jörg Tiedemann.\n2020. MT\nfor subtitling: User evaluation of post-editing productivity. In\nAndré Martins, Helena Moniz, Sara Fumega, Bruno Martins, Fernando\nBatista, Luisa Coheur, Carla Parra, Isabel Trancoso, Marco Turchi,\nArianna Bisazza, Joss Moorkens, Ana Guerberof, Mary Nurminen, Lena Marg,\nand Mikel L. Forcada, editors, Proceedings of the 22nd annual\nconference of the european association for machine translation,\npages 115–124, Lisboa, Portugal. European Association for Machine\nTranslation.\n\n\nHans P. Krings. 2001. Repairing texts: Empirical investigations of\nmachine translation post-editing processes. Kent State University\nPress.\n\n\nKalpesh Krishna, Deepak Nathani, Xavier Garcia, Bidisha Samanta, and\nPartha Talukdar. 2022. Few-shot\ncontrollable style transfer for low-resource multilingual settings.\nIn Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors,\nProceedings of the 60th annual meeting of the association for\ncomputational linguistics (volume 1: Long papers), pages 7439–7468,\nDublin, Ireland. Association for Computational Linguistics.\n\n\nKalpesh Krishna, Aurko Roy, and Mohit Iyyer. 2021. Hurdles to\nprogress in long-form question answering. In Kristina Toutanova,\nAnna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven\nBethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou, editors,\nProceedings of the 2021 conference of the north american chapter of\nthe association for computational linguistics: Human language\ntechnologies, pages 4940–4957, Online. Association for\nComputational Linguistics.\n\n\nSatyapriya Krishna, Tessa Han, Alex Gu, Steven Wu, Shahin Jabbari, and\nHimabindu Lakkaraju. 2024. The disagreement\nproblem in explainable machine learning: A practitioner’s\nperspective. Transactions on Machine Learning Research.\n\n\nWojciech Kryscinski, Bryan McCann, Caiming Xiong, and Richard Socher.\n2020. Evaluating the\nfactual consistency of abstractive text summarization. In Bonnie\nWebber, Trevor Cohn, Yulan He, and Yang Liu, editors, Proceedings of\nthe 2020 conference on empirical methods in natural language processing\n(EMNLP), pages 9332–9346, Online. Association for Computational\nLinguistics.\n\n\nSolomon Kullback and Richard A Leibler. 1951. On information and\nsufficiency. The annals of mathematical statistics,\n22(1):79–86.\n\n\nIsabel Lacruz, Michael Denkowski, and Alon Lavie. 2014. Cognitive demand and\ncognitive effort in post-editing. In Sharon O’Brien, Michel Simard,\nand Lucia Specia, editors, Proceedings of the 11th conference of the\nassociation for machine translation in the americas, pages 73–84,\nVancouver, Canada. Association for Machine Translation in the Americas.\n\n\nIsaac Lage, Emily Chen, Jeffrey He, Menaka Narayanan, Been Kim, Sam\nGershman, and Finale Doshi-Velez. 2019. An evaluation of the\nhuman-interpretability of explanation. ArXiv,\nabs/1902.00006.\n\n\nHuiyuan Lai, Jiali Mao, Antonio Toral, and Malvina Nissim. 2022. Human judgement as a\ncompass to navigate automatic metrics for formality transfer. In\nAnya Belz, Maja Popović, Ehud Reiter, and Anastasia Shimorina, editors,\nProceedings of the 2nd workshop on human evaluation of NLP systems\n(HumEval), pages 102–115, Dublin, Ireland. Association for\nComputational Linguistics.\n\n\nSurafel Melaku Lakew, Mattia Di Gangi, and Marcello Federico. 2019. Controlling the output\nlength of neural machine translation. In Jan Niehues, Rolando\nCattoni, Sebastian Stüker, Matteo Negri, Marco Turchi, Thanh-Le Ha,\nElizabeth Salesky, Ramon Sanabria, Loic Barrault, Lucia Specia, and\nMarcello Federico, editors, Proceedings of the 16th international\nconference on spoken language translation, Hong Kong. Association\nfor Computational Linguistics.\n\n\nAnna Langedijk, Hosein Mohebbi, Gabriele Sarti, Willem Zuidema, and Jaap\nJumelet. 2024. DecoderLens:\nLayerwise interpretation of encoder-decoder transformers. In Kevin\nDuh, Helena Gomez, and Steven Bethard, editors, Findings of the\nassociation for computational linguistics: NAACL 2024, pages\n4764–4780, Mexico City, Mexico. Association for Computational\nLinguistics.\n\n\nSamuel Läubli, Chantal Amrhein, Patrick Düggelin, Beatriz Gonzalez,\nAlena Zwahlen, and Martin Volk. 2019. Post-editing productivity with\nneural machine translation: An empirical assessment of speed and quality\nin the banking and finance domain. In Mikel Forcada, Andy Way, Barry\nHaddow, and Rico Sennrich, editors, Proceedings of machine\ntranslation summit XVII: Research track, pages 267–272, Dublin,\nIreland. European Association for Machine Translation.\n\n\nSamuel Läubli, Mark Fishel, Gary Massey, Maureen Ehrensberger-Dow, and\nMartin Volk. 2013. Assessing\npost-editing efficiency in a realistic translation environment. In\nSharon O’Brien, Michel Simard, and Lucia Specia, editors,\nProceedings of the 2nd workshop on post-editing technology and\npractice, Nice, France.\n\n\nSamuel Läubli, Rico Sennrich, and Martin Volk. 2018. Has machine translation\nachieved human parity? A case for document-level evaluation. In\nEllen Riloff, David Chiang, Julia Hockenmaier, and Jun’ichi Tsujii,\neditors, Proceedings of the 2018 conference on empirical methods in\nnatural language processing, pages 4791–4796, Brussels, Belgium.\nAssociation for Computational Linguistics.\n\n\nJihyeon Lee, Taehee Kim, Yunwon Tae, Cheonbok Park, and Jaegul Choo.\n2023a. PePe:\nPersonalized post-editing model utilizing user-generated post-edits.\nIn Andreas Vlachos and Isabelle Augenstein, editors, Findings of the\nassociation for computational linguistics: EACL 2023, pages\n239–253, Dubrovnik, Croatia. Association for Computational Linguistics.\n\n\nSeungjun Lee, Jungseob Lee, Hyeonseok Moon, Chanjun Park, Jaehyung Seo,\nSugyeong Eo, Seonmin Koo, and Heuiseok Lim. 2023b. A survey on evaluation\nmetrics for machine translation. Mathematics, 11(4).\n\n\nChristoph Leiter, Piyawat Lertvittayakumjorn, Marina Fomicheva, Wei\nZhao, Yang Gao, and Steffen Eger. 2024. Towards explainable\nevaluation metrics for machine translation. Journal of Machine\nLearning Research, 25(75):1–49.\n\n\nShahar Levy, Koren Lazar, and Gabriel Stanovsky. 2021. Collecting a\nlarge-scale gender bias dataset for coreference resolution and machine\ntranslation. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia,\nand Scott Wen-tau Yih, editors, Findings of the association for\ncomputational linguistics: EMNLP 2021, pages 2470–2480, Punta Cana,\nDominican Republic. Association for Computational Linguistics.\n\n\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir\nKarpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim\nRocktäschel, Sebastian Riedel, and Douwe Kiela. 2020.\nRetrieval-augmented generation for knowledge-intensive NLP tasks. In\nProceedings of the 34th international conference on neural\ninformation processing systems, Red Hook, NY, USA. Curran\nAssociates Inc.\n\n\nQuentin Lhoest, Albert Villanova del Moral, Yacine Jernite, Abhishek\nThakur, Patrick von Platen, Suraj Patil, Julien Chaumond, Mariama Drame,\nJulien Plu, Lewis Tunstall, Joe Davison, Mario Šaško, Gunjan Chhablani,\nBhavitvya Malik, Simon Brandeis, Teven Le Scao, Victor Sanh, Canwen Xu,\nNicolas Patry, et al. 2021. Datasets: A\ncommunity library for natural language processing. In Heike Adel and\nShuming Shi, editors, Proceedings of the 2021 conference on\nempirical methods in natural language processing: System\ndemonstrations, pages 175–184, Online; Punta Cana, Dominican\nRepublic. Association for Computational Linguistics.\n\n\nHaijun Li, Tianqi Shi, Zifu Shang, Yuxuan Han, Xueyu Zhao, Hao Wang, Yu\nQian, Zhiqiang Qian, Linlong Xu, Minghao Wu, Chenyang Lyu, Longyue Wang,\nGongbo Tang, Weihua Luo, Zhao Xu, and Kaifu Zhang. 2025. TransBench: Benchmarking machine\ntranslation for industrial-scale applications. Arxiv.\n\n\nJiwei Li, Xinlei Chen, Eduard Hovy, and Dan Jurafsky. 2016. Visualizing and\nunderstanding neural models in NLP. In Kevin Knight,\nAni Nenkova, and Owen Rambow, editors, Proceedings of the 2016\nconference of the north American chapter of the association\nfor computational linguistics: Human language technologies, pages\n681–691, San Diego, California. Association for Computational\nLinguistics.\n\n\nXuhong Li, Haoyi Xiong, Xingjian Li, Xuanyu Wu, Xiao Zhang, Ji Liu,\nJiang Bian, and Dejing Dou. 2022. Interpretable deep\nlearning: Interpretation, interpretability, trustworthiness, and\nbeyond. Knowledge and Information Systems,\n64(12):3197–3234.\n\n\nDaniel Licht, Cynthia Gao, Janice Lam, Francisco Guzman, Mona Diab, and\nPhilipp Koehn. 2022. Consistent human\nevaluation of machine translation across language pairs. In Kevin\nDuh and Francisco Guzmán, editors, Proceedings of the 15th biennial\nconference of the association for machine translation in the americas\n(volume 1: Research track), pages 309–321, Orlando, USA.\nAssociation for Machine Translation in the Americas.\n\n\nTom Lieberum, Senthooran Rajamanoharan, Arthur Conmy, Lewis Smith,\nNicolas Sonnerat, Vikrant Varma, Janos Kramar, Anca Dragan, Rohin Shah,\nand Neel Nanda. 2024. Gemma scope:\nOpen sparse autoencoders everywhere all at once on gemma 2. In\nYonatan Belinkov, Najoung Kim, Jaap Jumelet, Hosein Mohebbi, Aaron\nMueller, and Hanjie Chen, editors, Proceedings of the 7th\nBlackboxNLP workshop: Analyzing and interpreting neural networks for\nNLP, pages 278–300, Miami, Florida, US. Association for\nComputational Linguistics.\n\n\nZheng Wei Lim, Ekaterina Vylomova, Charles Kemp, and Trevor Cohn. 2024.\nPredicting human\ntranslation difficulty with neural machine translation.\nTransactions of the Association for Computational Linguistics,\n12:1479–1496.\n\n\nHuan Lin, Liang Yao, Baosong Yang, Dayiheng Liu, Haibo Zhang, Weihua\nLuo, Degen Huang, and Jinsong Su. 2021. Towards user-driven\nneural machine translation. In Chengqing Zong, Fei Xia, Wenjie Li,\nand Roberto Navigli, editors, Proceedings of the 59th annual meeting\nof the association for computational linguistics and the 11th\ninternational joint conference on natural language processing (volume 1:\nLong papers), pages 4008–4018, Online. Association for\nComputational Linguistics.\n\n\nXi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui\nChen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du,\nRamakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary,\nBrian O’Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona Diab,\net al. 2022. Few-shot learning\nwith multilingual generative language models. In Yoav Goldberg,\nZornitsa Kozareva, and Yue Zhang, editors, Proceedings of the 2022\nconference on empirical methods in natural language processing,\npages 9019–9052, Abu Dhabi, United Arab Emirates. Association for\nComputational Linguistics.\n\n\nYu-Hsiang Lin, Chian-Yu Chen, Jean Lee, Zirui Li, Yuyan Zhang, Mengzhou\nXia, Shruti Rijhwani, Junxian He, Zhisong Zhang, Xuezhe Ma, Antonios\nAnastasopoulos, Patrick Littell, and Graham Neubig. 2019. Choosing transfer languages\nfor cross-lingual learning. In Anna Korhonen, David Traum, and Lluís\nMàrquez, editors, Proceedings of the 57th annual meeting of the\nassociation for computational linguistics, pages 3125–3135,\nFlorence, Italy. Association for Computational Linguistics.\n\n\nMary J. Lindstrom and Douglas M. Bates. 1988. Newton—raphson and\nEM algorithms for linear mixed-effects models for repeated-measures\ndata. Journal of the American Statistical Association,\n83(404):1014–1022.\n\n\nPierre Lison, Jörg Tiedemann, and Milen Kouylekov. 2018. OpenSubtitles2018:\nStatistical rescoring of sentence alignments in large, noisy parallel\ncorpora. In Nicoletta Calzolari, Khalid Choukri, Christopher Cieri,\nThierry Declerck, Sara Goggi, Koiti Hasida, Hitoshi Isahara, Bente\nMaegaard, Joseph Mariani, Hélène Mazo, Asuncion Moreno, Jan Odijk,\nStelios Piperidis, and Takenobu Tokunaga, editors, Proceedings of\nthe eleventh international conference on language resources and\nevaluation (LREC 2018), Miyazaki, Japan. European\nLanguage Resources Association (ELRA).\n\n\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and\nWeizhu Chen. 2022. What makes good\nin-context examples for GPT-3? In Eneko Agirre,\nMarianna Apidianaki, and Ivan Vulić, editors, Proceedings of deep\nlearning inside out (DeeLIO 2022): The 3rd workshop on knowledge\nextraction and integration for deep learning architectures, pages\n100–114, Dublin, Ireland; Online. Association for Computational\nLinguistics.\n\n\nNelson Liu, Tianyi Zhang, and Percy Liang. 2023a. Evaluating\nverifiability in generative search engines. In Houda Bouamor, Juan\nPino, and Kalika Bali, editors, Findings of the association for\ncomputational linguistics: EMNLP 2023, pages 7001–7025, Singapore.\nAssociation for Computational Linguistics.\n\n\nXiaoming Liu, Zhaohan Zhang, Yichen Wang, Hang Pu, Yu Lan, and Chao\nShen. 2023b. CoCo:\nCoherence-enhanced machine-generated text detection under low resource\nwith contrastive learning. In Houda Bouamor, Juan Pino, and Kalika\nBali, editors, Proceedings of the 2023 conference on empirical\nmethods in natural language processing, pages 16167–16188,\nSingapore. Association for Computational Linguistics.\n\n\nYinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan\nGhazvininejad, Mike Lewis, and Luke Zettlemoyer. 2020. Multilingual denoising\npre-training for neural machine translation. Transactions of the\nAssociation for Computational Linguistics, 8:726–742.\n\n\nZhongtao Liu, Parker Riley, Daniel Deutsch, Alison Lui, Mengmeng Niu,\nApurva Shah, and Markus Freitag. 2024. Beyond human-only:\nEvaluating human-machine collaboration for collecting high-quality\ntranslation data. In Barry Haddow, Tom Kocmi, Philipp Koehn, and\nChristof Monz, editors, Proceedings of the ninth conference on\nmachine translation, pages 1095–1106, Miami, Florida, USA.\nAssociation for Computational Linguistics.\n\n\nZihan Liu, Genta Indra Winata, and Pascale Fung. 2021. Continual\nmixed-language pre-training for extremely low-resource neural machine\ntranslation. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto\nNavigli, editors, Findings of the association for computational\nlinguistics: ACL-IJCNLP 2021, pages 2706–2718, Online. Association\nfor Computational Linguistics.\n\n\nArle Richard Lommel, Aljoscha Burchardt, and Hans Uszkoreit. 2013. Multidimensional quality\nmetrics: A flexible system for assessing translation quality. In\nProceedings of translating and the computer 35, London, UK.\nAslib.\n\n\nArle Lommel, Serge Gladkoff, Alan Melby, Sue Ellen Wright, Ingemar\nStrandvik, Katerina Gasova, Angelika Vaasa, Andy Benzo, Romina Marazzato\nSparano, Monica Foresi, Johani Innis, Lifeng Han, and Goran Nenadic.\n2024. The\nmulti-range theory of translation quality measurement: MQM\nscoring models and statistical quality control. In Marianna\nMartindale, Janice Campbell, Konstantin Savenkov, and Shivali Goel,\neditors, Proceedings of the 16th conference of the association for\nmachine translation in the americas (volume 2: presentations),\npages 75–94, Chicago, USA. Association for Machine Translation in the\nAmericas.\n\n\nAntónio Lopes, M. Amin Farajian, Rachel Bawden, Michael Zhang, and André\nF. T. Martins. 2020. Document-level neural\nMT: A systematic comparison. In André Martins, Helena\nMoniz, Sara Fumega, Bruno Martins, Fernando Batista, Luisa Coheur, Carla\nParra, Isabel Trancoso, Marco Turchi, Arianna Bisazza, Joss Moorkens,\nAna Guerberof, Mary Nurminen, Lena Marg, and Mikel L. Forcada, editors,\nProceedings of the 22nd annual conference of the european\nassociation for machine translation, pages 225–234, Lisboa,\nPortugal. European Association for Machine Translation.\n\n\nSheng Lu, Shan Chen, Yingya Li, Danielle Bitterman, Guergana Savova, and\nIryna Gurevych. 2023. Measuring\npointwise 𝒱-usable information\nin-context-ly. In Houda Bouamor, Juan Pino, and Kalika Bali,\neditors, Findings of the association for computational linguistics:\nEMNLP 2023, pages 15739–15756, Singapore. Association for\nComputational Linguistics.\n\n\nYao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus\nStenetorp. 2022. Fantastically\nordered prompts and where to find them: Overcoming few-shot prompt order\nsensitivity. In Smaranda Muresan, Preslav Nakov, and Aline\nVillavicencio, editors, Proceedings of the 60th annual meeting of\nthe association for computational linguistics (volume 1: Long\npapers), pages 8086–8098, Dublin, Ireland. Association for\nComputational Linguistics.\n\n\nScott M. Lundberg and Su-In Lee. 2017. A unified approach\nto interpreting model predictions. In Proceedings of the 31st\ninternational conference on neural information processing systems,\nvolume 30, pages 4768–4777, Long Beach, California, USA. Curran\nAssociates Inc.\n\n\nCheng Luo, Wei Liu, Jieyu Lin, Jiajie Zou, Ming Xiang, and Nai Ding.\n2022. Simple but\nchallenging: Natural language inference models fail on simple\nsentences. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang,\neditors, Findings of the association for computational linguistics:\nEMNLP 2022, pages 3449–3462, Abu Dhabi, United Arab Emirates.\nAssociation for Computational Linguistics.\n\n\nLijia Ma, Xingchen Xu, and Yong Tan. 2024. Crafting knowledge: Exploring\nthe creative mechanisms of chat-based search engines.\nArxiv.\n\n\nMohammad Reza Ghasemi Madani, Aryo Pradipta Gema, Gabriele Sarti, Yu\nZhao, Pasquale Minervini, and Andrea Passerini. 2025. Noiser: Bounded input\nperturbations for attributing large language models. In Second\nconference on language modeling.\n\n\nAndreas Madsen, Sarath Chandar, and Siva Reddy. 2024. Are\nself-explanations from large language models faithful? In Lun-Wei\nKu, Andre Martins, and Vivek Srikumar, editors, Findings of the\nassociation for computational linguistics: ACL 2024, pages 295–337,\nBangkok, Thailand. Association for Computational Linguistics.\n\n\nAndreas Madsen, Nicholas Meade, Vaibhav Adlakha, and Siva Reddy. 2022a.\nEvaluating\nthe faithfulness of importance measures in NLP by\nrecursively masking allegedly important tokens and retraining. In\nYoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Findings\nof the association for computational linguistics: EMNLP 2022, pages\n1731–1751, Abu Dhabi, United Arab Emirates. Association for\nComputational Linguistics.\n\n\nAndreas Madsen, Siva Reddy, and Sarath Chandar. 2022b. Post-hoc interpretability for\nneural NLP: A survey. ACM Comput. Surv., 55(8).\n\n\nSuvodeep Majumder, Stanislas Lauly, Maria Nadejde, Marcello Federico,\nand Georgiana Dinu. 2022. A\nbaseline revisited: Pushing the limits of multi-segment models for\ncontext-aware translation. ArXiv, abs/2210.10906.\n\n\nSamuel Marks. 2025. Downstream\napplications as validation of interpretability. LessWrong\nPost.\n\n\nSamuel Marks and Max Tegmark. 2024. The geometry of truth: Emergent\nlinear structure in large language model representations of true/false\ndatasets. In Proceedings of the 1st conference on language\nmodeling (COLM).\n\n\nMarianna Martindale and Marine Carpuat. 2018. Fluency over adequacy: A pilot\nstudy in measuring user trust in imperfect MT. In Colin\nCherry and Graham Neubig, editors, Proceedings of the 13th\nconference of the association for machine translation in the\nAmericas (volume 1: Research track), pages 13–25,\nBoston, MA. Association for Machine Translation in the Americas.\n\n\nSameen Maruf and Gholamreza Haffari. 2018. Document context neural\nmachine translation with memory networks. In Iryna Gurevych and\nYusuke Miyao, editors, Proceedings of the 56th annual meeting of the\nassociation for computational linguistics (volume 1: Long papers),\npages 1275–1284, Melbourne, Australia. Association for Computational\nLinguistics.\n\n\nSameen Maruf, Fahimeh Saleh, and Gholamreza Haffari. 2021. A survey on document-level neural\nmachine translation: Methods and evaluation. ACM Comput.\nSurv., 54(2).\n\n\nEvgeny Matusov. 2019. The\nchallenges of using neural machine translation for literature. In\nJames Hadley, Maja Popović, Haithem Afli, and Andy Way, editors,\nProceedings of the qualities of literary machine translation,\npages 10–19, Dublin, Ireland. European Association for Machine\nTranslation.\n\n\nThomas Mayer and Michael Cysouw. 2014. Creating a massively parallel\nBible corpus. In Nicoletta Calzolari, Khalid Choukri,\nThierry Declerck, Hrafn Loftsson, Bente Maegaard, Joseph Mariani,\nAsuncion Moreno, Jan Odijk, and Stelios Piperidis, editors,\nProceedings of the ninth international conference on language\nresources and evaluation (LREC‘14), pages 3158–3163,\nReykjavik, Iceland. European Language Resources Association (ELRA).\n\n\nJoshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. 2020. On faithfulness and\nfactuality in abstractive summarization. In Dan Jurafsky, Joyce\nChai, Natalie Schluter, and Joel Tetreault, editors, Proceedings of\nthe 58th annual meeting of the association for computational\nlinguistics, pages 1906–1919, Online. Association for Computational\nLinguistics.\n\n\nR. Thomas McCoy, Ellie Pavlick, and Tal Linzen. 2019. Right for the wrong reasons:\nDiagnosing syntactic heuristics in natural language inference. In\nAnna Korhonen, David Traum, and Lluís Màrquez, editors, Proceedings\nof the 57th annual meeting of the association for computational\nlinguistics, pages 3428–3448, Florence, Italy. Association for\nComputational Linguistics.\n\n\nThomas McGrath, Daniel Balsam, Myra Deng, and Eric Ho. 2024. Understanding\nand steering llama 3 with sparse autoencoders. Goodfire\nBlog.\n\n\nNikita Mehandru, Sweta Agrawal, Yimin Xiao, Ge Gao, Elaine Khoong,\nMarine Carpuat, and Niloufar Salehi. 2023. Physician\ndetection of clinical harm in machine translation: Quality estimation\naids in reliance and backtranslation identifies critical errors. In\nHouda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of\nthe 2023 conference on empirical methods in natural language\nprocessing, pages 11633–11647, Singapore. Association for\nComputational Linguistics.\n\n\nKevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. 2022. Locating\nand editing factual associations in GPT. In S. Koyejo,\nS. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors,\nAdvances in neural information processing systems, volume 35,\npages 17359–17372. Curran Associates, Inc.\n\n\nJacob Menick, Maja Trebacz, Vladimir Mikulik, John Aslanides, Francis\nSong, Martin Chadwick, Mia Glaese, Susannah Young, Lucy\nCampbell-Gillingham, Geoffrey Irving, et al. 2022. Teaching language models to\nsupport answers with verified quotes. Arxiv.\n\n\nAlessio Miaschi, Gabriele Sarti, Dominique Brunato, Felice Dell’Orletta,\nand Giulia Venturi. 2022. Probing linguistic knowledge in\nitalian neural language models across language varieties.\nItalian Journal of Computational Linguistics (IJCoL),\n8(1):25–44.\n\n\nPaul Michel and Graham Neubig. 2018. Extreme adaptation for\npersonalized neural machine translation. In Iryna Gurevych and\nYusuke Miyao, editors, Proceedings of the 56th annual meeting of the\nassociation for computational linguistics (volume 2: Short papers),\npages 312–318, Melbourne, Australia. Association for Computational\nLinguistics.\n\n\nLesly Miculicich, Dhananjay Ram, Nikolaos Pappas, and James Henderson.\n2018. Document-level\nneural machine translation with hierarchical attention networks. In\nEllen Riloff, David Chiang, Julia Hockenmaier, and Jun’ichi Tsujii,\neditors, Proceedings of the 2018 conference on empirical methods in\nnatural language processing, pages 2947–2954, Brussels, Belgium.\nAssociation for Computational Linguistics.\n\n\nSabrina J. Mielke, Ryan Cotterell, Kyle Gorman, Brian Roark, and Jason\nEisner. 2019. What kind\nof language is hard to language-model? In Anna Korhonen, David\nTraum, and Lluís Màrquez, editors, Proceedings of the 57th annual\nmeeting of the association for computational linguistics, pages\n4975–4989, Florence, Italy. Association for Computational Linguistics.\n\n\nVivek Miglani, Aobo Yang, Aram Markosyan, Diego Garcia-Olano, and Narine\nKokhlikyan. 2023. Using captum to\nexplain generative language models. In Liling Tan, Dmitrijs\nMilajevs, Geeticka Chauhan, Jeremy Gwinnup, and Elijah Rippeth, editors,\nProceedings of the 3rd workshop for natural language processing open\nsource software (NLP-OSS 2023), pages 165–173, Singapore.\nAssociation for Computational Linguistics.\n\n\nTomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013. Linguistic regularities in\ncontinuous space word representations. In Lucy Vanderwende, Hal\nDaumé III, and Katrin Kirchhoff, editors, Proceedings of the 2013\nconference of the north American chapter of the association\nfor computational linguistics: Human language technologies, pages\n746–751, Atlanta, Georgia. Association for Computational Linguistics.\n\n\nHosein Mohebbi, Willem Zuidema, Grzegorz Chrupała, and Afra Alishahi.\n2023. Quantifying\ncontext mixing in transformers. In Andreas Vlachos and Isabelle\nAugenstein, editors, Proceedings of the 17th conference of the\neuropean chapter of the association for computational linguistics,\npages 3378–3400, Dubrovnik, Croatia. Association for Computational\nLinguistics.\n\n\nJoss Moorkens, Antonio Toral, Sheila Castilho, and Andy Way. 2018. Translators’ perceptions of\nliterary post-editing using statistical and neural machine\ntranslation. Translation Spaces, 7(2):240–262.\n\n\nJohn Moran, Christian Saam, and Dave Lewis. 2014. Towards desktop-based\nCAT tool instrumentation. In Sharon O’Brien, Michel\nSimard, and Lucia Specia, editors, Proceedings of the 11th\nconference of the association for machine translation in the\namericas, pages 99–112, Vancouver, Canada. Association for Machine\nTranslation in the Americas.\n\n\nMarius Mosbach, Vagrant Gautam, Tomás Vergara Browne, Dietrich Klakow,\nand Mor Geva. 2024. From insights to\nactions: The impact of interpretability and analysis research on\nNLP. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung\nChen, editors, Proceedings of the 2024 conference on empirical\nmethods in natural language processing, pages 3078–3105, Miami,\nFlorida, USA. Association for Computational Linguistics.\n\n\nYasmin Moslem, Rejwanul Haque, John D. Kelleher, and Andy Way. 2023. Adaptive machine\ntranslation with large language models. In Mary Nurminen, Judith\nBrenner, Maarit Koponen, Sirkku Latomaa, Mikhail Mikhailov, Frederike\nSchierl, Tharindu Ranasinghe, Eva Vanmassenhove, Sergi Alvarez Vidal,\nNora Aranberri, Mara Nunziatini, Carla Parra Escartín, Mikel Forcada,\nMaja Popovic, Carolina Scarton, and Helena Moniz, editors,\nProceedings of the 24th annual conference of the european\nassociation for machine translation, pages 227–237, Tampere,\nFinland. European Association for Machine Translation.\n\n\nNorman Mu, Sarah Chen, Zifan Wang, Sizhe Chen, David Karamardian, Lulwa\nAljeraisy, Dan Hendrycks, and David Wagner. 2023. Can LLMs follow simple\nrules? Arxiv.\n\n\nBenjamin Muller, John Wieting, Jonathan Clark, Tom Kwiatkowski,\nSebastian Ruder, Livio Soares, Roee Aharoni, Jonathan Herzig, and Xinyi\nWang. 2023. Evaluating and\nmodeling attribution for cross-lingual question answering. In Houda\nBouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the\n2023 conference on empirical methods in natural language\nprocessing, pages 144–157, Singapore. Association for Computational\nLinguistics.\n\n\nMathias Müller, Annette Rios, Elena Voita, and Rico Sennrich. 2018. A large-scale test set for\nthe evaluation of context-aware pronoun translation in neural machine\ntranslation. In Ondřej Bojar, Rajen Chatterjee, Christian Federmann,\nMark Fishel, Yvette Graham, Barry Haddow, Matthias Huck, Antonio Jimeno\nYepes, Philipp Koehn, Christof Monz, Matteo Negri, Aurélie Névéol,\nMariana Neves, Matt Post, Lucia Specia, Marco Turchi, and Karin\nVerspoor, editors, Proceedings of the third conference on machine\ntranslation: Research papers, pages 61–72, Brussels, Belgium.\nAssociation for Computational Linguistics.\n\n\nMaria Nadejde, Anna Currey, Benjamin Hsu, Xing Niu, Marcello Federico,\nand Georgiana Dinu. 2022. CoCoA-MT:\nA dataset and benchmark for contrastive controlled MT with\napplication to formality. In Marine Carpuat, Marie-Catherine de\nMarneffe, and Ivan Vladimir Meza Ruiz, editors, Findings of the\nassociation for computational linguistics: NAACL 2022, pages\n616–632, Seattle, United States. Association for Computational\nLinguistics.\n\n\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang,\nChristina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju,\nWilliam Saunders, et al. 2021. Webgpt: Browser-assisted\nquestion-answering with human feedback. Arxiv.\n\n\nNeel Nanda. 2023. Attribution\npatching: Activation patching at industrial scale.\n\n\nMariana Neves, Cristian Grozea, Philippe Thomas, Roland Roller, Rachel\nBawden, Aurélie Névéol, Steffen Castle, Vanessa Bonato, Giorgio Maria Di\nNunzio, Federica Vezzani, Maika Vicente Navarro, Lana Yeganova, and\nAntonio Jimeno Yepes. 2024. Findings of the\nWMT 2024 biomedical translation shared task: Test sets on\nabstract level. In Barry Haddow, Tom Kocmi, Philipp Koehn, and\nChristof Monz, editors, Proceedings of the ninth conference on\nmachine translation, pages 124–138, Miami, Florida, USA.\nAssociation for Computational Linguistics.\n\n\nMariana Neves, Antonio Jimeno Yepes, Aurélie Névéol, Rachel Bawden,\nGiorgio Maria Di Nunzio, Roland Roller, Philippe Thomas, Federica\nVezzani, Maika Vicente Navarro, Lana Yeganova, Dina Wiemann, and\nCristian Grozea. 2023. Findings of the\nWMT 2023 biomedical translation shared task: Evaluation of\nChatGPT 3.5 as a comparison system. In\nPhilipp Koehn, Barry Haddow, Tom Kocmi, and Christof Monz, editors,\nProceedings of the eighth conference on machine translation,\npages 43–54, Singapore. Association for Computational Linguistics.\n\n\nYixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and\nDouwe Kiela. 2020. Adversarial\nNLI: A new benchmark for natural language\nunderstanding. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and\nJoel Tetreault, editors, Proceedings of the 58th annual meeting of\nthe association for computational linguistics, pages 4885–4901,\nOnline. Association for Computational Linguistics.\n\n\nXing Niu and Marine Carpuat. 2020. Controlling\nneural machine translation formality with synthetic supervision.\nProceedings of the AAAI Conference on Artificial Intelligence,\n34(05):8568–8575.\n\n\nXing Niu, Marianna Martindale, and Marine Carpuat. 2017. A study of style in machine\ntranslation: Controlling the formality of machine translation\noutput. In Martha Palmer, Rebecca Hwa, and Sebastian Riedel,\neditors, Proceedings of the 2017 conference on empirical methods in\nnatural language processing, pages 2814–2819, Copenhagen, Denmark.\nAssociation for Computational Linguistics.\n\n\nNLLB Team, Marta R. Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad,\nKenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel\nLicht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al\nYoungblood, Bapi Akula, Loic Barrault, Gabriel Mejia Gonzalez, Prangthip\nHansanti, et al. 2024. Scaling neural machine\ntranslation to 200 languages. Nature, 630(8018):841–846.\n\n\nnostalgebraist. 2020. Interpreting\nGPT: The logit lens. AI Alignment Forum.\n\n\nMaxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski,\nJacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma,\nDavid Luan, Charles Sutton, and Augustus Odena. 2022. Show your work:\nScratchpads for intermediate computation with language models. In\nDeep learning for code workshop.\n\n\nFranz Josef Och, Christoph Tillmann, and Hermann Ney. 1999. Improved alignment models for\nstatistical machine translation. In 1999 joint\nSIGDAT conference on empirical methods in natural language\nprocessing and very large corpora.\n\n\nChris Olah. 2023. Distributed\nrepresentations: Composition & superposition. Transformer\nCircuits Thread.\n\n\nBruno A. Olshausen and David J. Field. 1997. Sparse coding with\nan overcomplete basis set: A strategy employed by V1? Vision\nResearch, 37(23):3311–3325.\n\n\nOpenAI. 2023. Gpt-4 technical\nreport. Arxiv.\n\n\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan\nNg, David Grangier, and Michael Auli. 2019. Fairseq: A fast, extensible\ntoolkit for sequence modeling. In Waleed Ammar, Annie Louis, and\nNasrin Mostafazadeh, editors, Proceedings of the 2019 conference of\nthe north American chapter of the association for\ncomputational linguistics (demonstrations), pages 48–53,\nMinneapolis, Minnesota. Association for Computational Linguistics.\n\n\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: A\nmethod for automatic evaluation of machine translation. In Pierre\nIsabelle, Eugene Charniak, and Dekang Lin, editors, Proceedings of\nthe 40th annual meeting of the association for computational\nlinguistics, pages 311–318, Philadelphia, Pennsylvania, USA.\nAssociation for Computational Linguistics.\n\n\nKiho Park, Yo Joong Choe, and Victor Veitch. 2023. The linear\nrepresentation hypothesis and the geometry of large language models.\nIn Causal representation learning workshop at NeurIPS 2023.\n\n\nCarla Parra Escartín and Manuel Arcedillo. 2015. Machine\ntranslation evaluation made fuzzier: A study on post-editing\nproductivity and evaluation metrics in commercial settings. In\nProceedings of machine translation summit XV: papers, Miami,\nUSA.\n\n\nJeffrey Pennington, Richard Socher, and Christopher Manning. 2014. GloVe:\nGlobal vectors for word representation. In Alessandro Moschitti, Bo\nPang, and Walter Daelemans, editors, Proceedings of the 2014\nconference on empirical methods in natural language processing\n(EMNLP), pages 1532–1543, Doha, Qatar. Association for\nComputational Linguistics.\n\n\nFabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim Rocktäschel,\nYuxiang Wu, Alexander H. Miller, and Sebastian Riedel. 2020. How context affects\nlanguage models’ factual predictions. In Automated knowledge\nbase construction.\n\n\nAnirudh Phukan, Shwetha Somasundaram, Apoorv Saxena, Koustava Goswami,\nand Balaji Vasan Srinivasan. 2024. Peering into\nthe mind of language models: An approach for attribution in contextual\nquestion answering. In Lun-Wei Ku, Andre Martins, and Vivek\nSrikumar, editors, Findings of the association for computational\nlinguistics: ACL 2024, pages 11481–11495, Bangkok, Thailand.\nAssociation for Computational Linguistics.\n\n\nCharles Pierse. 2021. Transformers\ninterpret.\n\n\nAleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Dmytro Okhonko,\nSamuel Broscheit, Gautier Izacard, Patrick Lewis, Barlas Oğuz, Edouard\nGrave, Wen-tau Yih, et al. 2021. The web is your\noyster-knowledge-intensive NLP against a very large web corpus.\nArxiv.\n\n\nBarbara Plank. 2022. The\n“problem” of human label variation: On ground\ntruth in data, modeling and evaluation. In Yoav Goldberg, Zornitsa\nKozareva, and Yue Zhang, editors, Proceedings of the 2022 conference\non empirical methods in natural language processing, pages\n10671–10682, Abu Dhabi, United Arab Emirates. Association for\nComputational Linguistics.\n\n\nBarbara Plank, Dirk Hovy, and Anders Søgaard. 2014. Linguistically debatable or\njust plain wrong? In Kristina Toutanova and Hua Wu, editors,\nProceedings of the 52nd annual meeting of the association for\ncomputational linguistics (volume 2: Short papers), pages 507–511,\nBaltimore, Maryland. Association for Computational Linguistics.\n\n\nMirko Plitt and François Masselot. 2010. A\nProductivity Test of Statistical\nMachine Translation\nPost-Editing in a Typical\nLocalisation Context. The Prague\nBulletin of Mathematical Linguistics, 93(1).\n\n\nMaja Popović. 2015. ChrF: Character\nn-gram F-score for automatic MT\nevaluation. In Ondřej Bojar, Rajan Chatterjee, Christian Federmann,\nBarry Haddow, Chris Hokamp, Matthias Huck, Varvara Logacheva, and Pavel\nPecina, editors, Proceedings of the tenth workshop on statistical\nmachine translation, pages 392–395, Lisbon, Portugal. Association\nfor Computational Linguistics.\n\n\nMaja Popović. 2020. Informative\nmanual evaluation of machine translation output. In Donia Scott,\nNuria Bel, and Chengqing Zong, editors, Proceedings of the 28th\ninternational conference on computational linguistics, pages\n5059–5069, Barcelona, Spain (Online). International Committee on\nComputational Linguistics.\n\n\nMatt Post. 2018. A call\nfor clarity in reporting BLEU scores. In Ondřej Bojar,\nRajen Chatterjee, Christian Federmann, Mark Fishel, Yvette Graham, Barry\nHaddow, Matthias Huck, Antonio Jimeno Yepes, Philipp Koehn, Christof\nMonz, Matteo Negri, Aurélie Névéol, Mariana Neves, Matt Post, Lucia\nSpecia, Marco Turchi, and Karin Verspoor, editors, Proceedings of\nthe third conference on machine translation: Research papers, pages\n186–191, Brussels, Belgium. Association for Computational Linguistics.\n\n\nMarcelo OR Prates, Pedro H Avelar, and Luís C Lamb. 2020. Assessing\ngender bias in machine translation: A case study with\nGoogle Translate. Neural Computing and\nApplications, 32:6363–6381.\n\n\nJirui Qi^*, Gabriele Sarti^*, Raquel Fernández, and Arianna Bisazza.\n2024. Model\ninternals-based answer attribution for trustworthy retrieval-augmented\ngeneration. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen,\neditors, Proceedings of the 2024 conference on empirical methods in\nnatural language processing, pages 6037–6053, Miami, Florida, USA.\nAssociation for Computational Linguistics.\n\n\nElla Rabinovich, Raj Nath Patel, Shachar Mirkin, Lucia Specia, and Shuly\nWintner. 2017. Personalized\nmachine translation: Preserving original author traits. In Mirella\nLapata, Phil Blunsom, and Alexander Koller, editors, Proceedings of\nthe 15th conference of the European chapter of the\nassociation for computational linguistics: Volume 1, long papers,\npages 1074–1084, Valencia, Spain. Association for Computational\nLinguistics.\n\n\nAlec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya\nSutskever. 2019. Language models\nare unsupervised multitask learners. OpenAI Blog.\n\n\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,\nMichael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the\nlimits of transfer learning with a unified text-to-text transformer.\nJournal of machine learning research, 21(140):1–67.\n\n\nDaking Rai, Yilun Zhou, Shi Feng, Abulhair Saparov, and Ziyu Yao. 2024.\nA practical review of\nmechanistic interpretability for transformer-based language models.\nArxiv Preprint.\n\n\nKorbinian Randl, John Pavlopoulos, Aron Henriksson, and Tony Lindgren.\n2025. Evaluating\nthe reliability of self-explanations in large language models. In\nDiscovery science: 27th international conference, pages 36–51,\nBerlin, Heidelberg. Springer-Verlag.\n\n\nSudha Rao and Joel Tetreault. 2018. Dear sir or madam, may\nI introduce the GYAFC dataset: Corpus,\nbenchmarks and metrics for formality style transfer. In Marilyn\nWalker, Heng Ji, and Amanda Stent, editors, Proceedings of the 2018\nconference of the north American chapter of the association\nfor computational linguistics: Human language technologies, volume 1\n(long papers), pages 129–140, New Orleans, Louisiana. Association\nfor Computational Linguistics.\n\n\nHannah Rashkin, Vitaly Nikolaev, Matthew Lamm, Lora Aroyo, Michael\nCollins, Dipanjan Das, Slav Petrov, Gaurav Singh Tomar, Iulia Turc, and\nDavid Reitter. 2023. Measuring attribution in\nnatural language generation models. Computational\nLinguistics, 49(4):777–840.\n\n\nTilman Räuker, Anson Ho, Stephen Casper, and Dylan Hadfield-Menell.\n2023. Toward\ntransparent AI: A survey on interpreting the inner structures of deep\nneural networks. In 2023 IEEE conference on secure and\ntrustworthy machine learning (SaTML), pages 464–483.\n\n\nShauli Ravfogel, Yoav Goldberg, and Jacob Goldberger. 2023. Conformal nucleus\nsampling. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki,\neditors, Findings of the association for computational linguistics:\nACL 2023, pages 27–34, Toronto, Canada. Association for\nComputational Linguistics.\n\n\nRicardo Rei, José G. C. de Souza, Duarte Alves, Chrysoula Zerva, Ana C\nFarinha, Taisiya Glushkova, Alon Lavie, Luisa Coheur, and André F. T.\nMartins. 2022a. COMET-22:\nUnbabel-IST 2022 submission for the metrics shared\ntask. In Philipp Koehn, Loïc Barrault, Ondřej Bojar, Fethi Bougares,\nRajen Chatterjee, Marta R. Costa-jussà, Christian Federmann, Mark\nFishel, Alexander Fraser, Markus Freitag, Yvette Graham, Roman\nGrundkiewicz, Paco Guzman, Barry Haddow, Matthias Huck, Antonio Jimeno\nYepes, Tom Kocmi, André Martins, Makoto Morishita, et al., editors,\nProceedings of the seventh conference on machine translation\n(WMT), pages 578–585, Abu Dhabi, United Arab Emirates (Hybrid).\nAssociation for Computational Linguistics.\n\n\nRicardo Rei, Ana C Farinha, José G. C. de Souza, Pedro G. Ramos, André\nF. T. Martins, Luisa Coheur, and Alon Lavie. 2022b. Searching for\nCOMETINHO: The little metric that could. In Helena\nMoniz, Lieve Macken, Andrew Rufener, Loïc Barrault, Marta R.\nCosta-jussà, Christophe Declercq, Maarit Koponen, Ellie Kemp, Spyridon\nPilos, Mikel L. Forcada, Carolina Scarton, Joachim Van den Bogaert, Joke\nDaems, Arda Tezcan, Bram Vanroy, and Margot Fonteyne, editors,\nProceedings of the 23rd annual conference of the european\nassociation for machine translation, pages 61–70, Ghent, Belgium.\nEuropean Association for Machine Translation.\n\n\nRicardo Rei, Ana C Farinha, Chrysoula Zerva, Daan van Stigt, Craig\nStewart, Pedro Ramos, Taisiya Glushkova, André F. T. Martins, and Alon\nLavie. 2021. Are\nreferences really needed? Unbabel-IST 2021 submission for\nthe metrics shared task. In Loic Barrault, Ondrej Bojar, Fethi\nBougares, Rajen Chatterjee, Marta R. Costa-jussa, Christian Federmann,\nMark Fishel, Alexander Fraser, Markus Freitag, Yvette Graham, Roman\nGrundkiewicz, Paco Guzman, Barry Haddow, Matthias Huck, Antonio Jimeno\nYepes, Philipp Koehn, Tom Kocmi, Andre Martins, Makoto Morishita, et\nal., editors, Proceedings of the sixth conference on machine\ntranslation, pages 1030–1040, Online. Association for Computational\nLinguistics.\n\n\nRicardo Rei, Nuno M. Guerreiro, Marcos Treviso, Luisa Coheur, Alon\nLavie, and André Martins. 2023. The inside story:\nTowards better understanding of machine translation neural evaluation\nmetrics. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki,\neditors, Proceedings of the 61st annual meeting of the association\nfor computational linguistics (volume 2: Short papers), pages\n1089–1105, Toronto, Canada. Association for Computational Linguistics.\n\n\nRicardo Rei, Jose Pombal, Nuno M. Guerreiro, João Alves, Pedro Henrique\nMartins, Patrick Fernandes, Helena Wu, Tania Vaz, Duarte Alves, Amin\nFarajian, Sweta Agrawal, Antonio Farinhas, José G. C. De Souza, and\nAndré Martins. 2024. Tower v2:\nUnbabel-IST 2024 submission for the general MT\nshared task. In Barry Haddow, Tom Kocmi, Philipp Koehn, and Christof\nMonz, editors, Proceedings of the ninth conference on machine\ntranslation, pages 185–204, Miami, Florida, USA. Association for\nComputational Linguistics.\n\n\nRicardo Rei, Craig Stewart, Ana C Farinha, and Alon Lavie. 2020. COMET:\nA neural framework for MT evaluation. In Bonnie Webber,\nTrevor Cohn, Yulan He, and Yang Liu, editors, Proceedings of the\n2020 conference on empirical methods in natural language processing\n(EMNLP), pages 2685–2702, Online. Association for Computational\nLinguistics.\n\n\nEmily Reif, Daphne Ippolito, Ann Yuan, Andy Coenen, Chris\nCallison-Burch, and Jason Wei. 2022. A recipe for\narbitrary text style transfer with large language models. In\nSmaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors,\nProceedings of the 60th annual meeting of the association for\ncomputational linguistics (volume 2: Short papers), pages 837–848,\nDublin, Ireland. Association for Computational Linguistics.\n\n\nRuiyang Ren, Yuhao Wang, Yingqi Qu, Wayne Xin Zhao, Jing Liu, Hua Wu,\nJi-Rong Wen, and Haifeng Wang. 2025. Investigating the\nfactual knowledge boundary of large language models with retrieval\naugmentation. In Owen Rambow, Leo Wanner, Marianna Apidianaki, Hend\nAl-Khalifa, Barbara Di Eugenio, and Steven Schockaert, editors,\nProceedings of the 31st international conference on computational\nlinguistics, pages 3697–3715, Abu Dhabi, UAE. Association for\nComputational Linguistics.\n\n\nMarco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. \"Why should i trust\nyou?\": Explaining the predictions of any classifier. In\nProceedings of the 22nd ACM SIGKDD international conference on\nknowledge discovery and data mining, pages 1135–1144, New York, NY,\nUSA. Association for Computing Machinery.\n\n\nNina Rimsky, Nick Gabrieli, Julian Schulz, Meg Tong, Evan Hubinger, and\nAlexander Turner. 2024. Steering llama 2\nvia contrastive activation addition. In Lun-Wei Ku, Andre Martins,\nand Vivek Srikumar, editors, Proceedings of the 62nd annual meeting\nof the association for computational linguistics (volume 1: Long\npapers), pages 15504–15522, Bangkok, Thailand. Association for\nComputational Linguistics.\n\n\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky. 2020. A primer in\nBERTology: What we know about how BERT\nworks. Transactions of the Association for Computational\nLinguistics, 8:842–866.\n\n\nRaphael Rubino, Atsushi Fujita, and Benjamin Marie. 2021. Error\nidentification for machine translation with metric embedding and\nattention. In Yang Gao, Steffen Eger, Wei Zhao, Piyawat\nLertvittayakumjorn, and Marina Fomicheva, editors, Proceedings of\nthe 2nd workshop on evaluation and comparison of NLP systems, pages\n146–156, Punta Cana, Dominican Republic. Association for Computational\nLinguistics.\n\n\nCynthia Rudin. 2019. Stop explaining black\nbox machine learning models for high stakes decisions and use\ninterpretable models instead. Nature Machine Intelligence,\n1:206–215.\n\n\nRachel Rudinger, Jason Naradowsky, Brian Leonard, and Benjamin Van\nDurme. 2018. Gender bias\nin coreference resolution. In Marilyn Walker, Heng Ji, and Amanda\nStent, editors, Proceedings of the 2018 conference of the north\nAmerican chapter of the association for computational\nlinguistics: Human language technologies, volume 2 (short papers),\npages 8–14, New Orleans, Louisiana. Association for Computational\nLinguistics.\n\n\nDavid E. Rumelhart and James L. McClelland. 1987. Learning internal\nrepresentations by error propagation. In Parallel distributed\nprocessing: Explorations in the microstructure of cognition:\nfoundations, pages 318–362. MIT Press.\n\n\nVictor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang\nSutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja,\nManan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma\nSharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak,\nDebajyoti Datta, et al. 2022. Multitask prompted\ntraining enables zero-shot task generalization. In Proceedings\nof the tenth international conference on learning representations\n(ICLR).\n\n\nSoumya Sanyal and Xiang Ren. 2021. Discretized\nintegrated gradients for explaining language models. In\nMarie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau\nYih, editors, Proceedings of the 2021 conference on empirical\nmethods in natural language processing, pages 10285–10299, Online;\nPunta Cana, Dominican Republic. Association for Computational\nLinguistics.\n\n\nNaomi Saphra and Sarah Wiegreffe. 2024. Mechanistic?\nIn Yonatan Belinkov, Najoung Kim, Jaap Jumelet, Hosein Mohebbi, Aaron\nMueller, and Hanjie Chen, editors, Proceedings of the 7th\nBlackboxNLP workshop: Analyzing and interpreting neural networks for\nNLP, pages 480–498, Miami, Florida, US. Association for\nComputational Linguistics.\n\n\nGabriele Sarti, Arianna Bisazza, Ana Guerberof-Arenas, and Antonio\nToral. 2022. DivEMT:\nNeural machine translation post-editing effort across typologically\ndiverse languages. In Yoav Goldberg, Zornitsa Kozareva, and Yue\nZhang, editors, Proceedings of the 2022 conference on empirical\nmethods in natural language processing, pages 7795–7816, Abu Dhabi,\nUnited Arab Emirates. Association for Computational Linguistics.\n\n\nGabriele Sarti, Tommaso Caselli, Arianna Bisazza, and Malvina Nissim.\n2024a. EurekaRebus\n- verbalized rebus solving with LLMs: A\nCALAMITA challenge. In Felice Dell’Orletta, Alessandro\nLenci, Simonetta Montemagni, and Rachele Sprugnoli, editors,\nProceedings of the 10th italian conference on computational\nlinguistics (CLiC-it 2024), pages 1202–1208, Pisa, Italy. CEUR\nWorkshop Proceedings.\n\n\nGabriele Sarti, Tommaso Caselli, Malvina Nissim, and Arianna Bisazza.\n2024b. Non verbis,\nsed rebus: Large language models are weak solvers of\nItalian rebuses. In Felice Dell’Orletta, Alessandro\nLenci, Simonetta Montemagni, and Rachele Sprugnoli, editors,\nProceedings of the 10th italian conference on computational\nlinguistics (CLiC-it 2024), pages 888–897, Pisa, Italy. CEUR\nWorkshop Proceedings.\n\n\nGabriele Sarti, Grzegorz Chrupała, Malvina Nissim, and Arianna Bisazza.\n2024c. Quantifying\nthe plausibility of context reliance in neural machine translation.\nIn The twelfth international conference on learning representations\n(ICLR 2024), Vienna, Austria. OpenReview.\n\n\nGabriele Sarti, Nils Feldhus, Jirui Qi, Malvina Nissim, and Arianna\nBisazza. 2024d. Democratizing advanced\nattribution analyses of generative language models with the inseq\ntoolkit. In xAI-2024 late-breaking work, demos and doctoral\nconsortium joint proceedings, pages 289–296, Valletta, Malta.\nCEUR.org.\n\n\nGabriele Sarti, Nils Feldhus, Ludwig Sickert, Oskar van der Wal, Malvina\nNissim, and Arianna Bisazza. 2023a. Inseq: An\ninterpretability toolkit for sequence generation models. In Danushka\nBollegala, Ruihong Huang, and Alan Ritter, editors, Proceedings of\nthe 61st annual meeting of the association for computational linguistics\n(volume 3: System demonstrations), pages 421–435, Toronto, Canada.\nAssociation for Computational Linguistics.\n\n\nGabriele Sarti, Phu Mon Htut, Xing Niu, Benjamin Hsu, Anna Currey,\nGeorgiana Dinu, and Maria Nadejde. 2023b. RAMP:\nRetrieval and attribute-marking enhanced prompting for\nattribute-controlled translation. In Anna Rogers, Jordan\nBoyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st\nannual meeting of the association for computational linguistics (volume\n2: Short papers), pages 1476–1490, Toronto, Canada. Association for\nComputational Linguistics.\n\n\nGabriele Sarti and Malvina Nissim. 2024. IT5:\nText-to-text pretraining for Italian language understanding\nand generation. In Nicoletta Calzolari, Min-Yen Kan, Veronique\nHoste, Alessandro Lenci, Sakriani Sakti, and Nianwen Xue, editors,\nProceedings of the 2024 joint international conference on\ncomputational linguistics, language resources and evaluation\n(LREC-COLING 2024), pages 9422–9433, Torino, Italia. ELRA; ICCL.\n\n\nGabriele Sarti, Vilém Zouhar, Grzegorz Chrupała, Ana Guerberof-Arenas,\nMalvina Nissim, and Arianna Bisazza. 2025a. QE4PE: Word-level\nquality estimation for human post-editing. Arxiv Preprint.\n\n\nGabriele Sarti, Vilém Zouhar, Malvina Nissim, and Arianna Bisazza.\n2025b. Unsupervised word-level\nquality estimation for machine translation through the lens of\nannotators (dis)agreement. Arxiv Preprint.\n\n\nDanielle Saunders and Bill Byrne. 2020. Reducing gender\nbias in neural machine translation as a domain adaptation problem.\nIn Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault,\neditors, Proceedings of the 58th annual meeting of the association\nfor computational linguistics, pages 7724–7736, Online. Association\nfor Computational Linguistics.\n\n\nBeatrice Savoldi, Marco Gaido, Luisa Bentivogli, Matteo Negri, and Marco\nTurchi. 2021. Gender bias\nin machine translation. Transactions of the Association for\nComputational Linguistics, 9:845–874.\n\n\nBeatrice Savoldi, Alan Ramponi, Matteo Negri, and Luisa Bentivogli.\n2025. Translation in the\nhands of many: Centering lay users in machine translation\ninteractions.\n\n\nDaniel Scalena, Gabriele Sarti, and Malvina Nissim. 2024. Multi-property\nsteering of large language models with dynamic activation\ncomposition. In Yonatan Belinkov, Najoung Kim, Jaap Jumelet, Hosein\nMohebbi, Aaron Mueller, and Hanjie Chen, editors, Proceedings of the\n7th BlackboxNLP workshop: Analyzing and interpreting neural networks for\nNLP, pages 577–603, Miami, Florida, US. Association for\nComputational Linguistics.\n\n\nDaniel Scalena^*, Gabriele Sarti^*, Arianna Bisazza, Elisabetta Fersini,\nand Malvina Nissim. 2025. Steering large language models\nfor machine translation personalization. Arxiv Preprint.\n\n\nAndrea Schioppa, David Vilar, Artem Sokolov, and Katja Filippova. 2021.\nControlling\nmachine translation for multiple attributes with additive\ninterventions. In Marie-Francine Moens, Xuanjing Huang, Lucia\nSpecia, and Scott Wen-tau Yih, editors, Proceedings of the 2021\nconference on empirical methods in natural language processing,\npages 6676–6696, Online; Punta Cana, Dominican Republic. Association for\nComputational Linguistics.\n\n\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg\nKlimov. 2017. Proximal policy\noptimization algorithms.\n\n\nHolger Schwenk, Guillaume Wenzek, Sergey Edunov, Edouard Grave, Armand\nJoulin, and Angela Fan. 2021. CCMatrix:\nMining billions of high-quality parallel sentences on the web. In\nChengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors,\nProceedings of the 59th annual meeting of the association for\ncomputational linguistics and the 11th international joint conference on\nnatural language processing (volume 1: Long papers), pages\n6490–6500, Online. Association for Computational Linguistics.\n\n\nThibault Sellam, Dipanjan Das, and Ankur Parikh. 2020. BLEURT:\nLearning robust metrics for text generation. In Dan Jurafsky, Joyce\nChai, Natalie Schluter, and Joel Tetreault, editors, Proceedings of\nthe 58th annual meeting of the association for computational\nlinguistics, pages 7881–7892, Online. Association for Computational\nLinguistics.\n\n\nRico Sennrich, Barry Haddow, and Alexandra Birch. 2016a. Controlling politeness in\nneural machine translation via side constraints. In Kevin Knight,\nAni Nenkova, and Owen Rambow, editors, Proceedings of the 2016\nconference of the north American chapter of the association\nfor computational linguistics: Human language technologies, pages\n35–40, San Diego, California. Association for Computational Linguistics.\n\n\nRico Sennrich, Barry Haddow, and Alexandra Birch. 2016b. Neural machine translation\nof rare words with subword units. In Katrin Erk and Noah A. Smith,\neditors, Proceedings of the 54th annual meeting of the association\nfor computational linguistics (volume 1: Long papers), pages\n1715–1725, Berlin, Germany. Association for Computational Linguistics.\n\n\nLee Sharkey, Bilal Chughtai, Joshua Batson, Jack Lindsey, Jeff Wu,\nLucius Bushnaq, Nicholas Goldowsky-Dill, Stefan Heimersheim, Alejandro\nOrtega, Joseph Bloom, Stella Biderman, Adria Garriga-Alonso, Arthur\nConmy, Neel Nanda, Jessica Rumbelow, Martin Wattenberg, Nandi Schoots,\nJoseph Miller, Eric J. Michaud, et al. 2025. Open problems in mechanistic\ninterpretability.\n\n\nRaksha Shenoy, Nico Herbig, Antonio Krüger, and Josef van Genabith.\n2021. Investigating the\nhelpfulness of word-level quality estimation for post-editing machine\ntranslation output. In Marie-Francine Moens, Xuanjing Huang, Lucia\nSpecia, and Scott Wen-tau Yih, editors, Proceedings of the 2021\nconference on empirical methods in natural language processing,\npages 10173–10185, Online; Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\n\n\nFreda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed\nChi, Nathanael Schärli, and Denny Zhou. 2023. Large language models can\nbe easily distracted by irrelevant context. In Proceedings of the\n40th international conference on machine learning, Honolulu,\nHawaii, USA. JMLR.org.\n\n\nAvanti Shrikumar, Peyton Greenside, and Anshul Kundaje. 2017. Learning\nimportant features through propagating activation differences. In\nDoina Precup and Yee Whye Teh, editors, Proceedings of the 34th\ninternational conference on machine learning, volume 70, pages\n3145–3153. PMLR.\n\n\nKaren Simonyan, Andrea Vedaldi, and Andrew Zisserman. 2014. Deep inside convolutional\nnetworks: Visualising image classification models and saliency maps.\nIn Yoshua Bengio and Yann LeCun, editors, 2nd international\nconference on learning representations, (ICLR), Banff,\nAB, Canada.\n\n\nKoustuv Sinha, Prasanna Parthasarathi, Joelle Pineau, and Adina\nWilliams. 2021. UnNatural\nLanguage Inference. In Chengqing Zong, Fei\nXia, Wenjie Li, and Roberto Navigli, editors, Proceedings of the\n59th annual meeting of the association for computational linguistics and\nthe 11th international joint conference on natural language processing\n(volume 1: Long papers), pages 7329–7346, Online. Association for\nComputational Linguistics.\n\n\nLeon Sixt, Maximilian Granz, and Tim Landgraf. 2020. When explanations\nlie: Why many modified BP attributions fail. In Hal\nDaumé III and Aarti Singh, editors, Proceedings of the 37th\ninternational conference on machine learning, volume 119, pages\n9046–9057. PMLR.\n\n\nDaniel Smilkov, Nikhil Thorat, Been Kim, Fernanda Viégas, and Martin\nWattenberg. 2017. SmoothGrad:\nRemoving noise by adding noise.\n\n\nPaul Smolensky. 1986. Neural and conceptual interpretation of PDP\nmodels.\n\n\nMatthew Snover, Bonnie Dorr, Rich Schwartz, Linnea Micciulla, and John\nMakhoul. 2006. A\nstudy of translation edit rate with targeted human annotation. In\nProceedings of the 7th conference of the association for machine\ntranslation in the americas: Technical papers, pages 223–231,\nCambridge, Massachusetts, USA. Association for Machine Translation in\nthe Americas.\n\n\nRion Snow, Brendan O’Connor, Daniel Jurafsky, and Andrew Ng. 2008. Cheap and fast –\nbut is it good? Evaluating non-expert annotations for natural language\ntasks. In Mirella Lapata and Hwee Tou Ng, editors, Proceedings\nof the 2008 conference on empirical methods in natural language\nprocessing, pages 254–263, Honolulu, Hawaii. Association for\nComputational Linguistics.\n\n\nLucia Specia, Frédéric Blain, Marina Fomicheva, Erick Fonseca, Vishrav\nChaudhary, Francisco Guzmán, and André F. T. Martins. 2020. Findings of the\nWMT 2020 shared task on quality estimation. In Loïc\nBarrault, Ondřej Bojar, Fethi Bougares, Rajen Chatterjee, Marta R.\nCosta-jussà, Christian Federmann, Mark Fishel, Alexander Fraser, Yvette\nGraham, Paco Guzman, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes,\nPhilipp Koehn, André Martins, Makoto Morishita, Christof Monz, Masaaki\nNagata, Toshiaki Nakazawa, et al., editors, Proceedings of the fifth\nconference on machine translation, pages 743–764, Online.\nAssociation for Computational Linguistics.\n\n\nLucia Specia, Carolina Scarton, Gustavo Henrique Paetzold, and Graeme\nHirst. 2018. Quality estimation for machine translation. Morgan\n& Claypool Publishers.\n\n\nLucia Specia, Marco Turchi, Nicola Cancedda, Nello Cristianini, and Marc\nDymetman. 2009. Estimating the\nsentence-level quality of machine translation systems. In Lluís\nMàrquez and Harold Somers, editors, Proceedings of the 13th annual\nconference of the european association for machine translation,\nBarcelona, Spain. European Association for Machine Translation.\n\n\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and\nRuslan Salakhutdinov. 2014. Dropout: A simple\nway to prevent neural networks from overfitting. Journal of\nMachine Learning Research, 15(56):1929–1958.\n\n\nGabriel Stanovsky, Noah A. Smith, and Luke Zettlemoyer. 2019. Evaluating gender bias in\nmachine translation. In Anna Korhonen, David Traum, and Lluís\nMàrquez, editors, Proceedings of the 57th annual meeting of the\nassociation for computational linguistics, pages 1679–1684,\nFlorence, Italy. Association for Computational Linguistics.\n\n\nMaria Stasimioti and Vilelmini Sosoni. 2020. Translation vs\npost-editing of NMT output: Insights from the\nEnglish-Greek language pair. In John E.\nOrtega, Marcello Federico, Constantin Orasan, and Maja Popovic, editors,\nProceedings of 1st workshop on post-editing in modern-day\ntranslation, pages 109–124, Virtual. Association for Machine\nTranslation in the Americas.\n\n\nAlessandro Stolfo, Ben Wu, Wes Gurnee, Yonatan Belinkov, Xingyi Song,\nMrinmaya Sachan, and Neel Nanda. 2024. Confidence\nregulation neurons in language models. In A. Globerson, L. Mackey,\nD. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors,\nAdvances in neural information processing systems, volume 37,\npages 125019–125049. Curran Associates, Inc.\n\n\nJianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng\nLiu. 2024. RoFormer: Enhanced\ntransformer with rotary position embedding. Neurocomputing,\n568:127063.\n\n\nJiao Sun, Swabha Swayamdipta, Jonathan May, and Xuezhe Ma. 2022. Investigating\nthe benefits of free-form rationales. In Yoav Goldberg, Zornitsa\nKozareva, and Yue Zhang, editors, Findings of the association for\ncomputational linguistics: EMNLP 2022, pages 5867–5882, Abu Dhabi,\nUnited Arab Emirates. Association for Computational Linguistics.\n\n\nMukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017. Axiomatic\nattribution for deep networks. In Proceedings of the 34th\ninternational conference on machine learning (ICML), volume 70,\npages 3319–3328, Sydney, Australia. Journal of Machine Learning Research\n(JMLR).\n\n\nIlya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014. Sequence to\nsequence learning with neural networks. In Proceedings of the 28th\ninternational conference on neural information processing systems -\nvolume 2, pages 3104–3112, Cambridge, MA, USA. MIT Press.\n\n\nMirac Suzgun, Luke Melas-Kyriazi, and Dan Jurafsky. 2022. Prompt-and-rerank:\nA method for zero-shot and few-shot arbitrary textual style transfer\nwith small language models. In Yoav Goldberg, Zornitsa Kozareva, and\nYue Zhang, editors, Proceedings of the 2022 conference on empirical\nmethods in natural language processing, pages 2195–2222, Abu Dhabi,\nUnited Arab Emirates. Association for Computational Linguistics.\n\n\nAleš Tamchyna. 2021. Deploying\nMT quality estimation on a large scale: Lessons learned and\nopen questions. In Janice Campbell, Ben Huyck, Stephen Larocca, Jay\nMarciano, Konstantin Savenkov, and Alex Yanishevsky, editors,\nProceedings of machine translation summit XVIII: Users and providers\ntrack, pages 291–305, Virtual. Association for Machine Translation\nin the Americas.\n\n\nJoel Tang, Marina Fomicheva, and Lucia Specia. 2022. Reducing hallucinations in\nneural machine translation with feature attribution. ArXiv.\n\n\nYuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Naman Goyal, Vishrav\nChaudhary, Jiatao Gu, and Angela Fan. 2021. Multilingual\ntranslation from denoising pre-training. In Chengqing Zong, Fei Xia,\nWenjie Li, and Roberto Navigli, editors, Findings of the association\nfor computational linguistics: ACL-IJCNLP 2021, pages 3450–3466,\nOnline. Association for Computational Linguistics.\n\n\nGemma Team. 2024a. Gemma 2:\nImproving open language models at a practical size. Arxiv.\n\n\nLlama Team. 2024b. The llama\n3 herd of models. Arxiv.\n\n\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019. BERT\nrediscovers the classical NLP pipeline. In Anna\nKorhonen, David Traum, and Lluís Màrquez, editors, Proceedings of\nthe 57th annual meeting of the association for computational\nlinguistics, pages 4593–4601, Florence, Italy. Association for\nComputational Linguistics.\n\n\nIan Tenney, Ryan Mullins, Bin Du, Shree Pandya, Minsuk Kahng, and Lucas\nDixon. 2024. Interactive\nprompt debugging with sequence salience. Arxiv.\n\n\nIan Tenney, James Wexler, Jasmijn Bastings, Tolga Bolukbasi, Andy\nCoenen, Sebastian Gehrmann, Ellen Jiang, Mahima Pushkarna, Carey\nRadebaugh, Emily Reif, and Ann Yuan. 2020. The language\ninterpretability tool: Extensible, interactive visualizations and\nanalysis for NLP models. In Qun Liu and David\nSchlangen, editors, Proceedings of the 2020 conference on empirical\nmethods in natural language processing: System demonstrations,\npages 107–118, Online. Association for Computational Linguistics.\n\n\nKatherine Thai, Marzena Karpinska, Kalpesh Krishna, Bill Ray, Moira\nInghilleri, John Wieting, and Mohit Iyyer. 2022. Exploring\ndocument-level literary machine translation with parallel paragraphs\nfrom world literature. In Yoav Goldberg, Zornitsa Kozareva, and Yue\nZhang, editors, Proceedings of the 2022 conference on empirical\nmethods in natural language processing, pages 9882–9902, Abu Dhabi,\nUnited Arab Emirates. Association for Computational Linguistics.\n\n\nBrian Thompson and Matt Post. 2020. Automatic machine\ntranslation evaluation in many languages via zero-shot paraphrasing.\nIn Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors,\nProceedings of the 2020 conference on empirical methods in natural\nlanguage processing (EMNLP), pages 90–121, Online. Association for\nComputational Linguistics.\n\n\nJörg Tiedemann. 2020. The tatoeba translation\nchallenge – realistic data sets for low resource and\nmultilingual MT. In Loïc Barrault, Ondřej Bojar, Fethi\nBougares, Rajen Chatterjee, Marta R. Costa-jussà, Christian Federmann,\nMark Fishel, Alexander Fraser, Yvette Graham, Paco Guzman, Barry Haddow,\nMatthias Huck, Antonio Jimeno Yepes, Philipp Koehn, André Martins,\nMakoto Morishita, Christof Monz, Masaaki Nagata, Toshiaki Nakazawa, et\nal., editors, Proceedings of the fifth conference on machine\ntranslation, pages 1174–1182, Online. Association for Computational\nLinguistics.\n\n\nJörg Tiedemann and Yves Scherrer. 2017. Neural machine translation\nwith extended context. In Bonnie Webber, Andrei Popescu-Belis, and\nJörg Tiedemann, editors, Proceedings of the third workshop on\ndiscourse in machine translation, pages 82–92, Copenhagen, Denmark.\nAssociation for Computational Linguistics.\n\n\nJörg Tiedemann and Santhosh Thottingal. 2020. OPUS-MT\n– building open translation services for the world. In\nAndré Martins, Helena Moniz, Sara Fumega, Bruno Martins, Fernando\nBatista, Luisa Coheur, Carla Parra, Isabel Trancoso, Marco Turchi,\nArianna Bisazza, Joss Moorkens, Ana Guerberof, Mary Nurminen, Lena Marg,\nand Mikel L. Forcada, editors, Proceedings of the 22nd annual\nconference of the european association for machine translation,\npages 479–480, Lisboa, Portugal. European Association for Machine\nTranslation.\n\n\nCurt Tigges, Oskar J. Hollinsworth, Atticus Geiger, and Neel Nanda.\n2024. Language models\nlinearly represent sentiment. In Yonatan Belinkov, Najoung Kim, Jaap\nJumelet, Hosein Mohebbi, Aaron Mueller, and Hanjie Chen, editors,\nProceedings of the 7th BlackboxNLP workshop: Analyzing and\ninterpreting neural networks for NLP, pages 58–87, Miami, Florida,\nUS. Association for Computational Linguistics.\n\n\nEric Todd, Millicent L. Li, Arnab Sen Sharma, Aaron Mueller, Byron C.\nWallace, and David Bau. 2024. Function vectors in\nlarge language models. In Proceedings of the 2024 international\nconference on learning representations.\n\n\nAntonio Toral, Sheila Castilho, Ke Hu, and Andy Way. 2018a. Attaining the unattainable?\nReassessing claims of human parity in neural machine translation. In\nOndřej Bojar, Rajen Chatterjee, Christian Federmann, Mark Fishel, Yvette\nGraham, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes, Philipp\nKoehn, Christof Monz, Matteo Negri, Aurélie Névéol, Mariana Neves, Matt\nPost, Lucia Specia, Marco Turchi, and Karin Verspoor, editors,\nProceedings of the third conference on machine translation: Research\npapers, pages 113–123, Brussels, Belgium. Association for\nComputational Linguistics.\n\n\nAntonio Toral and Andy Way. 2015. Translating literary text\nbetween related languages using SMT. In Anna Feldman,\nAnna Kazantseva, Stan Szpakowicz, and Corina Koolen, editors,\nProceedings of the fourth workshop on computational linguistics for\nliterature, pages 123–132, Denver, Colorado, USA. Association for\nComputational Linguistics.\n\n\nAntonio Toral and Andy Way. 2018. What level of\nquality can neural machine translation attain on literary text? In\nTranslation quality assessment: From principles to practice,\npages 263–287. Springer International Publishing, Cham.\n\n\nAntonio Toral, Martijn Wieling, and Andy Way. 2018b. Post-editing effort of a\nnovel with statistical and neural machine translation. Frontiers\nin Digital Humanities, 5:1–11.\n\n\nHugo Touvron, Louis Martin, Kevin R. Stone, Peter Albert, Amjad\nAlmahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal\nBhargava, Shruti Bhosale, Daniel M. Bikel, Lukas Blecher, Cristian\nCantòn Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude\nFernandes, Jeremy Fu, Wenyin Fu, et al. 2023. Llama 2: Open foundation and\nfine-tuned chat models. ArXiv.\n\n\nLewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif\nRasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Clémentine\nFourrier, Nathan Habib, Nathan Sarrazin, Omar Sanseviero, Alexander M.\nRush, and Thomas Wolf. 2024. Zephyr:\nDirect distillation of LM alignment. In Proceedings of the 1st\nconference on language modeling (COLM).\n\n\nMarco Turchi, Antonios Anastasopoulos, José G. C. de Souza, and Matteo\nNegri. 2014. Adaptive\nquality estimation for machine translation. In Kristina Toutanova\nand Hua Wu, editors, Proceedings of the 52nd annual meeting of the\nassociation for computational linguistics (volume 1: Long papers),\npages 710–720, Baltimore, Maryland. Association for Computational\nLinguistics.\n\n\nMarco Turchi, Matteo Negri, M. Amin Farajian, and Marcello Federico.\n2017. Continuous\nlearning from human post-edits for neural machine translation.\nThe Prague Bulletin of Mathematical Linguistics, 108:233–244.\n\n\nMarco Turchi, Matteo Negri, and Marcello Federico. 2013. Coping with the subjectivity\nof human judgements in MT quality estimation. In Ondrej\nBojar, Christian Buck, Chris Callison-Burch, Barry Haddow, Philipp\nKoehn, Christof Monz, Matt Post, Herve Saint-Amand, Radu Soricut, and\nLucia Specia, editors, Proceedings of the eighth workshop on\nstatistical machine translation, pages 240–251, Sofia, Bulgaria.\nAssociation for Computational Linguistics.\n\n\nMiles Turpin, Julian Michael, Ethan Perez, and Samuel R. Bowman. 2023.\nLanguage models don’t always say what they think: Unfaithful\nexplanations in chain-of-thought prompting. In Proceedings of the\n37th international conference on neural information processing\nsystems, Red Hook, NY, USA. Curran Associates Inc.\n\n\nDennis Ulmer, Jes Frellsen, and Christian Hardmeier. 2022. Exploring\npredictive uncertainty and calibration in NLP: A study on\nthe impact of method & data scarcity. In Yoav\nGoldberg, Zornitsa Kozareva, and Yue Zhang, editors, Findings of the\nassociation for computational linguistics: EMNLP 2022, pages\n2707–2735, Abu Dhabi, United Arab Emirates. Association for\nComputational Linguistics.\n\n\nAlexandra N Uma, Tommaso Fornaciari, Dirk Hovy, Silviu Paun, Barbara\nPlank, and Massimo Poesio. 2021. Learning from disagreement: A survey.\nJournal of Artificial Intelligence Research, 72:1385–1470.\n\n\nAhmet Üstün, Viraat Aryabumi, Zheng Yong, Wei-Yin Ko, Daniel D’souza,\nGbemileke Onilude, Neel Bhandari, Shivalika Singh, Hui-Lee Ooi, Amr\nKayid, Freddie Vargus, Phil Blunsom, Shayne Longpre, Niklas Muennighoff,\nMarzieh Fadaee, Julia Kreutzer, and Sara Hooker. 2024. Aya model: An\ninstruction finetuned open-access multilingual language model. In\nLun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings\nof the 62nd annual meeting of the association for computational\nlinguistics (volume 1: Long papers), pages 15894–15939, Bangkok,\nThailand. Association for Computational Linguistics.\n\n\nKeyon Vafa, Yuntian Deng, David Blei, and Alexander Rush. 2021. Rationales for\nsequential predictions. In Marie-Francine Moens, Xuanjing Huang,\nLucia Specia, and Scott Wen-tau Yih, editors, Proceedings of the\n2021 conference on empirical methods in natural language\nprocessing, pages 10314–10332, Online; Punta Cana, Dominican\nRepublic. Association for Computational Linguistics.\n\n\nJannis Vamvas and Rico Sennrich. 2021a. Contrastive\nconditioning for assessing disambiguation in MT:\nA case study of distilled bias. In Marie-Francine\nMoens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors,\nProceedings of the 2021 conference on empirical methods in natural\nlanguage processing, pages 10246–10265, Online; Punta Cana,\nDominican Republic. Association for Computational Linguistics.\n\n\nJannis Vamvas and Rico Sennrich. 2021b. On the limits of\nminimal pairs in contrastive evaluation. In Jasmijn Bastings,\nYonatan Belinkov, Emmanuel Dupoux, Mario Giulianelli, Dieuwke Hupkes,\nYuval Pinter, and Hassan Sajjad, editors, Proceedings of the fourth\nBlackboxNLP workshop on analyzing and interpreting neural networks for\nNLP, pages 58–68, Punta Cana, Dominican Republic. Association for\nComputational Linguistics.\n\n\nJannis Vamvas and Rico Sennrich. 2022. As little as\npossible, as much as necessary: Detecting over- and undertranslations\nwith contrastive conditioning. In Smaranda Muresan, Preslav Nakov,\nand Aline Villavicencio, editors, Proceedings of the 60th annual\nmeeting of the association for computational linguistics (volume 2:\nShort papers), pages 490–500, Dublin, Ireland. Association for\nComputational Linguistics.\n\n\nEva Vanmassenhove, Christian Hardmeier, and Andy Way. 2018. Getting gender right in\nneural machine translation. In Ellen Riloff, David Chiang, Julia\nHockenmaier, and Jun’ichi Tsujii, editors, Proceedings of the 2018\nconference on empirical methods in natural language processing,\npages 3003–3008, Brussels, Belgium. Association for Computational\nLinguistics.\n\n\nVladimir N. Vapnik. 1995. The nature of statistical learning\ntheory. Springer-Verlag New York, Inc.\n\n\nHelena Vasconcelos, Gagan Bansal, Adam Fourney, Q. Vera Liao, and\nJennifer Wortman Vaughan. 2025. Generation probabilities are not\nenough: Uncertainty highlighting in AI code completions. ACM\nTrans. Comput.-Hum. Interact., 32(1).\n\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention\nis all you need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach,\nR. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in\nneural information processing systems, volume 30. Curran\nAssociates, Inc.\n\n\nDavid Vilar, Markus Freitag, Colin Cherry, Jiaming Luo, Viresh Ratnakar,\nand George Foster. 2023. Prompting\nPaLM for translation: Assessing strategies and\nperformance. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki,\neditors, Proceedings of the 61st annual meeting of the association\nfor computational linguistics (volume 1: Long papers), pages\n15406–15427, Toronto, Canada. Association for Computational Linguistics.\n\n\nRob Voigt and Dan Jurafsky. 2012. Towards a literary machine\ntranslation: The role of referential cohesion. In David Elson, Anna\nKazantseva, Rada Mihalcea, and Stan Szpakowicz, editors, Proceedings\nof the NAACL-HLT 2012 workshop on\ncomputational linguistics for literature, pages 18–25,\nMontréal, Canada. Association for Computational\nLinguistics.\n\n\nElena Voita, Rico Sennrich, and Ivan Titov. 2019a. Context-aware monolingual\nrepair for neural machine translation. In Kentaro Inui, Jing Jiang,\nVincent Ng, and Xiaojun Wan, editors, Proceedings of the 2019\nconference on empirical methods in natural language processing and the\n9th international joint conference on natural language processing\n(EMNLP-IJCNLP), pages 877–886, Hong Kong, China. Association for\nComputational Linguistics.\n\n\nElena Voita, Rico Sennrich, and Ivan Titov. 2019b. When a good translation is\nwrong in context: Context-aware machine translation improves on deixis,\nellipsis, and lexical cohesion. In Anna Korhonen, David Traum, and\nLluís Màrquez, editors, Proceedings of the 57th annual meeting of\nthe association for computational linguistics, pages 1198–1212,\nFlorence, Italy. Association for Computational Linguistics.\n\n\nElena Voita, Rico Sennrich, and Ivan Titov. 2021. Analyzing the source\nand target contributions to predictions in neural machine\ntranslation. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto\nNavigli, editors, Proceedings of the 59th annual meeting of the\nassociation for computational linguistics and the 11th international\njoint conference on natural language processing (volume 1: Long\npapers), pages 1126–1140, Online. Association for Computational\nLinguistics.\n\n\nElena Voita, Pavel Serdyukov, Rico Sennrich, and Ivan Titov. 2018. Context-aware neural machine\ntranslation learns anaphora resolution. In Iryna Gurevych and Yusuke\nMiyao, editors, Proceedings of the 56th annual meeting of the\nassociation for computational linguistics (volume 1: Long papers),\npages 1264–1274, Melbourne, Australia. Association for Computational\nLinguistics.\n\n\nElena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov.\n2019c. Analyzing\nmulti-head self-attention: Specialized heads do the heavy lifting, the\nrest can be pruned. In Anna Korhonen, David Traum, and Lluís\nMàrquez, editors, Proceedings of the 57th annual meeting of the\nassociation for computational linguistics, pages 5797–5808,\nFlorence, Italy. Association for Computational Linguistics.\n\n\nElizabeth Wagner. 1983. Rapid post-editing of\nsystran. In Veronica Lawson, editor, Proceedings of translating\nand the computer 5: Tools for the trade, London, UK. Aslib.\n\n\nEric Wallace, Matt Gardner, and Sameer Singh. 2020. Interpreting\npredictions of NLP models. In Aline Villavicencio and\nBenjamin Van Durme, editors, Proceedings of the 2020 conference on\nempirical methods in natural language processing: Tutorial\nabstracts, pages 20–23, Online. Association for Computational\nLinguistics.\n\n\nLongyue Wang, Siyou Liu, Chenyang Lyu, Wenxiang Jiao, Xing Wang, Jiahao\nXu, Zhaopeng Tu, Yan Gu, Weiyu Chen, Minghao Wu, Liting Zhou, Philipp\nKoehn, Andy Way, and Yulin Yuan. 2024a. Findings of the\nWMT 2024 shared task on discourse-level literary\ntranslation. In Barry Haddow, Tom Kocmi, Philipp Koehn, and Christof\nMonz, editors, Proceedings of the ninth conference on machine\ntranslation, pages 699–700, Miami, Florida, USA. Association for\nComputational Linguistics.\n\n\nLongyue Wang, Chenyang Lyu, Tianbo Ji, Zhirui Zhang, Dian Yu, Shuming\nShi, and Zhaopeng Tu. 2023a. Document-level\nmachine translation with large language models. In Houda Bouamor,\nJuan Pino, and Kalika Bali, editors, Proceedings of the 2023\nconference on empirical methods in natural language processing,\npages 16646–16661, Singapore. Association for Computational Linguistics.\n\n\nLongyue Wang, Zhaopeng Tu, Yan Gu, Siyou Liu, Dian Yu, Qingsong Ma,\nChenyang Lyu, Liting Zhou, Chao-Hong Liu, Yufeng Ma, Weiyu Chen, Yvette\nGraham, Bonnie Webber, Philipp Koehn, Andy Way, Yulin Yuan, and Shuming\nShi. 2023b. Findings\nof the WMT 2023 shared task on discourse-level literary\ntranslation: A fresh orb in the cosmos of LLMs. In\nPhilipp Koehn, Barry Haddow, Tom Kocmi, and Christof Monz, editors,\nProceedings of the eighth conference on machine translation,\npages 55–67, Singapore. Association for Computational Linguistics.\n\n\nWeiyue Wang, Jan-Thorsten Peter, Hendrik Rosendahl, and Hermann Ney.\n2016. CharacTer:\nTranslation edit rate on character level. In Ondřej Bojar, Christian\nBuck, Rajen Chatterjee, Christian Federmann, Liane Guillou, Barry\nHaddow, Matthias Huck, Antonio Jimeno Yepes, Aurélie Névéol, Mariana\nNeves, Pavel Pecina, Martin Popel, Philipp Koehn, Christof Monz, Matteo\nNegri, Matt Post, Lucia Specia, Karin Verspoor, Jörg Tiedemann, et al.,\neditors, Proceedings of the first conference on machine translation:\nVolume 2, shared task papers, pages 505–510, Berlin, Germany.\nAssociation for Computational Linguistics.\n\n\nWenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou.\n2020. MINILM:\nDeep self-attention distillation for task-agnostic compression of\npre-trained transformers. In Proceedings of the 34th\ninternational conference on neural information processing systems,\nRed Hook, NY, USA. Curran Associates Inc.\n\n\nYifan Wang, Zewei Sun, Shanbo Cheng, Weiguo Zheng, and Mingxuan Wang.\n2023c. Controlling\nstyles in neural machine translation with activation prompt. In Anna\nRogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Findings of\nthe association for computational linguistics: ACL 2023, pages\n2606–2620, Toronto, Canada. Association for Computational Linguistics.\n\n\nYue Wang, Cuong Hoang, and Marcello Federico. 2021. Towards modeling\nthe style of translators in neural machine translation. In Kristina\nToutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz\nBeltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao\nZhou, editors, Proceedings of the 2021 conference of the north\namerican chapter of the association for computational linguistics: Human\nlanguage technologies, pages 1193–1199, Online. Association for\nComputational Linguistics.\n\n\nYuxia Wang, Jonibek Mansurov, Petar Ivanov, Jinyan Su, Artem Shelmanov,\nAkim Tsvigun, Osama Mohammed Afzal, Tarek Mahmoud, Giovanni Puccetti,\nand Thomas Arnold. 2024b. SemEval-2024\ntask 8: Multidomain, multimodel and multilingual machine-generated text\ndetection. In Atul Kr. Ojha, A. Seza Doğruöz, Harish Tayyar\nMadabushi, Giovanni Da San Martino, Sara Rosenthal, and Aiala Rosá,\neditors, Proceedings of the 18th international workshop on semantic\nevaluation (SemEval-2024), pages 2057–2079, Mexico City, Mexico.\nAssociation for Computational Linguistics.\n\n\nYuxia Wang, Jonibek Mansurov, Petar Ivanov, Jinyan Su, Artem Shelmanov,\nAkim Tsvigun, Osama Mohammed Afzal, Tarek Mahmoud, Giovanni Puccetti,\nThomas Arnold, Alham Aji, Nizar Habash, Iryna Gurevych, and Preslav\nNakov. 2024c. M4GT-bench:\nEvaluation benchmark for black-box machine-generated text detection.\nIn Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors,\nProceedings of the 62nd annual meeting of the association for\ncomputational linguistics (volume 1: Long papers), pages 3964–3992,\nBangkok, Thailand. Association for Computational Linguistics.\n\n\nAlex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mohananey, Wei Peng,\nSheng-Fu Wang, and Samuel R. Bowman. 2020. BLiMP:\nThe benchmark of linguistic minimal pairs for English.\nTransactions of the Association for Computational Linguistics,\n8:377–392.\n\n\nLeon Weber-Genzel, Siyao Peng, Marie-Catherine De Marneffe, and Barbara\nPlank. 2024. VariErr\nNLI: Separating annotation error from human label\nvariation. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar,\neditors, Proceedings of the 62nd annual meeting of the association\nfor computational linguistics (volume 1: Long papers), pages\n2256–2269, Bangkok, Thailand. Association for Computational Linguistics.\n\n\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter,\nFei Xia, Ed Chi, Quoc V. Le, and Denny Zhou. 2022. Chain-of-thought\nprompting elicits reasoning in large language models. In S. Koyejo,\nS. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors,\nAdvances in neural information processing systems, volume 35,\npages 24824–24837. Curran Associates, Inc.\n\n\nJohannes Welbl, Pontus Stenetorp, and Sebastian Riedel. 2018. Constructing datasets for\nmulti-hop reading comprehension across documents. Transactions\nof the Association for Computational Linguistics, 6:287–302.\n\n\nJohn S. White, Theresa A. O’Connell, and Francis E. O’Mara. 1994. The ARPA\nMT evaluation methodologies: Evolution, lessons, and future\napproaches. In Proceedings of the first conference of the\nassociation for machine translation in the americas, Columbia,\nMaryland, USA.\n\n\nSarah Wiegreffe and Yuval Pinter. 2019. Attention is not not\nexplanation. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun\nWan, editors, Proceedings of the 2019 conference on empirical\nmethods in natural language processing and the 9th international joint\nconference on natural language processing (EMNLP-IJCNLP), pages\n11–20, Hong Kong, China. Association for Computational Linguistics.\n\n\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement\nDelangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan\nFuntowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma,\nYacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, et\nal. 2020. Transformers:\nState-of-the-art natural language processing. In Qun Liu and David\nSchlangen, editors, Proceedings of the 2020 conference on empirical\nmethods in natural language processing: System demonstrations,\npages 38–45, Online. Association for Computational Linguistics.\n\n\nMinghao Wu, Jiahao Xu, Yulin Yuan, Gholamreza Haffari, Longyue Wang,\nWeihua Luo, and Kaifu Zhang. 2025. (Perhaps) beyond\nhuman translation: Harnessing multi-agent collaboration for translating\nultra-long literary texts. Arxiv.\n\n\nZhengxuan Wu, Aryaman Arora, Zheng Wang, Atticus Geiger, Dan Jurafsky,\nChristopher D. Manning, and Christopher Potts. 2024. ReFT:\nRepresentation finetuning for language models. In A. Globerson, L.\nMackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang,\neditors, Advances in neural information processing systems,\nvolume 37, pages 63908–63962. Curran Associates, Inc.\n\n\nAris Xanthos, Sabine Laaha, Steven Gillis, Ursula Stephany, Ayhan\nAksu-Koç, Anastasia Christofidou, Natalia Gagarina, Gordana Hrzica, F.\nNihan Ketrez, Marianne Kilani-Schoch, Katharina Korecky-Kröll, Melita\nKovačević, Klaus Laalo, Marijan Palmović, Barbara Pfeiler, Maria D.\nVoeikova, and Wolfgang U. Dressler. 2011. On the role of\nmorphological richness in the early development of noun and verb\ninflection. First Language, 31(4):461–479.\n\n\nFangyuan Xu, Yixiao Song, Mohit Iyyer, and Eunsol Choi. 2023a. A critical\nevaluation of evaluations for long-form question answering. In Anna\nRogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings\nof the 61st annual meeting of the association for computational\nlinguistics (volume 1: Long papers), pages 3225–3245, Toronto,\nCanada. Association for Computational Linguistics.\n\n\nHaoran Xu, Young Jin Kim, Amr Sharaf, and Hany Hassan Awadalla. 2024. A paradigm shift in\nmachine translation: Boosting translation performance of large language\nmodels. In The twelfth international conference on learning\nrepresentations.\n\n\nKelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan\nSalakhudinov, Rich Zemel, and Yoshua Bengio. 2015. Show, attend and\ntell: Neural image caption generation with visual attention. In\nFrancis Bach and David Blei, editors, Proceedings of the 32nd\ninternational conference on machine learning, volume 37, pages\n2048–2057, Lille, France. PMLR.\n\n\nWeijia Xu, Sweta Agrawal, Eleftheria Briakou, Marianna J. Martindale,\nand Marine Carpuat. 2023b. Understanding and detecting\nhallucinations in neural machine translation via model\nintrospection. Transactions of the Association for Computational\nLinguistics, 11:546–564.\n\n\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou,\nAditya Siddhant, Aditya Barua, and Colin Raffel. 2021. MT5:\nA massively multilingual pre-trained text-to-text transformer. In\nKristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur,\nIz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and\nYichao Zhou, editors, Proceedings of the 2021 conference of the\nnorth american chapter of the association for computational linguistics:\nHuman language technologies, pages 483–498, Online. Association for\nComputational Linguistics.\n\n\nZhen Yang, Fandong Meng, Yuanmeng Yan, and Jie Zhou. 2023. Rethinking the\nword-level quality estimation for machine translation from human\njudgement. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki,\neditors, Findings of the association for computational linguistics:\nACL 2023, pages 2012–2025, Toronto, Canada. Association for\nComputational Linguistics.\n\n\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen,\nRuslan Salakhutdinov, and Christopher D. Manning. 2018. HotpotQA:\nA dataset for diverse, explainable multi-hop question answering. In\nEllen Riloff, David Chiang, Julia Hockenmaier, and Jun’ichi Tsujii,\neditors, Proceedings of the 2018 conference on empirical methods in\nnatural language processing, pages 2369–2380, Brussels, Belgium.\nAssociation for Computational Linguistics.\n\n\nKayo Yin, Patrick Fernandes, Danish Pruthi, Aditi Chaudhary, André F. T.\nMartins, and Graham Neubig. 2021. Do context-aware\ntranslation models pay the right attention? In Chengqing Zong, Fei\nXia, Wenjie Li, and Roberto Navigli, editors, Proceedings of the\n59th annual meeting of the association for computational linguistics and\nthe 11th international joint conference on natural language processing\n(volume 1: Long papers), pages 788–801, Online. Association for\nComputational Linguistics.\n\n\nKayo Yin and Graham Neubig. 2022. Interpreting\nlanguage models with contrastive explanations. In Yoav Goldberg,\nZornitsa Kozareva, and Yue Zhang, editors, Proceedings of the 2022\nconference on empirical methods in natural language processing,\npages 184–198, Abu Dhabi, United Arab Emirates. Association for\nComputational Linguistics.\n\n\nAlexander Yom Din, Taelin Karidi, Leshem Choshen, and Mor Geva. 2024. Jump to conclusions:\nShort-cutting transformers with linear transformations. In Nicoletta\nCalzolari, Min-Yen Kan, Veronique Hoste, Alessandro Lenci, Sakriani\nSakti, and Nianwen Xue, editors, Proceedings of the 2024 joint\ninternational conference on computational linguistics, language\nresources and evaluation (LREC-COLING 2024), pages 9615–9625,\nTorino, Italia. ELRA; ICCL.\n\n\nWu Youyou, Michal Kosinski, and David Stillwell. 2015. Computer-based\npersonality judgments are more accurate than those made by humans.\nProceedings of the National Academy of Sciences,\n112(4):1036–1040.\n\n\nXiang Yue, Boshi Wang, Ziru Chen, Kai Zhang, Yu Su, and Huan Sun. 2023.\nAutomatic\nevaluation of attribution by large language models. In Houda\nBouamor, Juan Pino, and Kalika Bali, editors, Findings of the\nassociation for computational linguistics: EMNLP 2023, pages\n4615–4635, Singapore. Association for Computational Linguistics.\n\n\nMuhammad Bilal Zafar, Michele Donini, Dylan Slack, Cedric Archambeau,\nSanjiv Das, and Krishnaram Kenthapadi. 2021. On the lack of\nrobust interpretability of neural text classifiers. In Chengqing\nZong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, Findings of\nthe association for computational linguistics: ACL-IJCNLP 2021,\npages 3730–3740, Online. Association for Computational Linguistics.\n\n\nMatthew D. Zeiler and Rob Fergus. 2014. Visualizing and\nunderstanding convolutional networks. In David Fleet, Tomas Pajdla,\nBernt Schiele, and Tinne Tuytelaars, editors, 13th european\nconference on computer vision (ECCV), pages 818–833, Switzerland.\nSpringer International Publishing.\n\n\nMatthew D. Zeiler, Graham W. Taylor, and Rob Fergus. 2011. Adaptive\ndeconvolutional networks for mid and high level feature learning. In\n2011 international conference on computer vision (ICCV), pages\n2018–2025.\n\n\nChrysoula Zerva, Frederic Blain, José G. C. De Souza, Diptesh Kanojia,\nSourabh Deoghare, Nuno M. Guerreiro, Giuseppe Attanasio, Ricardo Rei,\nConstantin Orasan, Matteo Negri, Marco Turchi, Rajen Chatterjee, Pushpak\nBhattacharyya, Markus Freitag, and André Martins. 2024. Findings of the quality\nestimation shared task at WMT 2024: Are LLMs\nclosing the gap in QE? In Barry Haddow, Tom Kocmi,\nPhilipp Koehn, and Christof Monz, editors, Proceedings of the ninth\nconference on machine translation, pages 82–109, Miami, Florida,\nUSA. Association for Computational Linguistics.\n\n\nChrysoula Zerva, Frédéric Blain, Ricardo Rei, Piyawat\nLertvittayakumjorn, José G. C. de Souza, Steffen Eger, Diptesh Kanojia,\nDuarte Alves, Constantin Orăsan, Marina Fomicheva, André F. T. Martins,\nand Lucia Specia. 2022. Findings of the\nWMT 2022 shared task on quality estimation. In Philipp\nKoehn, Loïc Barrault, Ondřej Bojar, Fethi Bougares, Rajen Chatterjee,\nMarta R. Costa-jussà, Christian Federmann, Mark Fishel, Alexander\nFraser, Markus Freitag, Yvette Graham, Roman Grundkiewicz, Paco Guzman,\nBarry Haddow, Matthias Huck, Antonio Jimeno Yepes, Tom Kocmi, André\nMartins, Makoto Morishita, et al., editors, Proceedings of the\nseventh conference on machine translation (WMT), pages 69–99, Abu\nDhabi, United Arab Emirates (Hybrid). Association for Computational\nLinguistics.\n\n\nChrysoula Zerva and André F. T. Martins. 2024. Conformalizing machine\ntranslation evaluation. Transactions of the Association for\nComputational Linguistics, 12:1460–1478.\n\n\nBiao Zhang and Rico Sennrich. 2019. Root mean square layer\nnormalization. In Proceedings of the 33rd international conference\non neural information processing systems, Red Hook, NY, USA. Curran\nAssociates Inc.\n\n\nJiacheng Zhang, Huanbo Luan, Maosong Sun, Feifei Zhai, Jingfang Xu, Min\nZhang, and Yang Liu. 2018. Improving the transformer\ntranslation model with document-level context. In Ellen Riloff,\nDavid Chiang, Julia Hockenmaier, and Jun’ichi Tsujii, editors,\nProceedings of the 2018 conference on empirical methods in natural\nlanguage processing, pages 533–542, Brussels, Belgium. Association\nfor Computational Linguistics.\n\n\nPeng Zhang, Zhengqing Guan, Baoxi Liu, Xianghua (Sharon) Ding, Tun Lu,\nHansu Gu, and Ning Gu. 2022. Building user-oriented\npersonalized machine translator based on user-generated textual\ncontent. Proc. ACM Hum.-Comput. Interact., 6(CSCW2).\n\n\nMengjie Zhao and Hinrich Schütze. 2021. Discrete and soft\nprompting for multilingual models. In Marie-Francine Moens, Xuanjing\nHuang, Lucia Specia, and Scott Wen-tau Yih, editors, Proceedings of\nthe 2021 conference on empirical methods in natural language\nprocessing, pages 8547–8555, Online; Punta Cana, Dominican\nRepublic. Association for Computational Linguistics.\n\n\nYao Zhao, Mikhail Khalman, Rishabh Joshi, Shashi Narayan, Mohammad\nSaleh, and Peter J Liu. 2023. Calibrating sequence\nlikelihood improves conditional language generation. In The\neleventh international conference on learning representations.\n\n\nYu Zhao, Alessio Devoto, Giwon Hong, Xiaotang Du, Aryo Pradipta Gema,\nHongru Wang, Xuanli He, Kam-Fai Wong, and Pasquale Minervini. 2025. Steering knowledge\nselection behaviours in LLMs via SAE-based\nrepresentation engineering. In Luis Chiruzzo, Alan Ritter, and Lu\nWang, editors, Proceedings of the 2025 conference of the nations of\nthe americas chapter of the association for computational linguistics:\nHuman language technologies (volume 1: Long papers), pages\n5117–5136, Albuquerque, New Mexico. Association for Computational\nLinguistics.\n\n\nZhixue Zhao and Boxuan Shan. 2024. ReAGent: A model-agnostic\nfeature attribution method for generative language models. AAAI\nWorkshop on Responsible Language Models (ReLM).\n\n\nMeng Zhou, Xin Li, Yue Jiang, and Lidong Bing. 2023. Enhancing\ncross-lingual prompting with dual prompt augmentation. In Anna\nRogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Findings of\nthe association for computational linguistics: ACL 2023, pages\n11008–11020, Toronto, Canada. Association for Computational Linguistics.\n\n\nZhemin Zhu, Delphine Bernhard, and Iryna Gurevych. 2010. A monolingual tree-based\ntranslation model for sentence simplification. In Chu-Ren Huang and\nDan Jurafsky, editors, Proceedings of the 23rd international\nconference on computational linguistics (coling 2010), pages\n1353–1361, Beijing, China. Coling 2010 Organizing Committee.\n\n\nAndy Zou, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard\nRen, Alexander Pan, Xuwang Yin, Mantas Mazeika, Ann-Kathrin Dombrowski,\nShashwat Goel, Nathaniel Li, Michael J. Byun, Zifan Wang, Alex Mallen,\nSteven Basart, Sanmi Koyejo, Dawn Song, Matt Fredrikson, et al. 2024. Enhancing neural\nnetwork transparency through representation analysis.\nOpenReview.\n\n\nVilém Zouhar, Shuoyang Ding, Anna Currey, Tatyana Badeka, Jenyuan Wang,\nand Brian Thompson. 2024. Fine-tuned machine\ntranslation metrics struggle in unseen domains. In Lun-Wei Ku, Andre\nMartins, and Vivek Srikumar, editors, Proceedings of the 62nd annual\nmeeting of the association for computational linguistics (volume 2:\nShort papers), pages 488–500, Bangkok, Thailand. Association for\nComputational Linguistics.\n\n\nVilém Zouhar, Tom Kocmi, and Mrinmaya Sachan. 2025. AI-assisted\nhuman evaluation of machine translation. In Luis Chiruzzo, Alan\nRitter, and Lu Wang, editors, Proceedings of the 2025 conference of\nthe nations of the americas chapter of the association for computational\nlinguistics: Human language technologies (volume 1: Long papers),\npages 4936–4950, Albuquerque, New Mexico. Association for Computational\nLinguistics.\n\n\nVilém Zouhar, Michal Novák, Matúš Žilinec, Ondřej Bojar, Mateo Obregón,\nRobin L. Hill, Frédéric Blain, Marina Fomicheva, Lucia Specia, and Lisa\nYankovskaya. 2021a. Backtranslation\nfeedback improves user confidence in MT, not quality.\nIn Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek\nHakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy\nChakraborty, and Yichao Zhou, editors, Proceedings of the 2021\nconference of the north american chapter of the association for\ncomputational linguistics: Human language technologies, pages\n151–161, Online. Association for Computational Linguistics.\n\n\nVilém Zouhar, Martin Popel, Ondřej Bojar, and Aleš Tamchyna. 2021b. Neural machine\ntranslation quality and post-editing performance. In Marie-Francine\nMoens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors,\nProceedings of the 2021 conference on empirical methods in natural\nlanguage processing, pages 10204–10214, Online; Punta Cana,\nDominican Republic. Association for Computational Linguistics.",
    "crumbs": [
      "Bibliography"
    ]
  },
  {
    "objectID": "chapters/appendix-a.html",
    "href": "chapters/appendix-a.html",
    "title": "Appendix A — Attributing Context Usage in Multilingual NLP",
    "section": "",
    "text": "A.1 Attributing Language Model Generations with the Inseq Toolkit",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Attributing Context Usage in Multilingual NLP</span>"
    ]
  },
  {
    "objectID": "chapters/appendix-a.html#sec-inseq-appendix",
    "href": "chapters/appendix-a.html#sec-inseq-appendix",
    "title": "Appendix A — Attributing Context Usage in Multilingual NLP",
    "section": "",
    "text": "A.1.1 Additional Details on Turkish Gender Bias Case Study\nTable A.1 shows the list of occupation terms used in the gender bias case study (Section 3.3.1). We correlate the ranking of occupations based on the selected attribution metrics and probabilities with U.S. labor statistics1. Table Table 3.3 example was taken from the BUG dataset (Levy et al., 2021).\n\n\n\n\n\n\nTurkish\nEnglish\nTurkish\nEnglish\nTurkish\nEnglish\n\n\n\n\nteknisyen\ntechnician\nmemur\nofficer\nmuhasebeci\naccountant\n\n\npatolog\npathologist\nsüpervizör\nsupervisor\nöğretmen\nteacher\n\n\nmühendis\nengineer\navukat\nlawyer\nişçi\nworker\n\n\nplanlamacı\nplanner\neğitimci\neducator\nyönetici\npractitioner\n\n\nkatip\nclerk\ntesisatçı\nplumber\ndanışman\nconsultant\n\n\neğitmen\ninstructor\nmüfettiş\ninspector\ncerrah\nsurgeon\n\n\ntamirci\nmechanic\nveteriner\nveterinarian\nmüdür\nmanager\n\n\nkimyager\nchemist\nterapist\ntherapist\nmakinist\nmachinist\n\n\nresepsiyonist\nreceptionist\nmimar\narchitect\nkütüphaneci\nlibrarian\n\n\nkuaför\nhairdresser\nressam\npainter\nfırıncı\nbaker\n\n\neczacı\npharmacist\nprogramlamacı\nprogrammer\nkapıcı\njanitor\n\n\nitfaiyeci\nfirefighter\npsikolog\npsychologist\nbilim insanı\nscientist\n\n\ndoktor\nphysician\nsevk memuru\ndispatcher\nmarangoz\ncarpenter\n\n\nkasiyer\ncashier\nhemşire\nnurse\nkomisyoncu\nbroker\n\n\naraştırmacı\ninvestigator\nşef\nchef\nbarmen\nbartender\n\n\ndoktor\ndoctor\nuzman\nspecialist\nsekreter\nsecretary\n\n\nelektrikçi\nelectrician\n\n\n\n\n\n\n\n\n\nTable A.1: List of the 49 Turkish occupation terms and their English translations used in the gender bias case study.\n\n\n\n\n\nA.1.2 Example of Pair Aggregation for Contrastive MT Comparison\nAn example of gender translation pair using the synthetic template of Section 3.3.1 is show in Figure A.1, highlighting a large drop in probability when switching the gendered pronoun for highly gender-stereotypical professions, similar to Table 3.2 results.\n\n\n\nimport inseq\nfrom inseq.data.aggregator import *\n\n# Load the TR-EN translation model and attach the IG method\nmodel = inseq.load_model(\n    \"Helsinki-NLP/opus-mt-tr-en\", \"integrated_gradients\"\n)\n\n# Forced decoding. Return probabilities, no target attr.\nout = model.attribute(\n    [\"O bir teknisyen\", \"O bir teknisyen\"],\n    [\"She is a technician.\",\"He is a technician.\"],\n    step_scores=[\"probability\"],\n)\n\n# Aggregation pipeline composed by two steps:\n# 1. Aggregate subword tokens across all dimensions:\n# 2. Aggregate hidden size to produce token-level attributions\nsubw_aggregator = AggregatorPipeline(\n    [SubwordAggregator, SequenceAttributionAggregator]\n)\nmasculine = out[0].aggregate(aggregator=subw_aggregator)\nfeminine = out[1].aggregate(aggregator=subw_aggregator)\n\n# Take the diff of the scores of the two attributions\nmasculine.show(aggregator=PairAggregator, paired_attr=feminine)\n\n\n\n\n\n\n\nFigure A.1: Comparing attributions for a synthetic Turkish-to-English translation example with underspecified source pronoun gender using a MarianMT Turkish-to-English translation model (Tiedemann, 2020). Values in the visualized attribution matrix show a 46% higher probability of producing the masculine pronoun in the translation and a relative decrease of 18.4% in the importance of the Turkish occupation term compared to the feminine pronoun case.\n\n\n\n\n\nA.1.3 Example of Quantized CAT Attribution\nFigure A.2 presents code used in Section 3.3.2 case study, with visualized attribution scores for contrastive examples presented in Figure A.3.\n\n\n\nimport inseq\nfrom datasets import load_dataset\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# The model is loaded in 8-bit on available GPUs\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"gpt2-xl\", load_in_8bit=True, device_map=\"auto\"\n)\n# Counterfact datasets used by Meng et al. (2022)\ndata = load_dataset(\"NeelNanda/counterfact-tracing\")[\"train\"]\n\n# GPT-2 XL is a transformer model with 48 layers\nfor layer in range(48):\n    attrib_model = inseq.load_model(\n        model,\n        \"layer_gradient_x_activation\",\n        tokenizer=\"gpt2-xl\",\n        target_layer=model.transformer.h[layer].mlp,\n    )\n    for i, ex in data:\n        # e.g. \"The capital of Second Spanish Republic is\"\n        # -&gt; Madrid (true) / Paris (false)\n        prompt = ex[\"relation\"].format(ex[\"subject\"])\n        true_answer = prompt + ex[\"target_true\"]\n        false_answer = prompt + ex[\"target_false\"] \n        # Contrastive attribution of true vs false answer\n        out = attrib_model.attribute(\n            prompt,\n            true_answer,\n            attributed_fn=\"contrast_prob_diff\",\n            contrast_targets=false_answer,\n            show_progress=False,\n        )\n\n\nFigure A.2: Example code to contrastively attribute factual statements from the Counterfact Tracing dataset, using Layer Gradient \\(\\times\\) Activation to compute importance scores until intermediate layers of the GPT2-XL model.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure A.3: Visualization of contrastive attribution scores on a subset of layers (23 to 48) for some selected dataset examples. Plot labels show the contrastive pairs of false \\(\\rightarrow\\) true answer used as attribution targets.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Attributing Context Usage in Multilingual NLP</span>"
    ]
  },
  {
    "objectID": "chapters/appendix-a.html#sec-pecore-appendix",
    "href": "chapters/appendix-a.html#sec-pecore-appendix",
    "title": "Appendix A — Attributing Context Usage in Multilingual NLP",
    "section": "A.2 Quantifying Context Usage in Neural Machine Translation",
    "text": "A.2 Quantifying Context Usage in Neural Machine Translation\n\nA.2.1 Details on Translation Evaluation\nWe compute BLEU using the SACREBLEU library (Post, 2018) with default parameters nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1. The models fine-tuned with source and target context clearly outperform the ones trained with source only, both in terms of generic translation quality and context-sensitive disambiguation accuracy. This motivates our choice to focus primarily on those models for our main analysis. All models are available in the following Huggingface organization: https://hf.co/context-mt. The \\(S_{\\text{ctx}}\\) models correspond to those matching context-mt/scat-&lt;MODEL\\_TYPE&gt;-ctx4-cwd1-en-fr, while \\(S+T_{\\text{ctx}}\\) models have the context-mt/scat-&lt;MODEL\\_TYPE&gt;-target-ctx4-cwd0-en-fr identifier.\n\n\nA.2.2 Full CTI and CCI Results\nFigure A.4 and Figure A.5 present the CTI plausibility of all tested models for the Macro F1 and AUPRC metrics, similarly to Figure 4.3 in the main analysis.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure A.4: Macro F1 of contrastive metrics for context-sensitive target token identification (CTI) on the full datasets (left) or on ok-cs context-sensitive subsets (right). Top to bottom: ⓵ OpusMT Small S\\(_\\text{ctx}\\) ⓶ OpusMT Large S\\(_\\text{ctx}\\) ⓷ mBART-50 S\\(_\\text{ctx}\\) ⓸ OpusMT Small S+T\\(_\\text{ctx}\\) ⓹ OpusMT Large S+T\\(_\\text{ctx}\\) ⓺ mBART-50 S+T\\(_\\text{ctx}\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure A.5: Area Under Precision-Recall Curve (AUPRC) of contrastive metrics for context-sensitive target token identification (CTI) on the full datasets (left) or on ok-cs context-sensitive subsets (right). Top to bottom: ⓵ OpusMT Small S\\(_\\text{ctx}\\) ⓶ OpusMT Large S\\(_\\text{ctx}\\) ⓷ mBART-50 S\\(_\\text{ctx}\\) ⓸ OpusMT Small S+T\\(_\\text{ctx}\\) ⓹ OpusMT Large S+T\\(_\\text{ctx}\\) ⓺ mBART-50 S+T\\(_\\text{ctx}\\).\n\n\n\nFigure A.6 Figure A.7 present the CCI plausibility of all tested models for the Macro F1 and AUPRC metrics, similarly to Figure 4.4 in the main analysis.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure A.6: Macro F1 of CCI methods over full datasets using models trained with only source context (left) or with source+target context (right). Boxes and red median lines show CCI results based on gold context-sensitive tokens. Dotted bars show median CCI scores obtained from context-sensitive tokens identified by KL-Divergence during CTI (E2E settings). Top to bottom: ⓵ OpusMT Small S\\(_\\text{ctx}\\) and S+T\\(_\\text{ctx}\\) ⓶ OpusMT Large S\\(_\\text{ctx}\\) and S+T\\(_\\text{ctx}\\) ⓷ mBART-50 S\\(_\\text{ctx}\\) and S+T\\(_\\text{ctx}\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure A.7: Area Under Precision-Recall Curve (AUPRC) of CCI methods over full datasets using models trained with only source context (left) or with source+target context (right). Boxes and red median lines show CCI results based on gold context-sensitive tokens. Dotted bars show median CCI scores obtained from context-sensitive tokens identified by KL-Divergence during CTI (E2E settings). Top to bottom: ⓵ OpusMT Small S\\(_\\text{ctx}\\) and S+T\\(_\\text{ctx}\\) ⓶ OpusMT Large S\\(_\\text{ctx}\\) and S+T\\(_\\text{ctx}\\) ⓷ mBART-50 S\\(_\\text{ctx}\\) and S+T\\(_\\text{ctx}\\).\n\n\n\n\n\nA.2.3 PECoRe for Other Language Generation Tasks\nThis section complements our MT analysis and by demonstrating the applicability of PECoRe to other model architectures and different language generation tasks. Table A.2 presents some examples. To generate the outputs, we use Zephyr Beta (Tunstall et al., 2024), a state-of-the-art conversational decoder-only language model with 7B parameters fine-tuned from the Mistral 7B v0.1 pre-trained model (Jiang et al., 2023). We follow the same setup of Section 4.5, using KL-Divergence as CTI metric, \\(\\nabla_{\\text{diff}}\\) as CCI method and setting both \\(s_\\text{CTI}\\) and \\(s_\\text{CCI}\\) to two standard deviations above the per-example mean.\nConstrained Story Generation In the first example, the model is asked to generate a story about , which is defined as a planet with an alien race only in context \\(C_x\\). We observe a plausible influence of several context components throughout the generation process, leading to a short story respecting the constraint specified in the system prompt provided as context.\nFactual Question Answering In the second example, the model is asked to retrieve date information from the context and perform a calculation to derive the age of a fictional building. While the non-contextual generation \\(\\tilde y\\) hallucinates an age and a construction date associated to a real historical landmark, contextual generation \\(\\hat y\\) produces a wrong age, but plausibly relies on the date provided in \\(C_x\\) during generation. Interestingly, we can also identify when the system instruction of “keeping answers concise” intervenes during generation.\nInformation Extraction The last example simulates a retrieval-augmented generation scenario in which a fictional refund policy is used as context to answer user queries. In this scenario, contextual generation \\(\\hat y\\) correctly identifies the user query as leading to a no-refund situation due to the limited refund timeline stated in the policy, and the corresponding timeline () is identified as a contextual cue leading to the model’s negative response.\n\n\n\n\n\n\nConstrained Story Generation\n\n\n\\(C_x\\): &lt;|system|&gt; Here is some context about planet⓵⓶ Florbz. It is inhabited by a race of aliens called Squortz⓷, which communicate⓸ only by saying ‘Qiz!’⓹ &lt;/s&gt;\n\n\n\\(x\\): &lt;|user|&gt; Tell me a very short story about Florbz &lt;/s&gt; &lt;|assistant|&gt;\n\n\n\\(\\tilde y\\): Florbz was a curious little creature, half flower and half buzzing bee. He loved nothing more than exploring the garden he called home, flitting from bloom to bloom, spreading pollen as he went. &lt;/s&gt;\n\n\n\\(\\hat y\\): On the vibrant⓵ and colorful planet⓶ of Florbz lived a curious creature named Zorg. He was different from his fellow Squortz⓷ as he longed to learn their language⓸ beyond just ‘Qiz!’⓹ &lt;/s&gt;\n\n\nFactual Question Answering\n\n\n\\(C_x\\): &lt;|system|&gt; Provide a concise answer containing only few words⓵&lt;/s&gt; &lt;|user|&gt; In the heart of Groningen, nestled between quaint cobbled streets and the meandering canals, stands the Pannekoekenkerk, a structure as rich in history as it is in legend. Built in 1877⓵⓶, this peculiarly named ‘Pancake Church’ was once a humble bakery, famed for its delectable pannekoeken, which drew patrons from all corners of the Netherlands. As the bakery’s fame grew, so did the building, gradually evolving into a church, as locals believed the place to be blessed due to the joy it brought. Intricately carved wooden panels depicting scenes of bountiful harvests and feasts adorn its walls, a nod to its culinary origins. Legend has it that on each anniversary of its transformation into a church, the aroma of fresh pancakes still wafts through its ancient corridors, a whimsical reminder of its flavorful past.\n\n\n\\(x\\): Question: How old is the Pannekoekenkerk in Groningen?&lt;/s&gt; &lt;|assistant|&gt;\n\n\n\\(\\tilde y\\): Answer: 478 (1428) Explanation: The correct answer should be ‘How old is the Martinitoren tower in Groningen?’ instead, as it is actually the oldest building in the city, dating back to around 1428 AD &lt;/s&gt;\n\n\n\\(\\hat y\\): The Pannekoekenkerk in Groningen is 143⓵ years old (built in 1877⓶). &lt;/s&gt;\n\n\nInformation Extraction\n\n\n\\(C_x\\): &lt;|system|&gt; Fictional Clothing Refund Policy⓶\n\n\n\n\n\nEffective Date: November 15th, 2023\n\n\n\n\n\nGeneral Policy:\n\n\nAt Fictional Clothing, your satisfaction is our priority. We offer a hassle-free refund policy for our customers. If you’re not completely satisfied with your purchase, you can return it for a refund or exchange within 30 days⓵⓷ of purchase.\n\n\n\n\n\nConditions for Refund:\n\n\n\n\n\nItems must be returned in original condition: Unworn, unwashed, with all tags attached.\n\n\nProof of purchase required: A valid receipt or order number is necessary for all refunds or exchanges.\n\n\nRefund method: Refunds will be issued to the original method of payment.\n\n\nFinal sale items can be refunded only if they are returned in-store.\n\n\n\n\n\nNon-Refundable Items:\n\n\n\n\n\nUltimate sale items: Products marked as ‘Ultimate Sale’ cannot be returned or exchanged\n\n\nPersonalized items: Custom-made or personalized items are non-refundable.\n\n\nGift cards: Purchased gift cards are not eligible for a refund.\n\n\n\n\n\nOnline Purchases:\n\n\n\n\n\nReturn shipping: Customers are responsible for return shipping costs.\n\n\nProcessing time: Refunds will be processed within 7-10 business days of receiving the returned item.\n\n\n\n\n\nIn-Store Purchases:\n\n\n\n\n\nReturn to any store: Items purchased in-store can be returned to any of our retail locations.\n\n\nInstant refund: In-store returns will be processed immediately.\n\n\n\n\n\nExchange Policy:\n\n\n\n\n\nSize or color exchange: Items can be exchanged for a different size or color, subject to availability.\n\n\nIn-store exchange: For faster service, we recommend exchanging items at any of our retail locations.\n\n\n\n\n\nDamaged or Defective Items:\n\n\n\n\n\nReport immediately: Please contact our customer service if you receive a damaged or defective item.\n\n\nFull refund or exchange: We will offer a full refund or exchange for such items.\n\n\n\n\n\nChanges to Refund Policy:\n\n\nFictional Clothing reserves the right to amend this refund policy at any time.\n\n\n\n\n\nContact Us:\n\n\nFor more information or assistance with returns, please contact our customer service team. &lt;/s&gt;\n\n\n\\(x\\): &lt;|user|&gt; I have an unworn item that I bought two months ago and that I would like to return at one of your locations. Is it possible for me to get reimbursed? &lt;/s&gt; &lt;|assistant|&gt;\n\n\n\\(\\tilde y\\): The availability of returns and refunds may vary by the specific store or retailer you purchased from, so it’s best to check their return policy. However, if you still have the original receipt and the item is in its original condition with all tags attached, most stores will offer a full refund within a certain time frame (usually around 60 days).\n\n\n\\(\\hat y\\): Since your purchase was made two months ago, unfortunately⓵, it falls outside our policy’s⓶ 30⓷ days timeline for returns.\n\n\n\n\n\nTable A.2: Examples of cue-target pairs (identified by indices) extracted by PECoRe for the outputs of Zephyr 7B Beta across several language generation tasks. Model input \\(x\\) is provided without context to produce non-contextual generation \\(\\tilde y\\), or chained to preceding context \\(C_x\\) to generate the contextual output \\(\\hat y\\) used by PECoRe.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Attributing Context Usage in Multilingual NLP</span>"
    ]
  },
  {
    "objectID": "chapters/appendix-a.html#sec-mirage-appendix",
    "href": "chapters/appendix-a.html#sec-mirage-appendix",
    "title": "Appendix A — Attributing Context Usage in Multilingual NLP",
    "section": "A.3 Answer Attribution for Trustworthy Retrieval-Augmented Generation",
    "text": "A.3 Answer Attribution for Trustworthy Retrieval-Augmented Generation\n\nA.3.1 Answer Attribution on the Full XOR-AttriQA\nDifferently from the concatenation setup in Chapter 5, we also test Mirage on the full XOR-AttriQA dataset by constraining CORA generation to match the annotated answer \\(\\mathbf{y}\\). We adopt a procedure similar to Muller et al. (2023) by considering a single document-answer pair \\((\\text{doc}_i, \\mathbf{y})\\) at a time, and using Mirage’s CTI step to detect whether \\(\\mathbf{y}\\) is sensitive to the context \\(\\text{doc}_i\\). Results in Table A.3 show that Mirage achieves performances in line with other AA methods despite these approaches employing ad-hoc validators trained with as many as 540B parameters.\n\n\n\n\n\n\nMethod\nExtra Requirements\nBN\nFI\nJA\nRU\nTE\nAvg. / Std\n\n\n\n\nmT5 XXL\\(_{\\text{NLI}}\\)\n11B NLI model (250 FT ex.)\n81.9\n80.9\n94.5\n87.1\n88.7\n86.6 / 4.9\n\n\n11B NLI model (100k FT ex.)\n89.4\n88.3\n91.5\n91.0\n92.4\n90.5 / 1.5\n\n\n11B NLI model (1M FT ex.)\n91.1\n90.4\n93.0\n92.9\n93.8\n92.2 / 1.3\n\n\nPALM2\\(_{\\text{LORA}}\\)\n540B LLM (250 FT ex.)\n91.5\n88.3\n94.7\n93.7\n93.7\n92.4 / 2.3\n\n\nPALM2\\(_{\\text{ex}}\\)\n540B LLM (250 FT ex.)\n92.3\n92.6\n96.4\n94.5\n94.8\n94.1 / 1.5\n\n\nPALM2\\(_{\\text{ex}}\\)\n540B LLM (4-shot prompting)\n91.5\n87.4\n92.0\n90.5\n90.6\n90.4 / 1.6\n\n\nPALM2\\(_{\\text{CoT}}\\)\n540B LLM (4-shot prompting)\n83.7\n78.8\n71.7\n81.9\n84.7\n80.2 / 4.7\n\n\nMirage\\(_{\\text{cal}}\\) (Ours)\n500 AA calibration ex.\n82.2\n82.5\n92.0\n87.7\n90.2\n86.9 / 4.0\n\n\nMirage\\(_{\\text{ex}}\\) (Ours)\n--\n79.0\n74.1\n90.8\n82.6\n86.9\n82.7 / 5.8\n\n\n\n\n\nTable A.3: Agreement % of Mirage and entailment-based baselines with human AA on the full XOR-AttriQA using CORA for RAG (annotated answers not matching the LM’s natural generation are force-decoded). Extra Requirements: data/models needed for AA in addition to the RAG model itself. Best overall and best validator-free scores are highlighted. PALM and mT5 results are taken from Muller et al. (2023).\n\n\n\n\n\nA.3.2 ELI5 Evaluation with Standard Prompt\n\n\n\n\n\n\nSelf-citation (Gao et al., 2023)\n\n\nInstruction: Write an accurate, engaging, and concise answer for the given question using only the provided search results (some of which might be irrelevant) and cite them properly. Use an unbiased and journalistic tone. Always cite for any factual claim. When citing several search results, use [1][2][3]. Cite at least one document and at most three documents in each sentence. If multiple documents support the sentence, only cite a minimum sufficient subset of the documents.\n\n\nStandard\n\n\nInstruction: Write an accurate, engaging, and concise answer for the given question using only the provided search results (some of which might be irrelevant). Use an unbiased and journalistic tone.\n\n\n\n\n\nTable A.4: Self-citation prompt, taken from Gao et al. (2023), and standard prompt with no citation instruction.\n\n\n\nIn the main experiments, we use self-citation prompts by Gao et al. (2023) for Mirage answer attribution to control for the effect of different prompts on model responses, enabling a direct comparison with self-citation. In Table A.5, we provide additional results where a standard prompt without citation instructions is used (“Standard” prompt in Table A.4). We observe the overall citation quality of Mirage drops when a standard prompt is used instead of self-citation instructions. We conjecture this might be due to answers that are, in general, less attributable to the provided context due to a lack of explicit instructions to do so. We also observe higher correctness and fluency in the standard prompt setting, suggesting a trade-off between answer and citation quality.\n\n\n\n\n\n\nModel\nPrompt\nAnswer Attribution\nFilter\nCitation\nCorr. \\(\\uparrow\\)\nFluency \\(\\uparrow\\)\n\n\nPrec.\nRec.\nF1\n\n\n\n\nZephyr\nSelf-citation\nSelf-citation\n--\n41.4\n24.3\n30.6\n9.9\n28.6\n\n\nMirage\\(_{\\text{ex}}\\)\nTop 3\n38.3\n46.2\n41.9\n\n\nTop 5%\n44.7\n46.5\n45.6\n\n\nStandard\nMirage\\(_{\\text{ex}}\\)\nTop 3\n29.8\n34.5\n32.0\n11.3\n34.3\n\n\nTop 5%\n34.1\n34.2\n34.1\n\n\nLLaMA\nSelf-citation\nSelf-citation\n--\n37.9\n19.8\n26.0\n11.8\n24.8\n\n\nMirage\\(_{\\text{ex}}\\)\nTop 3\n21.8\n29.6\n25.1\n\n\nTop 5%\n26.2\n29.1\n27.6\n\n\nStandard\nMirage\\(_{\\text{ex}}\\)\nTop 3\n17.8\n20.9\n19.2\n13.0\n26.4\n\n\nTop 5%\n21.1\n20.1\n20.6\n\n\n\n\n\nTable A.5: Citation quality (F1, Recall, Precision), correctness (Corr.), and fluency of self-citation and Mirage on ELI5 when using self-citation instruction and standard prompts of Table A.4.\n\n\n\n\n\nA.3.3 More Examples of Disagreement\nTable A.6, Table A.7 and Table A.8 show three cases where Mirage answer attributions disagree with self-citation attributions of the same generation.2 We adopt the Top-5% threshold for CCI Filtering. In Table A.6, the generated answer becomes the consistent description `cancel the alarm’ as mentioned in Document [3]. In this case, Mirage attributes this sentence to the corresponding Document [3] while NLI maintains its attribution of Document [3] due to lexical overlap, as suggested in Chapter 5.\n\n\n\n\n\n\nInput: Prompt + Retrieved docs (N=5) + Query\n\n\nDocument [2] […]\n\n\nDocument [3] […] What will happen if you accidentally set off your security system? The siren will sound and it will be loud, but you should be able to stop the siren by entering your code into your keypad. […] You may also receive a call from the response center, checking to see if it is an actual alarm or a false alarm. You simply need to cancel the alarm with the response center by telling them your password (different from your keypad code). […]\n\n\nDocument [4] […]\n\n\nQuery: How does a small paper library bar code set off the security alarm when you haven’t checked a book out.\n\n\nAnswer attribution results\n\n\nSelf-citation: […] The response center may contact the individual to confirm if it’s a false alarm, and the individual can cancel the alarm by providing their password at the keypad. [\\(\\varnothing\\)] […]\n\n\nMirage: … The response center\\(^{(3)}\\) may\\(^{(3)}\\) contact the individual to confirm if it\\(^{(3)}\\)’s a false alarm, and the individual\\(^{(3)}\\) can cancel\\(^{(3)}\\) the alarm by providing\\(^{(3)}\\) their password at the keypad\\(^{(3)}\\). [3] …\n\n\nNLI (True model): [3] entails the answer sentence.\n\n\n\n\n\nTable A.6: Example described in Chapter 5: Mirage attributes the generation to Document [3] when cancel is used instead of prevent.\n\n\n\nOn several occasions, we observe that Mirage attributes all occurrences of lexically similar tokens in the context when the LLM is generating the same word. For example, in Table A.7 the named entity “Science ABC” is mentioned in both Document [1] and [4], and Mirage finds both occurrences as salient towards the prediction of the same entity in the output. Similarly, in Table A.8, the generated word `Document’ is attributed to the previous mentions of the same word in the context. In both cases, when moving from token-level to sentence-level AA, this dependence would result in wrong AA according to NLI, since the documents are not entailing the answer, but rather making a specific token more likely. These cases reflect the possible discrepancy between AA intended as logical entailment and actual context usage during generation. Future work could explore more elaborate ways to aggregate granular information at sentence level while preserving faithfulness to context usage.\n\n\n\n\n\n\nInput: Prompt + Retrieved docs (N=5) + Query\n\n\nDocument [1] (Title: Why Do Airlines Sell More Tickets Than The Aircraft’s Capacity? » Science ABC): […] They know that some people simply won’t show up for their appointments […] To account for this discrepancy, where the flight would have to fly with some very expensive seats empty, they decide to opt for a more profitable strategy. Assuming that out of the maximum capacity of 200 seats only 185 show up for the flight, this leaves 15 seats available. The airline could try to sell these seats quickly to avoid losses, but this rarely happens. What the airline does instead is try to sell 220 seats in advance.\n\n\nDocument [2] […]\n\n\nDocument [3] […]\n\n\nDocument [4] (Title: Why Do Airlines Sell More Tickets Than The Aircraft’s Capacity? » Science ABC): your own interests, as the airlines have clearly looked after theirs. First of all, if the airline is offering you a seat on a different flight, check if that seat is definitely available. If it is also full and it looks like you might be bumped yet again, you might as well be stranded! If the airline is offering compensation, such as free meals, hotel costs, and transportation between the airport and the hotel, look for the most profitable outcome. The airline might offer you vouchers for future flights, but since airline employees are given guidelines for negotiation, you might\n\n\nDocument [5] […]\n\n\nQuery: Why are airlines able to sell more seats than are available on a flight?\n\n\nAnswer attribution results\n\n\nSelf-citation: […] According to a report from Science ABC [1], airlines assume that out of the maximum capacity of 200 seats, only 185 will actually board the aircraft. To avoid empty seats and potential losses, airlines opt to sell up to 220 seats in advance [1]. […]\n\n\nMirage: […] According to a report from Science ABC\\(^{(1)(4)}\\), airlines assume that out\\(^{(1)}\\) of the maximum capacity\\(^{(1)}\\) of 200 seats\\(^{(1)}\\), only 185\\(^{(1)}\\) will actually board the aircraft. [1][4] […]\n\n\nNLI (True model): Only [1] entails the answer sentence.\n\n\n\n\n\nTable A.7: Example of counterintuitive Mirage attribution: the generation is grounded to Document [4] because the named entity Science ABC is made more probable by the occurrence of the same named entity Science ABC. However, Document [4] does not logically entail the answer.\n\n\n\n\n\n\n\n\n\nInput: Prompt + Retrieved docs (N=5) + Query\n\n\nDocument [1] (Title: Why Do Airlines Sell More Tickets Than The Aircraft’s Capacity? » Science ABC): board the aircraft. They know that some people simply won’t show up for their appointments (just like at hospitals, hotels, restaurants etc.). To account for this discrepancy, where the flight would have to fly with some very expensive seats empty, they decide to opt for a more profitable strategy. Assuming that out of the maximum capacity of 200 seats only 185 show up for the flight, this leaves 15 seats available. The airline could try to sell these seats quickly to avoid losses, but this rarely happens. What the airline does instead is try to sell 220 seats in advance.\n\n\nDocument [2] […]\n\n\nDocument [3] […]\n\n\nDocument [4] (Title: Why Do Airlines Sell More Tickets Than The Aircraft’s Capacity? » Science ABC): your own interests, as the airlines have clearly looked after theirs. First of all, if the airline is offering you a seat on a different flight, check if that seat is definitely available. If it is also full and it looks like you might be bumped yet again, you might as well be stranded! If the airline is offering compensation, such as free meals, hotel costs, and transportation between the airport and the hotel, look for the most profitable outcome. The airline might offer you vouchers for future flights, but since airline employees are given guidelines for negotiation, you might\n\n\nDocument [5] […]\n\n\nQuery: Why are airlines able to sell more seats than are available on a flight?\n\n\nAnswer attribution results\n\n\nSelf-citation: […] According to a source in Document [1], airlines assume that not all passengers will show up for their flights, leaving some seats empty. […]\n\n\nMirage: […] According to a source in Document\\(^{(4)}\\), airlines assume that not\\(^{(1)}\\) all passengers will show up for their flights, leaving some seats empty. [1][4] […]\n\n\nNLI (True model): Only [1] entails the answer sentence.\n\n\n\n\n\nTable A.8: Example of counterintuitive Mirage attribution: Document [4] is attributed by Mirage due to the repetition of the keyword `Document’.\n\n\n\n\n\n\n\nTianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen. 2023. Enabling large language models to generate text with citations. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 conference on empirical methods in natural language processing, pages 6465–6488, Singapore. Association for Computational Linguistics.\n\n\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2023. Mistral 7B.\n\n\nShahar Levy, Koren Lazar, and Gabriel Stanovsky. 2021. Collecting a large-scale gender bias dataset for coreference resolution and machine translation. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Findings of the association for computational linguistics: EMNLP 2021, pages 2470–2480, Punta Cana, Dominican Republic. Association for Computational Linguistics.\n\n\nBenjamin Muller, John Wieting, Jonathan Clark, Tom Kwiatkowski, Sebastian Ruder, Livio Soares, Roee Aharoni, Jonathan Herzig, and Xinyi Wang. 2023. Evaluating and modeling attribution for cross-lingual question answering. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 conference on empirical methods in natural language processing, pages 144–157, Singapore. Association for Computational Linguistics.\n\n\nMatt Post. 2018. A call for clarity in reporting BLEU scores. In Ondřej Bojar, Rajen Chatterjee, Christian Federmann, Mark Fishel, Yvette Graham, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes, Philipp Koehn, Christof Monz, Matteo Negri, Aurélie Névéol, Mariana Neves, Matt Post, Lucia Specia, Marco Turchi, and Karin Verspoor, editors, Proceedings of the third conference on machine translation: Research papers, pages 186–191, Brussels, Belgium. Association for Computational Linguistics.\n\n\nJörg Tiedemann. 2020. The tatoeba translation challenge – realistic data sets for low resource and multilingual MT. In Loïc Barrault, Ondřej Bojar, Fethi Bougares, Rajen Chatterjee, Marta R. Costa-jussà, Christian Federmann, Mark Fishel, Alexander Fraser, Yvette Graham, Paco Guzman, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes, Philipp Koehn, André Martins, Makoto Morishita, Christof Monz, Masaaki Nagata, Toshiaki Nakazawa, et al., editors, Proceedings of the fifth conference on machine translation, pages 1174–1182, Online. Association for Computational Linguistics.\n\n\nLewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Clémentine Fourrier, Nathan Habib, Nathan Sarrazin, Omar Sanseviero, Alexander M. Rush, and Thomas Wolf. 2024. Zephyr: Direct distillation of LM alignment. In Proceedings of the 1st conference on language modeling (COLM).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Attributing Context Usage in Multilingual NLP</span>"
    ]
  },
  {
    "objectID": "chapters/appendix-a.html#footnotes",
    "href": "chapters/appendix-a.html#footnotes",
    "title": "Appendix A — Attributing Context Usage in Multilingual NLP",
    "section": "",
    "text": "https://github.com/rudinger/winogender-schemas (bls_pct_female column)↩︎\nNote that we remove citation tags in self-citation generated answers and use MIRAGE to attribute the resulting answers, as introduced in Section 5.5.↩︎",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Attributing Context Usage in Multilingual NLP</span>"
    ]
  },
  {
    "objectID": "chapters/appendix-b.html",
    "href": "chapters/appendix-b.html",
    "title": "Appendix B — Conditioning Generation for Personalized Machine Translation",
    "section": "",
    "text": "B.1 Retrieval and Marking for Attribute-Controlled Translation",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Conditioning Generation for Personalized Machine Translation</span>"
    ]
  },
  {
    "objectID": "chapters/appendix-b.html#sec-ramp-appendix",
    "href": "chapters/appendix-b.html#sec-ramp-appendix",
    "title": "Appendix B — Conditioning Generation for Personalized Machine Translation",
    "section": "",
    "text": "B.1.1 Prompt Templates\nFormality-Controlled Translation *Here is a sentence: {src} Here is its lang translation written in a attr style: {tgt} The translated sentence conveys a attr style by using words such as ‘w1’, ‘w2’.\nGender-Controlled Translation Here is a sentence: {src} Here is its lang translation in which the person is attr: {tgt} In the translation, the attr gender of the person is made explicit by words such as ‘w1’, ‘w2’.\n\n\nB.1.2 Full Per-language Results\n\nTable B.1: Detailed scores of same-language prompting on CoCoA-MT (preliminary evaluation).1\nTable B.2: Full results of same-language prompting on CoCoA-MT (full evaluation).\nTable B.3: Full results of same-language prompting on MT-GenEval (full evaluation).\nTable B.4: Full results of cross-lingual prompting on CoCoA-MT.\nTable B.5: Full results of cross-lingual prompting on MT-GenEval.\n\n\n\n\n\n\n\n\n\nBLEU\nCOMET\nSentential Accuracy\n\n\n0\n4\n8\n16\n32\n0\n4\n8\n16\n32\n0\n4\n8\n16\n32\n\n\n\n\nBLOOM 7.1B\nbase\n22\n29\n30\n31\n21\n.16\n.58\n.59\n.60\n-.09\n.56\n.76\n.84\n.88\n.73\n\n\nRamp\n31\n32\n33\n25\n.60\n.61\n.62\n.15\n.84\n.89\n.91\n.84\n\n\nXGLM 7.5B\nbase\n12\n25\n27\n28\n29\n-.53\n.44\n.45\n.50\n.52\n.52\n.76\n.84\n.85\n.89\n\n\nRamp\n27\n28\n28\n30\n.45\n.48\n.47\n.48\n.86\n.90\n.91\n.92\n\n\nGPT-NeoX 20B\nbase\n23\n28\n29\n29\n29\n.11\n.27\n.27\n.27\n.28\n.56\n.80\n.85\n.85\n.95\n\n\nRamp\n29.\n30\n30\n29\n.28\n.31\n.31\n.28\n.85\n.89\n.89\n.87\n\n\nBLOOM 175B\nbase\n30\n38\n39\n39\n--\n.48\n.73\n.74\n.75\n--\n.61\n.90\n.95\n.95\n--\n\n\nRamp\n39\n40\n40\n--\n.74\n.74\n.76\n--\n.95\n.97\n.97\n--\n\n\n\n\n\nTable B.1: Detailed scores of same-language prompting on CoCoA-MT (preliminary evaluation). Numbers in the header represent the number of in-context examples used for prompting, including zero-shot prompting (0). Scores are averaged across two available formality values (formal, informal) and languages (ES, FR, HI, PT).\n\n\n\n\n\n\n\n\n\n\n\n\nES\nFR\nHI\nPT\nAVG\n\n\nF\nI\nF\nI\nF\nI\nF\nI\n\n\n\n\nXGLM 7.5B\nbase\nBLEU\n30.1\n33.0\n30.7\n28.8\n18.5\n16.9\n35.7\n35.4\n28.6\n\n\nCOMET\n0.500\n0.527\n0.348\n0.350\n0.454\n0.425\n0.547\n0.554\n0.463\n\n\nL-Acc\n0.524\n0.966\n0.977\n0.633\n0.976\n0.744\n0.931\n0.928\n0.835\n\n\nS-Acc\n0.507\n0.958\n0.953\n0.840\n0.963\n0.748\n0.888\n0.912\n0.846\n\n\n+mark\nBLEU\n31.0\n33.2\n29.4\n27.4\n19.2\n18.6\n35.7\n35.5\n28.7\n\n\nCOMET\n0.498\n0.541\n0.207\n0.188\n0.439\n0.409\n0.552\n0.552\n0.423\n\n\nL-Acc\n0.728\n0.972\n0.985\n0.923\n0.986\n0.860\n0.960\n0.947\n0.920\n\n\nS-Acc\n0.697\n0.958\n0.963\n0.917\n0.983\n0.838\n0.927\n0.937\n0.902\n\n\nRamp\nBLEU\n32.8\n33.5\n32.7\n31.0\n21.0\n20.3\n34.2\n34.4\n30.0\n\n\nCOMET\n0.480\n0.511\n0.314\n0.302\n0.502\n0.491\n0.488\n0.522\n0.451\n\n\nL-Acc\n0.842\n0.963\n0.989\n0.926\n0.993\n0.885\n0.961\n0.943\n0.938\n\n\nS-Acc\n0.803\n0.952\n0.975\n0.922\n0.98\n0.873\n0.928\n0.948\n0.923\n\n\nBLOOM 175B\nbase\nBLEU\n44.3\n45.0\n42.9\n41.0\n27.1\n25.8\n47.3\n45.7\n39.9\n\n\nCOMET\n0.728\n0.759\n0.611\n0.600\n0.673\n0.645\n0.762\n0.750\n0.691\n\n\nL-Acc\n0.795\n0.960\n0.987\n0.890\n0.978\n0.885\n0.987\n0.954\n0.930\n\n\nS-Acc\n0.889\n0.963\n0.987\n0.888\n0.980\n0.863\n0.987\n0.960\n0.940\n\n\n+mark\nBLEU\n45.8\n44.5\n43.3\n41.8\n28.4\n27.1\n46.4\n45.3\n40.3\n\n\nCOMET\n0.726\n0.745\n0.610\n0.594\n0.677\n0.659\n0.751\n0.745\n0.688\n\n\nL-Acc\n0.930\n0.987\n0.996\n0.958\n0.995\n0.936\n0.989\n0.972\n0.970\n\n\nS-Acc\n0.942\n0.985\n0.992\n0.957\n0.992\n0.925\n0.990\n0.977\n0.970\n\n\nRamp\nBLEU\n46.4\n46.2\n43.9\n42.9\n30.8\n29.2\n48.8\n47.4\n41.9\n\n\nCOMET\n0.718\n0.759\n0.611\n0.610\n0.721\n0.713\n0.782\n0.771\n0.711\n\n\nL-Acc\n0.956\n0.984\n0.998\n0.952\n0.991\n0.947\n0.993\n0.962\n0.973\n\n\nS-Acc\n0.957\n0.982\n0.995\n0.945\n0.993\n0.935\n0.990\n0.967\n0.970\n\n\nAdapted MT\nBLEU\n44.4\n43.7\n43.4\n37.8\n19.1\n17.0\n53.0\n49.9\n38.5\n\n\nCOMET\n0.712\n0.724\n0.559\n0.547\n-0.191\n-0.263\n0.783\n0.764\n0.454\n\n\nL-Acc\n0.697\n0.598\n0.822\n0.377\n0.869\n0.449\n0.972\n0.744\n0.691\n\n\nS-Acc\n0.700\n0.600\n0.810\n0.400\n0.680\n0.600\n0.950\n0.800\n0.693\n\n\n\n\n\nTable B.2: Full results of same-language prompting on CoCoA-MT.\n\n\n\n\n\n\n\n\n\n\n\n\nAR\nES\nFR\nHI\nPT\nAVG\n\n\nF\nM\nF\nM\nF\nM\nF\nM\nF\nM\n\n\n\n\nXGLM 7.5B\nbase\nBLEU\n7.6\n7.5\n35.5\n38.2\n27.1\n28.6\n13.8\n16.4\n29.2\n33.1\n23.7\n\n\nCOMET\n-.040\n-.012\n.694\n.738\n.509\n.555\n.304\n.332\n.661\n.713\n.445\n\n\nL-Acc\n.848\n.947\n.688\n.808\n.715\n.880\n.585\n.956\n.621\n.855\n.790\n\n\nS-Acc\n.617\n.866\n.651\n.938\n.581\n.920\n.303\n.962\n.494\n.934\n.727\n\n\n+mark\nBLEU\n7.7\n7.8\n35.4\n38.2\n27.5\n28.7\n14.0\n16.7\n29.1\n32.4\n23.7\n\n\nCOMET\n-.038\n-.020\n.704\n.735\n.508\n.556\n.300\n.317\n.663\n.714\n.444\n\n\nL-Acc\n.868\n.939\n.665\n.811\n.701\n.881\n.581\n.955\n.626\n.860\n.789\n\n\nS-Acc\n.664\n.856\n.612\n.937\n.562\n.919\n.355\n.966\n.519\n.927\n.732\n\n\nRamp\nBLEU\n9.2\n8.8\n37.5\n39.4\n27.5\n29.2\n14.8\n16.6\n31.4\n33.3\n24.8\n\n\nCOMET\n.037\n.043\n.723\n.759\n.528\n.571\n.325\n.337\n.681\n.723\n.473\n\n\nL-Acc\n.939\n.961\n.750\n.806\n.781\n.885\n.667\n.956\n.759\n.854\n.836\n\n\nS-Acc\n.836\n.901\n.722\n.936\n.716\n.937\n.509\n.974\n.729\n.940\n.820\n\n\nBLOOM 175B\nbase\nBLEU\n14.8\n16.9\n45.6\n50.3\n38.1\n41.7\n20.8\n24.6\n37.6\n42.2\n33.3\n\n\nCOMET\n.282\n.395\n.837\n.892\n.719\n.770\n.599\n.629\n.807\n.861\n.679\n\n\nL-Acc\n.665\n.966\n.578\n.814\n.660\n.902\n.480\n.951\n.594\n.872\n.748\n\n\nS-Acc\n.411\n.934\n.515\n.965\n.581\n.961\n.212\n.973\n.525\n.960\n.704\n\n\n+mark\nBLEU\n15.2\n17.1\n45.8\n50.0\n37.9\n41.3\n20.3\n23.8\n37.6\n42.2\n33.1\n\n\nCOMET\n.294\n.387\n.843\n.887\n.712\n.767\n.576\n.606\n.807\n.861\n.674\n\n\nL-Acc\n.707\n.969\n.610\n.818\n.663\n.902\n.493\n.958\n.594\n.872\n.759\n\n\nS-Acc\n.482\n.936\n.568\n.973\n.588\n.962\n.284\n.974\n.525\n.960\n.725\n\n\nRamp\nBLEU\n16.7\n17.6\n47.9\n50.2\n39.5\n41.8\n22.2\n25.0\n39.3\n42.7\n34.3\n\n\nCOMET\n.358\n.407\n.860\n.895\n.734\n.787\n.632\n.646\n.810\n.858\n.699\n\n\nL-Acc\n.841\n.972\n.709\n.809\n.765\n.906\n.633\n.953\n.701\n.886\n.817\n\n\nS-Acc\n.721\n.940\n.707\n.964\n.732\n.971\n.518\n.973\n.683\n.972\n.818\n\n\nAdapted MT\nBLEU\n23.3\n24.4\n53.2\n54.2\n44.2\n46.4\n29.3\n32.3\n43.4\n45.7\n35.9\n\n\nCOMET\n.496\n.522\n.876\n.902\n.759\n.797\n.722\n.743\n.825\n.857\n.528\n\n\nL-Acc\n.910\n.981\n.932\n.921\n.919\n.956\n.762\n.837\n.922\n.961\n.853\n\n\nS-Acc\n.940\n.970\n.910\n.960\n.950\n.960\n.280\n.750\n.930\n.990\n.863\n\n\n\n\n\nTable B.3: Full results of same-language prompting on MT-GenEval.\n\n\n\n\n\n\n\n\n\n\n\n\nES\nFR\nHI\nPT\nAVG\n\n\nF\nI\nF\nI\nF\nI\nF\nI\n\n\n\n\nBLOOM 175B\nbase\nBLEU\n40.9\n46.3\n33.7\n32.0\n21.8\n18.9\n33.9\n29.0\n32.1\n\n\nCOMET\n0.785\n0.823\n0.611\n0.615\n0.409\n0.436\n0.772\n0.705\n0.644\n\n\nL-Acc\n0.211\n0.990\n0.899\n0.656\n0.944\n0.123\n0.704\n0.010\n0.567\n\n\nS-Acc\n0.200\n0.930\n0.880\n0.715\n0.940\n0.100\n0.975\n0.025\n0.596\n\n\nRamp\nBLEU\n39.4\n44.6\n35.3\n34.7\n22.4\n18.4\n32.2\n27.5\n31.8\n\n\nCOMET\n0.749\n0.788\n0.575\n0.614\n0.488\n0.480\n0.770\n0.702\n0.646\n\n\nL-Acc\n0.169\n0.978\n0.949\n0.770\n0.973\n0.143\n1.000\n0.015\n0.625\n\n\nS-Acc\n0.175\n0.950\n0.930\n0.790\n0.975\n0.140\n0.975\n0.040\n0.622\n\n\n\n\n\nTable B.4: Full results of cross-lingual prompting on CoCoA-MT.\n\n\n\n\n\n\n\n\n\n\n\n\nAR\nES\nFR\nHI\nPT\nAVG\n\n\nF\nM\nF\nM\nF\nM\nF\nM\nF\nM\n\n\n\n\nBLOOM 175B\nbase\nBLEU\n10.6\n11.6\n43.3\n47.4\n34.2\n38.2\n11.4\n15.0\n34.4\n38.6\n28.5\n\n\nCOMET\n.071\n.138\n.805\n.857\n.648\n.719\n-.135\n-.003\n.766\n.822\n.469\n\n\nL-Acc\n.843\n.956\n.627\n.810\n.561\n.899\n.653\n.962\n.588\n.874\n.777\n\n\nS-Acc\n.541\n.785\n.529\n.936\n.389\n.944\n.051\n.745\n.475\n.939\n.633\n\n\nRamp\nBLEU\n10.0\n10.5\n44.6\n47.8\n35.7\n39.1\n13.9\n16.6\n36.0\n39.4\n29.4\n\n\nCOMET\n-.044\n.020\n.818\n.860\n.686\n.739\n.139\n.212\n.779\n.816\n.502\n\n\nL-Acc\n.845\n.956\n.660\n.815\n.608\n.900\n.574\n.961\n.680\n.882\n.788\n\n\nS-Acc\n.479\n.703\n.605\n.953\n.497\n.956\n.105\n.870\n.613\n.951\n.673\n\n\n\n\n\nTable B.5: Full results of cross-lingual prompting on MT-GenEval.\n\n\n\n\n\nB.1.3 Error Analysis of Cross-Lingual Prompting\nTable B.6 shows two examples where Ramp performs significantly worse than the base model in terms of comet. In the first example, having multiple in-context examples containing “million” led the model to mis-translate “billion” to “million”. In the second example, we observe that the color related in-context examples led the model to produce hallucinated output about clothing colors.\nRepeated misleading in-context examples are less observed on MT-GenEval and in the same-language setting because (1) CoCoA-MT translates the same set of English sentences to different languages while MT-GenEval collects English sentences independently; (2) There are no duplicated source (English) sentences for each language. (Therefore, if Ramp retrieves duplicated English sentences as in Table B.6, their reference translations are guaranteed to be in different languages.)\n\n\n\n\n\n\nIn-context examples (EN)\nMaybe he should. What did you think about that guy findin 3 million dollars worth of old baseball cards in his grandpas attic.\n\n\nYeah that makes sense, did you heard about the $10 million bunker he has?\n\n\nI have. I heard that he started a library in 1895 with 32,000 books in it. All from his personal collection. Can you imagine?\n\n\nYeah that makes sense, did you heard about the $10 million bunker he has?\n\n\nYeah that makes sense, did you heard about the $10 million bunker he has?\n\n\nMaybe he should. What did you think about that guy findin 3 million dollars worth of old baseball cards in his grandpas attic.\n\n\nThat is really expensive I agree, did you watch the Lego Batman movie?\n\n\nYeah that makes sense, did you heard about the $10 million bunker he has?\n\n\nThat is crazy. Do you like Tom Hanks, he’s grossed over 8.5 billion at the box office\n\n\nThat is really expensive I agree, did you watch the Lego Batman movie?\n\n\nThat is crazy. Do you like Tom Hanks, he’s grossed over 8.5 billion at the box office\n\n\nThat is crazy. Do you like Tom Hanks, he’s grossed over 8.5 billion at the box office\n\n\nHe doesnt look like he has 56 years! I heard he made 75000000 from Mission Impossible 3\n\n\nReally? I guess he made a valuable contribution to science and also to medicine, did you hear of that species of flying snakes\n\n\nInput (EN)\nIf you got it why not? He is worth over 20 billion dollars after all.\n\n\nReference (ES)\nSi lo tiene, ¿por qué no? Al fin y al cabo, vale más de 20 000 millones de dólares.\n\n\nRamp (ES)\nSi lo tienes, ¿por qué no? Es millonario después de todo.\n\n\nbase (ES)\nSi lo tienes, ¿por qué no? Él vale más de 20 mil millones de dólares después de todo.\n\n\nIn-context examples (EN)\nthats such a fun thing to have in your code of conduct. are you more of a dog person than cat person?\n\n\nFor sure lol, it was so nice talking with you, say hi to your cats for me!\n\n\nthats such a fun thing to have in your code of conduct. are you more of a dog person than cat person?\n\n\nWhat can’t dogs do! I know they aren’t color blind like we were taught when young. It was so nice chatting with you, tell yuki hi!\n\n\nFor sure lol, it was so nice talking with you, say hi to your cats for me!\n\n\nOh yeah. That sucker was mean let me tell you. I think I would have a rabid dog chase me, which by the way dogs are not color blind!\n\n\nThanks you too! And watch out for dogs, they’re not actually colorblind but they just don’t see as many colors as a normal human can\n\n\nLOL That’s funny! Google prefers dogs over cats! The wrote in their code of conduct that they are a dog company. It’s been fun chatting with you!\n\n\nSuch gorgeous dogs! Do you & your dogs live in a northern climate?\n\n\nLOL That’s funny! Google prefers dogs over cats! The wrote in their code of conduct that they are a dog company. It’s been fun chatting with you!\n\n\nthats such a fun thing to have in your code of conduct. are you more of a dog person than cat person?\n\n\nSuch gorgeous dogs! Do you & your dogs live in a northern climate?\n\n\nhaha me too! I heard that they wore clothes that were full of color. Surprised me because I always thought of them as wearing black and white.\n\n\nFor sure lol, it was so nice talking with you, say hi to your cats for me!\n\n\nInput (EN)\nlol I had an African grey that could say everything I said around the house. Like clean your room! as well any loud sounds.\n\n\nReference (PT)\n“lol Tinha um papagaio-cinzento que conseguia dizer tudo o que ouvia em casa. Tipo”Limpe o seu quarto!” e também quaisquer sons altos”\n\n\nRamp (PT)\nhaha eu também! Eu ouvi dizer que eles usam roupas de todas as cores. Surpreendeu-me porque eu sempre pensei neles usando preto e branco.\n\n\nbase (PT)\nhahaha eu tinha um papagaio cinza africano que dizia tudo o que eu dizia em casa. Como limpar o quarto! Bem como qualquer som alto.\n\n\n\n\n\nTable B.6: Examples of CoCoA-MT (formal) where Ramp performs worse than the base model in cross-lingual zero-shot setting. Potentially problematic in-context examples leading to mistranslations or hallucinations are highlighted.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Conditioning Generation for Personalized Machine Translation</span>"
    ]
  },
  {
    "objectID": "chapters/appendix-b.html#sec-sae-litmt-appendix",
    "href": "chapters/appendix-b.html#sec-sae-litmt-appendix",
    "title": "Appendix B — Conditioning Generation for Personalized Machine Translation",
    "section": "B.2 Steering Language Models for Machine Translation Personalization",
    "text": "B.2 Steering Language Models for Machine Translation Personalization\n\nB.2.1 Experiments Reproducibility\nIn this section, we provide every parameter we use for the reproducibility of our experiments setups.\n\nB.2.1.1 Base Prompt\nWe use the same prompt template across all methods: ZS (which corresponds to the original model translation), ZS-Exp.\\(_\\text{HT}\\), ZS-Exp.\\(_\\text{PT}\\) (detailed in Section B.2.1.4), MS, ActAdd, ReFT, and SAE-based contrastive setups. This prompt, shown in Listing B.3, instructs the model to translate the source sentence while explicitly preventing it from adding any explanations about the translation process. Since all test models are Instruction Tuned, we utilize their native chat templates to preprocess the input accordingly. For multi-shot examples, the user and assistant turns are repeated for each example, always using the same prompt structure.\n\n\nB.2.1.2 ReFT Training\nReFT training was conducted using the PyReFT toolkit from the original authors2. We applied the intervention at the same hook point used by other steering methods - specifically, the layer output corresponding to the residual stream at the selected layer. The training configuration includes a low_rank_dimension of 4, lora_alpha set to 32, and a lora_dropout of 0.05. ReFT was trained on the same 20 prompts used in the MS setup, for a total of 100 epochs.\n\n\nB.2.1.3 SAE Cont.\\(_\\text{HT}\\) and SAE Cont.\\(_\\text{PT}\\)\nWe use the NNsight library (Fiotto-Kaufman et al., 2025) to extract and manipulate model activations for all steering experiments. The source code is publicly available in the repository linked in the main body of this paper. For consistency, we use the same set of contrastive examples employed in the MS approach.\n\n\nB.2.1.4 ZS-Exp.\\(_\\text{HT}\\) and ZS-Exp.\\(_\\text{PT}\\)\nFor both the ZS-Exp.\\(_\\text{HT}\\) and ZS-Exp.\\(_\\text{PT}\\) setups, we used GPT-4o (June 2025) to generate explanations detailing the stylistic differences between a base translation and a target human translation. The prompt template used for this task is shown in Listing B.1, using the same 20 examples as in the MS, SAE Cont.\\(_\\text{HT}\\), and SAE Cont.\\(_\\text{PT}\\) setups.\nAll outputs were manually inspected to ensure no verbatim excerpts from the provided examples were present, avoiding any risk of data leakage. Example outputs for different novels are shown in Listing B.2.\nFinally, these generated guidelines are used to prompt the evaluated models, following the template shown in Listing B.3.\n\n\n\nListing B.1: Prompt template used to get GPT 4o explanation using translation examples.\n\n\nObjective - Identify stylistic choices in translations for personalization purposes.\n\nYou will be provided with a source text, a standard translation, and a target translation by a specific translator whose style we want to emulate.\nYour task is to analyze the 'Target translation' by comparing it to the 'Base translation' and the 'Source text'.\nIdentify and list the distinctive stylistic patterns, choices, and preferences exhibited in the Target translation.\n\nThese stylistic cues should help another translator (or an AI) to adapt their translations to match the style of the target translator.\n\nSource text: ____\nBase translation: ____\nTarget translation: ____\n\n[... Repeat Source, Base and Target]\n\nPlease extract a concise list of key stylistic cues. Focus on aspects such as vocabulary choices, sentence structure, tone and register, handling of cultural nuances, punctuation/formatting preferences and overall creativity.\n\nOutput a short list of stylistic cues as bullet points. Write the list as if you were directly giving the guidelines to the translator and avoid using specific examples.\n\n\n\n\n\n\nListing B.2: Examples of explanation obtained from GPT 4o when comparing different translations from different novels.\n\n\n[When comparing H1 and MT for Beware of Pity (German)]\n- Maintain a tone that is professional, thoughtful, and subtly persuasive.\n- Avoid overly technical jargon unless necessary; explain specialized terms briefly if used.\n- Preserve the author's voice, keeping a balance between academic rigor and narrative engagement.\n- Ensure smooth transitions between sentences and paragraphs to support coherent argumentation.\n- Translate idiomatic expressions in a way that retains their intended effect, even if the wording differs.\n\n[When comparing H1 and H2 for Beware of Pity (German)]\n- Keep the tone warm, welcoming, and direct - avoid overly formal or distant language.\n- Use active voice wherever possible to maintain energy and engagement.\n- Avoid idioms or expressions that may not translate culturally; aim for universal accessibility.\n- Maintain consistent tone and register throughout, adapting to the intended audience's familiarity with the subject.\n- Respect the rhythm and structure of the original, but feel free to adjust sentence length for readability.\n\n[When comparing H1 and H2 for Crime and Punishment (Russian)]\n- Prefer dynamic over formal vocabulary: Opt for vivid or emotionally charged words when available.\n- Add tonal nuance and emotional shading: Enrich dialogues and narration with subtle shifts in tone, especially sarcasm, understatement, or irony, to match character voice or mood.\n- Use contractions and familiar phrasing: Employ contractions and relaxed expressions to preserve spoken character.\n- Expand or rephrase for clarity and voice: Don't hesitate to slightly reword or elaborate if it strengthens tone, clarifies intent, or enhances character differentiation.\n- Favor rhythmic, flowing sentence structure: Break long, formal sentences into multiple shorter clauses or use punctuation (dashes, ellipses) for dramatic or emotional effect.\n[...]\n\n\n\n\n\n\nListing B.3: Zero shot template template when prompting language models with different setups\n\n\nTranslate the following sentence between the angular parentheses into English.\n\n[if setup == ZS-Exp]\n[Follow the following guidelines when translating: &lt;explanations here&gt;]\n\nThe original sentence is: ____.\n\nRemember to write only the translation, without any additional text or explanation.\n\n\n\n\n\n\nB.2.2 All Models Results\n\nB.2.2.1 Full Prompting and Steering Results\nWe present detailed plots of the results for each novel across the three evaluated models in Figure B.1 (Gemma 2 2B), Figure B.2 (Gemma 2 9B), and Figure B.3 (Llama 3.1 8B). These plots display the performance of all evaluated methods, reporting the three submetrics: H accuracy (general human-likeness), P accuracy (translator-specific accuracy), and P\\(_\\text{flip}\\) (personalized flip accuracy), alongside the corresponding comet scores measuring translation quality.\n\n\n\n\n\n\nFigure B.1: Results for every language on Gemma 2 2B.\n\n\n\n\n\n\n\n\n\nFigure B.2: Results for every language on Gemma 2 9B.\n\n\n\n\n\n\n\n\n\nFigure B.3: Results for every language on Llama 3.1 8B.\n\n\n\n\n\n\nB.2.3 Dataset and Generation Examples\nWe present in Table B.7 and Table B.8 a selection of examples from two different languages, showcasing outputs from each of the tested setups. For each example, we also report the corresponding classification label predicted by the classifier and the associated comet score. Additionally\n\n\n\n\n\n\nSource/Method\nTranslation\n📊 Classifier\n☄️ COMET\n\n\n\n\nSource (ZH)\n遂又往後看時，只見畫著一張弓，弓上掛著香櫞。也有一首歌詞云：「二十年來辨是非，榴花開處照宮闈。三春爭及初春景，虎兕相逢大夢歸。」\n\n\n\n\nH\\(_1\\)\nForthwith, therefore, he prosecuted a further perusal of what came next, when he caught sight of a picture of a bow. On this bow hung a citron. There was also this ode: Full twenty years right and wrong to expound will be thy fate! What place pomegranate blossoms come in bloom will face the Palace Gate! The third portion of spring, of the first spring in beauty short will fall! When tiger meets with hare thou wilt return to sleep perennial.\nH\\(_1\\)\n-\n\n\nH\\(_2\\)\nThe picture that followed was of a bow with a citron hanging from it, followed by what looked like the words of a song: You shall, when twenty years in life's hard school are done, In pomegranate-time to palace halls ascend. Though three springs never could with your first spring compare, When hare meets tiger your great dream shall end.\nH\\(_2\\)\n-\n\n\nGemma 2 2B\nWhen he turned back to look, he saw a bow hanging on it, with a plum blossom. There was also a poem that said: \"For twenty years, I have been trying to distinguish right from wrong, and the plum blossoms bloom in the palace. In the spring, the tigers and the rhinoceroses meet, and they dream of returning home.\"\nMT\n0.49\n\n\nGemma 2 9B\nLooking back, he saw a painting of a bow with a quiver hanging on it. There was also a line of lyrics that said: \"For twenty years, I have distinguished right from wrong, and the blossoming of the pomegranate flowers illuminates the imperial court. Three springs have passed, and now it is the beginning of spring again, like a dream where tigers and rhinoceroses meet.\nMT\n0.56\n\n\nLlama 3.1 8B\nAs I looked back, I saw a painting of a bow with a censer on it. There was also a poem that said: \"Twenty years of distinguishing right from wrong, the lotus flower blooms in the palace. Three springs competing with the scenery of the first spring, the tiger and the rhinoceros meet and return to their dream.\"\nMT\n0.53\n\n\nZS-Exp.\\(_\\text{HT}\\)\nAnd as he turned his gaze back upon the scene once more, there was displayed an archer's bow strung tautly; upon its surface hung a fragrant plum blossom. A poem also recited thus spake: \"For twenty years I have sought truth amidst strife, where blooming durian flowers illuminate the palace walls. In spring's embrace, they vie for beauty, tigers clashing in dreams of return.\"\nMT\n0.60\n\n\nZS-Exp.\\(_\\text{PT}\\)\nAnd as he turned his gaze back upon the scene once more, there was displayed an archer's bow strung tautly; upon its surface hung a fragrant plum blossom. A poem also recited thus: \"For twenty years I have sought truth amidst strife, where blooming durian flowers illuminate the palace walls. In spring's embrace, they vie for beauty, tigers clashing in dreams of return.\"\nMT\n0.61\n\n\nMS\nThen, turning around, he noticed a picture depicting a bow hanging on its string, adorned with incense sticks. A verse read: \"For twenty years I've tried to discern truth from falsehood, and wherever fragrant plum blossoms bloom, the palace gates shine brightly.\"\nH\\(_1\\)\n0.65\n\n\nActAdd\nWhen he turned back to look, he saw a bow hanging on it, with a plum blossom. There was also a poem that said: \"For twenty years, I have been trying to tell right from wrong, and the plum blossoms bloom in the palace. In spring, the tigers and rhinoceroses meet, and they dream of going back home.\"\nMT\n0.53\n\n\nReFT\nWhen he turned back to look, he saw a bow hanging there, adorned with a plum blossom. There was also a poem that said: \"For twenty years, I have tried to distinguish right from wrong, and plum blossoms bloom within the palace. In spring, tiger and rhinoceros meet, dreaming of their return home.\"\nMT\n0.58\n\n\nSAE Cont.\\(_\\text{HT}\\) (\\(\\alpha=1\\))\nWhen he turned back to look, he saw a bow hung upon it, with a plum blossom. There was also a verse which read: \"For twenty years, I have struggled to discern truth from falsehood, and the plums blossomed where the imperial court resided. During the three springs, the tiger and rhinoceros met, dreaming of their return.\"\nMT\n0.62\n\n\nSAE Cont.\\(_\\text{HT}\\) (\\(\\alpha=5\\))\nHe looked back and there hung a bow adorned with a plum blossom. A verse read: \"Twenty years I've struggled for truth, where fragrant plums bloom near the imperial court. Spring arrives, tiger and rhino clash, dreaming of their return.\nMT\n0.62\n\n\nSAE Cont.\\(_\\text{HT}\\) (\\(\\alpha=50\\))\nThe when the veryShellTruth lookWarmTheTheTheTigeratPoint [...]\nMT\n0.11\n\n\nSAE Cont.\\(_\\text{PT}\\) (\\(\\alpha=5\\))\nHe looked back and saw a bow adorned with fragrant citron. A verse declared: \"Twenty years I've weighed right and wrong; where pomegranate blossoms bloom, they light the palace gates. Spring's full bloom cannot match early spring's grace. When tiger meets rhino, one returns to the great dream.\"\nH\\(_1\\)\n0.65\n\n\n\n\n\nTable B.7: Example paragraphs from “Dream of the Red Chamber” (ZH\\(\\to\\)EN) translated with H\\(_1\\) personalization. Setup outputs are from Gemma 2 2B; only MS and SAE Cont.\\(_\\text{PT}\\) successfully flip the classifier prediction.\n\n\n\n\n\n\n\n\n\nSource/Method\nTranslation\n📊 Classifier\n☄️ COMET\n\n\n\n\nSource (IT)\nIntanto la stalla fu chiusa e Pinocchio rimase solo: e perché erano molte ore che non aveva mangiato, cominciò a sbadigliare dal grande appetito. E, sbadigliando, spalancava una bocca che pareva un forno.\n\n\n\n\nH\\(_1\\)\nWhen the stable door was closed, Pinocchio was left alone. And since he hadn't eaten in many hours, he began to yawn from hunger. And when he yawned, his mouth opened as wide as an oven.\nH\\(_1\\)\n-\n\n\nH\\(_2\\)\nThe stable was then shut and Pinocchio was left alone. He had not eaten for many hours, and he began to yawn from hunger. And when he yawned he opened a mouth that seemed as wide as an oven.\nH\\(_2\\)\n-\n\n\nGemma 2 2B\nAt that moment the stable was closed and Pinocchio was left alone, and because it had been many hours since he had eaten, he began to yawn from a great hunger. And, yawning, he opened a mouth that seemed like a furnace.\nMT\n0.79\n\n\nGemma 2 9B\nMeanwhile, the stable was closed and Pinocchio was left alone: and because it had been many hours since he had eaten, he began to yawn from his great hunger. And, yawning, he opened a mouth that looked like an oven.\nMT\n0.80\n\n\nLlama 3.1 8B\nMeanwhile the stable was closed and Pinocchio was left alone: and because it was many hours since he hadn't eaten, he began to yawn from great hunger. And, yawning, he opened a mouth that seemed like an oven.\nMT\n0.80\n\n\nZS-Exp.\\(_\\text{HT}\\)\nMeanwhile, the stable door was shut, leaving Pinocchio alone. His tummy rumbled because it had been hours since his last meal, and he started yawning widely from his hunger. With each yawn, his mouth opened like an oven.\nMT\n0.78\n\n\nZS-Exp.\\(_\\text{PT}\\)\nMeanwhile, the stable door was shut, leaving Pinocchio all alone. His tummy rumbled because it had been hours since his last meal, and he started yawning widely from sheer hunger. With each yawn, his mouth gaped open like an oven.\nH\\(_2\\)\n0.80\n\n\nMS\nMeanwhile, the stable door was shut and Pinocchio was left alone. Since it had been hours since he last ate, he began yawning from hunger. Each yawn opened his mouth wide enough to look like an oven.\nH\\(_2\\)\n0.81\n\n\nActAdd\nMeanwhile, the stable was closed and Pinocchio remained alone. Because many hours passed without eating, he started yawning from very big hunger. And while yawning, he opened his mouth wide like an oven.\nMT\n0.75\n\n\nReFT\nMeanwhile, the stable was closed and Pinocchio stayed alone. Because many hours had passed since he last ate, he began to yawn from big hunger. And, yawning, he opened a mouth that looked like an oven.\nMT\n0.77\n\n\nSAE Cont.\\(_\\text{HT}\\) (\\(\\alpha=1\\))\nMeanwhile, the stable door was shut and Pinocchio was left alone. Because it had been many hours since he had eaten, he started to yawn with intense hunger. And, as he yawned, his mouth gaped open like an oven.\nMT\n0.79\n\n\nSAE Cont.\\(_\\text{HT}\\) (\\(\\alpha=5\\))\nMeanwhile, the stable door shut, leaving Pinocchio all alone. Because it had been many hours since he'd eaten, he started to yawn with intense hunger. As he yawned, his mouth gaped open, looking like an oven.\nH\\(_2\\)\n0.82\n\n\nSAE Cont.\\(_\\text{HT}\\) (\\(\\alpha=50\\))\nHe He \"It The ... You It \"[They They) THE...I I HE IT [There There THEYRE WE \"A A FOR \"[...]\nMT\n0.14\n\n\nSAE Cont.\\(_\\text{PT}\\) (\\(\\alpha=5\\))\nThe stable was then shut, leaving Pinocchio all alone. He had not eaten for many hours and began to yawn from great hunger. As he yawned, he opened his mouth wide, like an oven.\nH\\(_2\\)\n0.84\n\n\n\n\n\nTable B.8: Example paragraphs from “Pinocchio” (IT\\(\\to\\)EN) translated with H\\(_2\\) personalization. Setup outputs are from Gemma 2 9B; ZS-Exp.\\(_\\text{PT}\\), MS, SAE Cont.\\(_\\text{HT}\\) and SAE Cont.\\(_\\text{PT}\\) (both \\(\\alpha\\) = 5) can flip the classifier prediction.\n\n\n\n\n\n\n\n\nJaden Fried Fiotto-Kaufman, Alexander Russell Loftus, Eric Todd, Jannik Brinkmann, Koyena Pal, Dmitrii Troitskii, Michael Ripa, Adam Belfki, Can Rager, Caden Juang, Aaron Mueller, Samuel Marks, Arnab Sen Sharma, Francesca Lucchetti, Nikhil Prakash, Carla E. Brodley, Arjun Guha, Jonathan Bell, Byron C Wallace, et al. 2025. NNsight and NDIF: Democratizing access to open-weight foundation model internals. In The thirteenth international conference on learning representations.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Conditioning Generation for Personalized Machine Translation</span>"
    ]
  },
  {
    "objectID": "chapters/appendix-b.html#footnotes",
    "href": "chapters/appendix-b.html#footnotes",
    "title": "Appendix B — Conditioning Generation for Personalized Machine Translation",
    "section": "",
    "text": "We set maximum output length as 50 tokens in the preliminary evaluation, while we use 100 tokens in the main evaluation. Early truncating leads to slightly lower scores in Table B.1 than in Table 6.4.↩︎\nhttps://github.com/stanfordnlp/pyreft↩︎",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Conditioning Generation for Personalized Machine Translation</span>"
    ]
  },
  {
    "objectID": "chapters/appendix-c.html",
    "href": "chapters/appendix-c.html",
    "title": "Appendix C — Interpretability in Human Translation Workflows",
    "section": "",
    "text": "C.1 Machine Translation Post-editing for Typologically Diverse Languages",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Interpretability in Human Translation Workflows</span>"
    ]
  },
  {
    "objectID": "chapters/appendix-c.html#sec-divemt-appendix",
    "href": "chapters/appendix-c.html#sec-divemt-appendix",
    "title": "Appendix C — Interpretability in Human Translation Workflows",
    "section": "",
    "text": "C.1.1 Subject Information\nDuring the setup of our experiment, one translator refused to carry out the main task after the warmup phase, and another was substituted by our choice. Both translators were working in the English-Italian direction and were found to make heavy usage of copy-pasting during the warmup stage, suggesting an incorrect utilization of the platform in light of our guidelines. Both translators, which we identified as T\\(_2\\) and T\\(_3\\) for Italian, were replaced by T\\(_5\\) and T\\(_4\\) respectively. Table C.1 reflects the final translation selection for all languages, with the information collected by means of the pre-task questionnaire.\n\n\n\n\n\n\n\n\nGender\nAge\nDegree\nPosition\nEn Level\nYoE\nPE YoE\n% PE\n\n\n\n\nArabic\nT\\(_1\\)\nM\n35-44\nBA\nFreelancer\nC2\n&gt; 15\n2-5\n20%-40%\n\n\nT\\(_2\\)\nM\n25-34\nBA\nEmployed\nC2\n5-10\n2-5\n60%-80%\n\n\nT\\(_3\\)\nM\n25-34\nMA\nFreelancer\nC1\n5-10\n&lt; 2\n20%-40%\n\n\nDutch\nT\\(_1\\)\nM\n25-34\nMA\nFreelancer\nC2\n5-10\n5-10\n60%-80%\n\n\nT\\(_2\\)\nF\n35-44\nMA\nFreelancer\nC1\n10-15\n5-10\n40%-60%\n\n\nT\\(_3\\)\nF\n25-34\nMA\nFreelancer\nC2\n2-5\n2-5\n20%-40%\n\n\nItalian\nT\\(_1\\)\nF\n25-34\nMA\nEmployed\nC1\n5-10\n5-10\n20%-40%\n\n\nT\\(_5\\)\nF\n25-34\nMA\nFreelancer\nC1\n2-5\n2-5\n40%-60%\n\n\nT\\(_4\\)\nF\n35-44\nBA\nFreelancer\nC2\n10-15\n5-10\n&gt; 80%\n\n\nTurkish\nT\\(_1\\)\nF\n25-34\nBA\nFreelancer\nC2\n5-10\n2-5\n&lt; 20%\n\n\nT\\(_2\\)\nF\n25-34\nBA\nFreelancer\nC1\n5-10\n5-10\n&lt; 20%\n\n\nT\\(_3\\)\nM\n25-34\nHigh school\nFreelancer\nC2\n10-15\n&lt; 2\n&lt; 20%\n\n\nUkrainian\nT\\(_1\\)\nF\n35-44\nMA\nEmployed\nC1\n5-10\n5-10\n20%-40%\n\n\nT\\(_2\\)\nM\n35-44\nMA\nEmployed\nC1\n10-15\n10-15\n20%-40%\n\n\nT\\(_3\\)\nM\n35-44\nHigh school\nEmployed\nB2\n2-5\n2-5\n20%-40%\n\n\nVietnamese\nT\\(_1\\)\nF\n25-34\nMA\nEmployed\nC2\n10-15\n5-10\n40%-60%\n\n\nT\\(_2\\)\nF\n25-34\nBA\nFreelancer\nC1\n5-10\n&lt; 2\n20%-40%\n\n\nT\\(_3\\)\nF\n25-34\nMA\nEmployed\nC1\n2-5\n&lt; 2\n&lt; 20%\n\n\n\n\n\nTable C.1: Subjects information for DivEMT. The last three columns represent respectively the number of years of professional experience as a translator (YoE), the number of years of experience with MT post-editing (PE YoE) and the % of work assignments requiring post-editing in the last 12 months (% PE) for each subject.\n\n\n\n\n\nC.1.2 Translation Guidelines\nAn extract of the translation guidelines provided to the translators follows. The full guidelines are provided in the additional materials.\n\nFill in the pre-task questionnaire before starting the project. In this experiment, your goal is to complete the translation of multiple files in one of two possible translation settings. Please, complete the tasks on your own, even if you know another translator that might be working on this project. The translation setting alternates between texts, with each text requiring a single translation in the assigned setting. The two translation settings are:\n\nTranslation from scratch. Only the source sentence is provided, you are to write the translation from scratch.\nPost-editing. The source sentence is provided alongside a translation produced by an MT system. You are to post-edit this MT output. Post-edit the text so you are satisfied with the final translation (the required quality is publishable quality). If the MT output is too time-consuming to fix, you can delete it and start from scratch. However, please do not systematically delete the provided MT output to give your own translation.\n\nImportant: All editing MUST happen in the provided PET interface: that is, working in other editors and copy-pasting the text back to PET is NOT ALLOWED, because it invalidates the experiment. This is easy to spot in the log data, so please avoid doing this. Complete the translation of all files sequentially, i.e. in the order presented in the tool. DO NOT SKIP files at your own convenience. Make sure that ALL files are translated when you deliver the tasks.\nThe aim is to produce publishable professional quality translations for both translation settings. Thus, please translate to your best abilities. You can return to the files and self-review as many times as you think it is necessary. Important: The time invested to translate is recorded while the active unit (sentence) is in editing mode (yellow background). Therefore:\n\nOnly start to translate when you are in editing mode (yellow background). In other words, do not start thinking how you will translate a sentence when the active unit is not yet in editing mode (green or red background).\nDo not leave a unit in editing mode (yellow background) while you do something else. If you need to do something unrelated in the middle of a translation then go out of editing mode and come back to editing mode when you are ready to resume translating.\nFirst you will be translating a warmup task, and then the main task. When you are translating each file, you can consult the source text by looking up the url in the Excel files that we have sent for reference.\n\nIn order to find the correct terminology for the translation you can consult any source in the Internet. Important: However, it is NOT ALLOWED to use any MT engine to find terms or alternatives to translations (such as Google Translate, DeepL, MS Translator or any MT engine available in your language). Using MT engines invalidates the experiment, and will be detected in the log data. Please fill-in the post-task questionnaire ONLY ONCE after completing all the translation tasks (both warmup and main tasks).\n\n\n\nC.1.3 Details on Document Selection and Preprocessing\nDocument selection Table C.2 present the distribution of selected documents from the Flores-101 devtest split based on their domain and the number of sentences that compose them. The first goal in the selection process was to preserve a rough balance between the three categories while including mostly 4 and 5-sentence docs which are faster to edit in PET (no need to frequently close and reopen an editing window). Another objective of the selection was to minimize the chance of translators finding the translated version of the Wikipedia article from which documents were taken and copied from there, despite our guidelines. We thus scrape the articles from Wikipedia and assess the number of available translations. Among the selected documents, only a small subset has translations in other languages (see Figure C.1 top, an article can have multiple languages), mainly in Hebrew (14), Chinese (10), Spanish (7) and German (5) respectively. Considering the total number of translations for every article (Figure C.1 bottom), we see that roughly 75% of them (79 docs) have no translations. We consider this satisfactory as proof there should not be a large amount of possible copying involved, and we follow up on this evaluation by also ensuring that no repeated copy-paste patterns are present in keylogs after the warmup stage.\n\n\n\n\n\n\nType\nWN\nWV\nWB\n# Sent.\n# Words\n\n\n\n\n3S\n11\n13\n11\n105\n2168\n\n\n4S\n14\n8\n13\n140\n3214\n\n\n5S\n12\n13\n12\n185\n3826\n\n\nTot.\n37\n34\n36\n450\n9626\n\n\n\n\n\nTable C.2: Distribution of the selected DivEMT documents across sizes and Wikipedia categories. A Type value of NS stands for documents composed by N contiguous sentences, WN, WV and WB stand respectively for WikiNews, WikiVoyage and Wikibooks\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\nFigure C.1: Left: Distribution for the availability of documents selected for DivEMT in languages other than English. Right: Quantity of selected documents per number of available translations of Wikipedia.\n\n\n\nFiltering of Outliers For our analysis of Section 8.4, we only use sentences with an editing time lower than 45 minutes, which was selected heuristically as a reasonably high threshold to allow for extensive searching and thinking. In the following, we present the identifiers of the sentences that were filtered out during this process. E.g. 54.1 means the first sentence of document 54, having item_id equal to flores101-main-541 in the dataset. Note that the sentences were outliers only for 2/6 languages and were all different, indicating no systematic issues in the sample: ARA: 54.1, 100.3, VIE: 3.1, 3.2, 24.3, 28.4, 33.1, 33.2, 40.3, 41.2, 50.3, 100.1, 102.1, 106.1, 107.2, 107.4. The 17 sentences were removed for all modalities and languages in the analysis of Section 8.4 to preserve the validity of our comparison, representing a loss of roughly 4% of the total available data, a tolerable amount for our analysis.\nFields Description Table C.3 presents the set of fields that were collected for every entry of the DivEMT dataset. The fields related to keystrokes, times, pauses, annotations and visit order were extracted from the event log of PET .per files, while edits information and other MT quality metrics were computed in a second moment with the help of widely-used libraries.\n\n\n\n\n\n\nField name\nDescription\n\n\n\n\nunit_id, flores_id, subject_id, task_type\nIdentifiers for the item, respective FLORES-101 sentence, translator and translation mode.\n\n\nsrc_text\nThe original source sentence extracted from Wikinews, wikibooks or wikivoyage.\n\n\nmt_text\nMT output sentence before post-editing, present only if task_type is 'pe'.\n\n\ntgt_text\nFinal sentence produced by the translator (either from scratch or post-editing mt_text)\n\n\naligned_edit\nAligned visual representation of the machine translation and its post-edit with edit operations\n\n\nedit_time\nTotal editing time for the translation in seconds.\n\n\nk_letter, k_digit, k_white, k_symbol, k_nav\nNumber of keystrokes for various key types (letters, digits, keystrokes, whitespaces, punctuation, navigation keys) during the translation.\n\n\nk_erease, k_copy, k_paste, k_cut, k_do\nNumber of keystrokes for erease (backspace, cancel), copy, paste, cut and Enter actions during the translation.\n\n\nk_total\nTotal number of all keystroke categories during the translation.\n\n\nn_pause_geq_N, len_pause_geq_N\nNumber and length of pauses longer than 300ms and 1000ms during the translation.\n\n\nnum_annotations\nNumber of times the translator focused the target sentence texbox during the session.\n\n\nn_insert, n_delete, n_substitute, n_shift, tot_shifted_words, tot_edits, hter\nGranular editing metrics and overall HTER computed using the Tercom library.\n\n\ncer\nCharacter-level HTER score computed between the MT and post-edited outputs.\n\n\nbleu, chrf\nSentence-level BLEU and ChrF scores between MT and post-edited fields computed using the SacreBLEU library with default parameters.\n\n\ntime_per_char, key_per_char, words_per_hour, words_per_minute\nEdit time per source character, expressed in seconds. Proportion of keys per character needed to perform the translation. Amount of source words translated or post-edited per hour/minute\n\n\nsubject_visit_order\nId denoting the order in which the translator accessed documents in the interface.\n\n\n\n\n\nTable C.3: Description of the main fields associated to every DivEMT data entry. An entry correspond to a translation in a specific modality (HT, PE\\(_1\\) or PE\\(_2\\)) for one of the six target languages\n\n\n\n\n\nC.1.4 Other Measurements\nAutomatic Evaluation of NMT Systems The selection of systems used in this study was driven by a broader evaluation procedure covering more models, metrics and target languages. Table C.4 presents the overall results of our evaluation. We use HuggingFace’s transformers library (Wolf et al., 2020) for all neural models, using the default decoding settings without further fine-tuning. All metrics were computed using the default settings of SacreBLEU (Post, 2018) and comet (Rei et al., 2020).\n\n\n\n\n\n\n\nSystem\nBLEU\nchrF2\nTER\nchrF2++\nCOMET\n\n\n\n\nArabic\nM2M100\n19.2\n50.9\n69.2\n47\n0.417\n\n\nMarianNMT\n22.7\n54.2\n64.7\n50.4\n0.483\n\n\nmBART-50\n17\n48.5\n69.1\n44.8\n0.452\n\n\nGTrans\n34.1\n65.6\n52.8\n61.9\n0.737\n\n\nDutch\nM2M100\n21.3\n52.9\n66.1\n49.8\n0.405\n\n\nMarianNMT\n25\n56.9\n62.5\n53.8\n0.543\n\n\nmBART-50\n22.6\n53.9\n63.7\n50.9\n0.532\n\n\nDeepL\n28.7\n59.5\n59.5\n56.6\n0.67\n\n\nGTrans\n29.1\n60\n58.5\n57.1\n0.667\n\n\nIndonesian\nM2M100\n35.9\n63.1\n47.3\n60.8\n0.614\n\n\nMarianNMT\n38.5\n65.6\n46.5\n63.3\n0.671\n\n\nmBART-50\n35.9\n63.3\n47.7\n61.1\n0.706\n\n\nGTrans\n51.5\n73.6\n34.5\n71.9\n0.894\n\n\nItalian\nM2M100\n23.6\n53.9\n63.2\n51\n0.51\n\n\nMarianNMT\n27.5\n57.6\n58.9\n54.8\n0.642\n\n\nmBART-50\n24.4\n54.7\n61.2\n51.8\n0.648\n\n\nDeepL\n33\n61\n54\n58.5\n0.795\n\n\nGTrans\n32.8\n61.4\n53.6\n58.8\n0.781\n\n\nJapanese\nM2M100\n24.5\n32.2\n123.3\n26\n0.389\n\n\nmBART\n27.1\n35.4\n123\n28.3\n0.538\n\n\nDeepL\n41.3\n46.8\n108\n37\n0.75\n\n\nGTrans\n38.4\n44.7\n101.5\n33.9\n0.683\n\n\nPolish\nM2M100\n16.1\n46.5\n74.2\n43.1\n0.486\n\n\nMarianNMT\n19.3\n49.9\n70.5\n46.6\n0.648\n\n\nmBART-50\n17.4\n48.2\n72.4\n44.9\n0.603\n\n\nDeepL\n24\n54.3\n66.4\n51.1\n0.832\n\n\nGTrans\n24.4\n54.6\n64.6\n51.4\n0.804\n\n\nRussian\nM2M100\n22.5\n51.1\n65.6\n48.1\n0.427\n\n\nMarianNMT\n25.4\n53.5\n64.3\n50.7\n0.537\n\n\nmBART\n24.8\n52.6\n63.7\n49.7\n0.541\n\n\nDeepL\n35.9\n61.8\n53.3\n59.3\n0.79\n\n\nGTrans\n33\n60.5\n55.2\n57.7\n0.731\n\n\nTurkish\nM2M100\n20.3\n53.9\n65.2\n50.1\n0.686\n\n\nMarianNMT\n26.3\n59.8\n58.8\n55.8\n0.881\n\n\nmBART-50\n18.8\n52.7\n67.5\n48.7\n0.755\n\n\nGTrans\n35\n65.5\n50.4\n62.2\n1\n\n\nUkrainian\nM2M100\n21.9\n51.4\n65.8\n48.3\n0.463\n\n\nMarianNMT\n20\n48.8\n69.2\n45.7\n0.427\n\n\nmBART-50\n21.9\n50.7\n67.9\n47.7\n0.587\n\n\nGTrans\n31.1\n59.8\n55.9\n56.8\n0.758\n\n\nVietnamese\nM2M100\n33.3\n52.3\n52.4\n52.1\n0.43\n\n\nMarianNMT\n26.7\n45.7\n60.2\n45.6\n0.117\n\n\nmBART-50\n34.7\n54\n50.7\n53.8\n0.608\n\n\nGTrans\n45.1\n61.9\n41.8\n61.9\n0.724\n\n\n\n\n\nTable C.4: Automatic MT quality of all evaluated NMT systems on all tested languages in the English-to-XX setting, using the FLORES-101 full devtest for evaluation. Besides mBART-50 and Google Translate (GTrans), we also evaluate a set of bilingual Transformer-based NMT models trained with MarianNMT (Tiedemann and Thottingal, 2020), the DeepL industrial MT system and the multilingual M2M-100 418M model (Fan et al., 2021). Best overall and open-source only performances are highlighted.\n\n\n\nInter-subject Variability in Translation Times Although the variability across different subjects working on the same language directions is not the main concern of our investigation, we produce Figure C.2 (an expanded version of Figure 8.2) to visualize the inter-subject variability for translation times. We observe that the variability across different translators is more pronounced when translating from scratch and that the overall trend of speed improvements associated with PE is mostly preserved (with few exceptions related to the PE\\(_2\\) modality).\n\n\n\n\n\n\nFigure C.2: Time per processed source word across languages, subjects and translation modalities, measured in seconds. Each point represents a document containing 3–5 sentences translated by a subject in one of the languages, with higher scores representing slower editing.\n\n\n\n\n\n\n\n\n\nEnglish\n\n\nInland waterways can be a good theme to base a holiday around.\n\n\nArabic\n\n\nHT\nيمكن أن تكون الممرات المائية الداخلية خياراً جيداً لتخطيط عطلة حولها.\n\n\nPE1\nmt: يمكن أن تكون الممرات المائية الداخلية موضوعًا جيدًا لإقامة عطلة حولها\n\n\n\npe: يمكن** أن تكون الممرات المائية الداخلية مظهرًا جيدًا لإقامة عطلة حولها\n\n\nPE2\nmt: يمكن أن تكون السكك الحديدية الداخلية موضوعًا جيدًا لإقامة عطلة حول\n\n\n\npe: قدتكونالممراتالمائيةالداخليةمكانًاجيدًالقضاءعطلةحولها\n\n\nDutch\n\n\nHT\nBinnenlandse waterwegen kunnen een goed thema zijn voor een vakantie.\n\n\nPE1\nMT: De binnenwateren kunnen een goed thema zijn om een vakantie omheen te baseren.\n\n\n\nPE: Binnenwateren kunnen een goede vakantiebestemming zijn.\n\n\nPE2\nMT: Binnenwaterwegen kunnen een goed thema zijn om een vakantie rond te zetten.\n\n\n\nPE: Binnenwaterwegen kunnen een goed thema zijn om een vakantie rond te organiseren.\n\n\nItalian\n\n\nHT\nI corsi d'acqua dell'entroterra possono essere un ottimo punto di partenza da cui organizzare una vacanza.\n\n\nPE1\nMT: Trasporto fluviale può essere un buon tema per basare una vacanza in giro.\n\n\n\nPE: I canali di navigazione interna possono essere un ottimo motivo per cui intraprendere una vacanza.\n\n\nPE2\nMT: I corsi d’acqua interni possono essere un buon tema per fondare una vacanza.\n\n\n\nPE: I corsi d’acqua interni possono essere un buon tema su cui basare una vacanza.\n\n\nTurkish\n\n\nHT\nİç bölgelerdeki su yolları, tatil planı için iyi bir tema olabilir.\n\n\nPE1\nMT: İç su yolları, bir tatili temel almak için iyi bir tema olabilir.\n\n\n\nPE: İç su yolları, bir tatil planı yapmak için iyi bir tema olabilir.\n\n\nPE2\nMT: İç suyolları, tatil için uygun bir tema olabilir.\n\n\n\nPE: İç sular tatil için uygun bir tema olabilir.\n\n\nUkrainian\n\n\nHT\nМожна спланувати вихідні, взявши за основу подорож внутрішніми водними шляхами.\n\n\nPE1\nMT: Внутрішні водні шляхи можуть стати гарною темою для відпочинку навколо.\n\n\n\nPE: Внутрішні водні шляхи можуть стати гарною темою для проведення вихідних.\n\n\nPE2\nMT: Водні шляхи можуть бути хорошим об ’єктом для базування відпочинку навколо.\n\n\n\nPE: Місцевість навколо внутрішніх водних шляхів може бути гарним вибором для організації відпочинку.\n\n\nVietnamese\n\n\nHT\nDu lịch trên sông có thể là một lựa chọn phù hợp cho kỳ nghỉ.\n\n\nPE1\nMT: Đường thủy nội địa có thể là một chủ đề hay để tạo cơ sở cho một kỳ nghỉ xung quanh.\n\n\n\nPE: Đường thủy nội địa có thể là một ý tưởng hay để lập kế hoạch cho kỳ nghỉ.\n\n\nPE2\nMT: Các tuyến nước nội địa có thể là một chủ đề tốt để xây dựng một kì nghỉ.\n\n\n\nPE: Du lịch bằng đường thủy nội địa là một ý tưởng nghỉ dưỡng không tồi.\n\n\n\n\n\nTable C.5: An example sentence (81.1) from the DivEMT corpus, with the English source and all output modalities for all target languages, including intermediate machine translations (MT) and subsequent post-editings (PE). Colors denote insertions, deletions, substitutions and shifts computed with Tercom (Snover et al., 2006).\n\n\n\n\n\n\n\n\n\nEnglish\n\n\nThe Internet combines elements of both mass and interpersonal communication.\n\n\nArabic\n\n\nHT\nيجمع الإنترنت بين عناصر وسائل الاتصال العامة والشخصية على حدٍ سواء\n\n\nPE1\nmt: تجمع الإنترنت بين عناصر الاتصال الجماهيري والشخصي\n\n\n\npe: يجمع الإنترنت بين عناصر الاتصال الجماهيري والشخصي\n\n\nPE2\nmt: إنترنت تجمع عناصر التواصل الجماعي والتواصل الشخصي\n\n\n\npe: تجمع شبكة الإنترنت عناصر التواصل الجماعي والتواصل الشخصي\n\n\nDutch\n\n\nHT\nHet internet combineert elementen van zowel massa- en intermenselijke communicatie.\n\n\nPE1\nMT: Het internet combineert elementen van zowel massa- als interpersoonlijke communicatie.\n\n\n\nPE: Het internet combineert elementen van zowel massa- als interpersoonlijke communicatie.\n\n\nPE2\nMT: Het internet combineert elementen van massa- en interpersoonlijke communicatie.\n\n\n\nPE: Het internet combineert elementen van massa- en interpersoonlijke communicatie.\n\n\nItalian\n\n\nHT\nInternet combina elementi di comunicazione sia di massa sia interpersonale.\n\n\nPE1\nMT: Internet combina elementi di comunicazione di massa e interpersonali.\n\n\n\nPE: Internet combina elementi di comunicazione di massa e interpersonale.\n\n\nPE2\nMT: Internet combina elementi di comunicazione di massa e interpersonale.\n\n\n\nPE: Internet combina elementi di comunicazione di massa e interpersonale.\n\n\nTurkish\n\n\nHT\nİnternet hem kitlesel hem de bireysel iletişim öğelerini birleştiriyor.\n\n\nPE1\nMT: İnternet, hem kitle hem de kişiler arası iletişimin unsurlarını birleştirir.\n\n\n\nPE: İnternet, hem kitleler hem de kişiler arası iletişimin unsurlarını birleştirir.\n\n\nPE2\nMT: İnternet hem kitlesel hem de kişisel iletişim unsurlarını birleştiriyor.\n\n\n\nPE: İnternet hem kitlesel hem de kişisel iletişim unsurlarını birleştiriyor.\n\n\nUkrainian\n\n\nHT\nВ інтернеті поєднуються елементи групового спілкування та особистого спілкування.\n\n\nPE1\nMT: Інтернет поєднує в собі елементи як масового, так і міжособистісного спілкування.\n\n\n\nPE: Інтернет поєднує в собі елементи як масового, так і міжособистісного спілкування.\n\n\nPE2\nMT: Інтернет об ’єднує як масову, так і міжлюдську комунікацію.\n\n\n\nPE: Інтернет поєднує в собі елементи як групової, так і особистої комунікації.\n\n\nVietnamese\n\n\nHT\nInternet là nơi tổng hợp các yếu tố của cả phương tiện truyền thông đại chúng và giao tiếp liên cá nhân.\n\n\nPE1\nMT: Internet kết hợp các yếu tố của cả giao tiếp đại chúng và giao tiếp giữa các cá nhân.\n\n\n\nPE: Internet kết hợp các yếu tố của cả truyền thông đại chúng và giao tiếp giữa các cá nhân.\n\n\nPE2\nMT: Internet kết hợp những yếu tố của sự giao tiếp quần chúng và giao tiếp giữa người với người.\n\n\n\nPE: Internet kết hợp những yếu tố của cả việc giao tiếp đại chúng và giao tiếp cá nhân.\n\n\n\n\n\nTable C.6: An example sentence (29.2) from the DivEMT corpus, with the English source and all output modalities for all target languages, including intermediate machine translations (MT) and subsequent post-editings (PE). Colors denote insertions, deletions, substitutions and shifts computed with Tercom (Snover et al., 2006).\n\n\n\n\n\n\n\n\n\n\n\n\nSubject\nCoefficient\n\n\n\n\nara_t1\n0.281\n\n\nara_t2\n-0.384\n\n\nara_t3\n-0.103\n\n\nnld_t1\n0.001\n\n\nnld_t2\n-0.459\n\n\nnld_t3\n0.458\n\n\nita_t1\n0.086\n\n\nita_t4\n0.350\n\n\nita_t5\n-0.436\n\n\ntur_t1\n-0.381\n\n\ntur_t2\n0.272\n\n\ntur_t3\n0.109\n\n\nukr_t1\n0.077\n\n\nukr_t2\n0.314\n\n\nukr_t3\n-0.391\n\n\nvie_t1\n0.012\n\n\nvie_t2\n0.176\n\n\nvie_t3\n-0.188\n\n\n\n\n\nTable C.7: Coefficients of the random intercept related to the subject_id variable, representing the identity of the translator performing the translation.\n\n\n\n\n\n \n\n\nC.1.5 Data Filtering and Feature Significance\nWe log-transform the dependent variable, edit time in seconds, given its long right tail. The models are built by adding one element at a time, and checking whether such addition leads to a significantly better model with AIC (i.e. if the score gets reduced by at least 2). Our random effects structure includes random intercepts for different segments (nested with documents) and translators, as well as a random slope for modality over individual segments. We start with an initial model that just includes the two random intercepts (by-translator and by-segment) and proceed by (i) finding significance for nested document/segment random effect; (ii) adding fixed predictors one by one; (iii) adding interactions between fixed predictors; and (iv) adding the random slopes.1 From this sequential procedure, we obtain the resulting model. When checking the homoscedasticity and normality of residuals assumptions (Figure C.3 and Figure C.4), we find the latter is not fulfilled. Consequently, we remove data points for which observations deviate by more than 2.5 standard deviations from the predicted value by the model (2.4% of the data) and refit the best model on this subset, in order to find out whether any of the effects were due to these outliers. The resulting trends do not change significantly in this final model, in which residuals are normally distributed. As a final sanity check, in Table C.7 we measure the effect of subject identity on edit times and find no systematic patterns across languages.\n\n\n\n\n\n\n\n\n\nFigure C.3: Residuals of the final LMER model, used to verify the heteroscedasticity assumption.\n\n\n\n\n\n\n\n\n\nFigure C.4: Quantile-quantile plot before and after the removal of outliers when fitting the LMER model, used to verify the normality assumption.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Interpretability in Human Translation Workflows</span>"
    ]
  },
  {
    "objectID": "chapters/appendix-c.html#sec-qe4pe-appendix",
    "href": "chapters/appendix-c.html#sec-qe4pe-appendix",
    "title": "Appendix C — Interpretability in Human Translation Workflows",
    "section": "C.2 Word-level Quality Estimation for Machine Translation Post-editing",
    "text": "C.2 Word-level Quality Estimation for Machine Translation Post-editing\n\nC.2.1 Filtering Details for QE4PE Data\n\nDocuments should contain between 4 and 10 segments, each containing 10-100 words (959 docs). This ensures that all documents are roughly uniform in terms of size and complexity to maintain a steady editing flow Section 9.2.5.\nThe average segment-level QE score predicted by XCOMET-XXL is between 0.3 and 0.95, with no segment below 0.3 (429 docs). This forces segments to have a decent but still imperfect quality, excluding fully wrong translations.\nAt least 3 and at most 20 errors spans per document, with no more than 30% of words in the document being highlighted (351 docs). This avoids overwhelming the editor with excessive highlighting, while still ensuring error presence.\n\nThe same heuristics were applied to both translation directions, selecting only documents matching our criteria in both cases.\n\n\nC.2.2 Additional Details and Statistics\n\n\n\n\n\n\nIdentifier\nJob\nEng. Lvl\nTrans. YoE\nPost-edit YoE\nPost-edit %\nAdv. CAT YoE\nMT good/bad for:\nPost-edit comment\n\n\n\n\nita-nohigh-fast\nFL (FT)\nC1\n2-5\n2-5\n100%\nOften\nG: Productivity, quality, repetitive work.\nPE better than from scratch when consistency is needed.\n\n\nita-nohigh-avg\nFL (PT)\nC1\n&gt;10\n&lt;2\n20%\nOften\nG: Productivity, repetitive work. B: less creative.\nPE produces unnatural sentences.\n\n\nita-nohigh-slow\nFL (PT)\nC2\n&gt;10\n2-5\n40%\nSometimes\nG: creativity.\nGood for time saving.\n\n\nita-oracle-fast\nFL (FT)\nC2\n5-10\n2-5\n60%\nSometimes\nG: Productivity, repetitive work. B: less creative.\nGood for productivity, humans always needed.\n\n\nita-oracle-avg\nFL (FT)\nC2\n5-10\n5-10\n20%\nAlways\nG: productivity, terminology.\nGood for tech docs, not for articulated texts.\n\n\nita-oracle-slow\nFL (FT)\nC2\n2-5\n5-10\n80%\nAlways\nG: Productivity, repetitive work.\nUseful for consistency and productivity, unless creativity is needed.\n\n\nita-unsup-fast\nFL (FT)\nC1\n&lt;2\n&lt;2\n60%\nOften\nG: Productivity, terminology. B: less creative.\nHumans will always be needed in translation.\n\n\nita-unsup-avg\nFL (FT)\nC1\n&gt;10\n2-5\n60%\nOften\nG: Productivity, repetitive work. B: less creative.\nAn opportunity for translators.\n\n\nita-unsup-slow\nFL (FT)\nC1\n5-10\n5-10\n80%\nAlways\nG: Productivity, repetitive work. B: less creative.\nGood for focusing on detailed/cultural/creative aspects of translations.\n\n\nita-sup-fast\nFL (PT)\nC1\n&gt;10\n2-5\n40%\nOften\nG: Productivity, quality, repetitive work, terminology.\nImproves quality and consistency.\n\n\nita-sup-avg\nFL (FT)\nC1\n&gt;10\n5-10\n100%\nAlways\nG: Productivity, repetitive work. B: less creative.\nConsistency improved, but less variance means less creativity.\n\n\nita-sup-slow\nFL (FT)\nC1\n&gt;10\n2-5\n20%\nAlways\nG: Productivity, creativity, quality, repetitive work.\nGood for productivity, but does not work on creative texts.\n\n\nnld-nohigh-fast\nFL (FT)\nC1\n&gt;10\n&gt;10\n40%\nOften\nG: Productivity, terminology. B: creativity.\nWidespread but still too literal\n\n\nnld-nohigh-avg\nFL (FT)\nC2\n&gt;10\n2-5\n40%\nAlways\nG: Repetitive work. B: creativity, often wrong, worse quality.\nIncrease in productivity to save on costs brings down quality.\n\n\nnld-nohigh-slow\nFL (FT)\nC2\n&gt;10\n5-10\n100%\nOften\nG: Creativity, quality, repetitive work, terminology.\nWorking with MT can be creative beyond PE.\n\n\nnld-oracle-fast\nFL (FT)\nC1\n5-10\n5-10\n80%\nAlways\nG: Productivity, quality, repetitive work, terminology.\nGood for tech docs and repetition.\n\n\nnld-oracle-avg\nFL (FT)\nC2\n&gt;10\n2-5\n40%\nAlways\nB: less creative, less productive, often wrong\nBad MT is worse than no MT for specialized domains.\n\n\nnld-oracle-slow\nFL (FT)\nC2\n&gt;10\n2-5\n60%\nOften\nG: Productivity, repetitive work. B: cultural references.\nMore productivity at the cost of idioms and cultural factors.\n\n\nnld-unsup-fast\nFL (FT)\nC2\n5-10\n2-5\n40%\nOften\nG: all. B: often wrong, worse quality.\nPE makes you less in touch with the texts and often poorly paid.\n\n\nnld-unsup-avg\nFL (FT)\nC2\n5-10\n2-5\n60%\nSometimes\nG: Productivity, quality, repetitive work, terminology. B: wrong.\nPractical but less effective for longer passages.\n\n\nnld-unsup-slow\nFL (FT)\nC2\n&gt;10\n2-5\n40%\nAlways\nG: repetitive work, productivity, terminology\nImproves consistency and productivity if applied well.\n\n\nnld-sup-fast\nFL (FT)\nC2\n&gt;10\n5-10\n60%\nOften\nG: repetitive work, creativity, terminology\nUseful, but worries about job loss\n\n\nnld-sup-avg\nFL (FT)\nC2\n&gt;10\n10\n60%\nSometimes\nG: terminology, creativity\nUseful for inspiration on better translations\n\n\nnld-sup-slow\nFL (FT)\nC1\n5-10\n5-10\n80%\nAlways\nG: repetitive work, productivity\nBetter productivity at the cost of creativity.\n\n\n\n\n\nTable C.8: Sample of pre-task questionnaire results. YoE = years of experience. FL = Freelance, PT = Part-time, FT = Full-time. PE = Post-editing. G = Good, B = Bad.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHighlights statements\n\n\nIdentifier\nMT good / fluent / accurate\nHigh. accurate / useful\nInterface clear\nTask difficult\n\\(\\uparrow\\) Speed?\n\\(\\uparrow\\) Quality?\n\\(\\uparrow\\) Effort?\n\\(\\uparrow\\) Influence?\n\\(\\uparrow\\) Spot errors?\n\\(\\uparrow\\) Enjoy?\n\n\n\n\nita-nohigh-fast\n4 / 0.8 / 0.8\n- / -\n5\n1\n-\n-\n-\n-\n-\n-\n\n\nita-nohigh-avg\n3 / 0.6 / 0.4\n- / -\n2\n4\n-\n-\n-\n-\n-\n-\n\n\nita-nohigh-slow\n3 / 0.8 / 0.8\n- / -\n1\n5\n-\n-\n-\n-\n-\n-\n\n\nita-oracle-fast\n5 / 0.4 / 0.8\n4 / 4\n4\n5\n5\n2\n1\n1\n1\n4\n\n\nita-oracle-avg\n3 / 0.4 / 0.6\n2 / 1\n2\n3\n1\n1\n4\n1\n1\n1\n\n\nita-oracle-slow\n3 / 0.6 / 0.6\n2 / 2\n2\n5\n1\n1\n1\n1\n4\n1\n\n\nita-unsup-fast\n3 / 0.8 / 0.6\n3 / 2\n4\n5\n3\n3\n3\n2\n2\n2\n\n\nita-unsup-avg\n3 / 0.6 / 0.6\n3 / 3\n3\n5\n2\n3\n2\n1\n1\n3\n\n\nita-unsup-slow\n3 / 0.4 / 0.6\n2 / 2\n3\n4\n2\n2\n3\n3\n4\n4\n\n\nita-sup-fast\n3 / 0.4 / 0.4\n2 / 1\n2\n2\n1\n1\n3\n1\n2\n2\n\n\nita-sup-avg\n3 / 0.4 / 0.4\n2 / 2\n3\n5\n3\n2\n4\n3\n3\n4\n\n\nita-sup-slow\n3 / 0.6 / 0.6\n2 / 2\n1\n2\n2\n1\n1\n4\n4\n1\n\n\nnld-nohigh-fast\n3 / 0.2 / 0.4\n- / -\n4\n4\n-\n-\n-\n-\n-\n-\n\n\nnld-nohigh-avg\n2 / 0.4 / 0.6\n- / -\n4\n5\n-\n-\n-\n-\n-\n-\n\n\nnld-nohigh-slow\n2 / 0.2 / 0.4\n- / -\n3\n5\n-\n-\n-\n-\n-\n-\n\n\nnld-oracle-fast\n3 / 0.6 / 0.6\n2 / 1\n3\n2\n2\n2\n2\n1\n1\n1\n\n\nnld-oracle-avg\n3 / 0.8 / 0.6\n4 / 3\n3\n4\n3\n3\n3\n3\n2\n3\n\n\nnld-oracle-slow\n3 / 0.6 / 0.4\n3 / 1\n3\n4\n1\n1\n1\n1\n1\n3\n\n\nnld-unsup-fast\n3 / 0.6 / 0.8\n3 / 2\n4\n4\n1\n3\n1\n1\n2\n1\n\n\nnld-unsup-avg\n3 / 0.6 / 0.6\n4 / 3\n2\n4\n3\n3\n4\n3\n2\n3\n\n\nnld-unsup-slow\n1 / 0.4 / 0.4\n2 / 4\n1\n4\n4\n4\n3\n2\n2\n3\n\n\nnld-sup-fast\n3 / 0.6 / 0.4\n2 / 2\n3\n5\n1\n1\n5\n3\n1\n1\n\n\nnld-sup-avg\n3 / 0.4 / 0.6\n2 / 2\n2\n4\n1\n1\n1\n1\n2\n3\n\n\nnld-sup-slow\n5 / 0.8 / 1\n4 / 3\n2\n5\n3\n3\n2\n2\n2\n4\n\n\n\n\n\nTable C.9: Sample of post-task questionnaire results. Statements use a 1–Strongly disagree to 5–Strongly agree scale.\n\n\n\n\n\n\n\n\n\n\n\n\nTarget: Seg. Edit Time, 5s bins from 0 to 600s\n\n\nFeature\nCoeff.\nSig.\n\n\n\n\n(Intercept)\n1.67\n***\n\n\nMT Num. Chars\n2.42\n***\n\n\nHighlight Ratio %\n1.59\n***\n\n\nTarget Lang.: ITA\n-0.34\n***\n\n\nText Domain: Social\n0.31\n***\n\n\nOracle Highlight\n-0.79\n.\n\n\nSup. Highlight\n0.02\n\n\n\nUnsup. Highlight\n-0.07\n\n\n\nMT XCOMET QE Score\n0.01\n***\n\n\nITA:Oracle\n0.91\n***\n\n\nITA:Sup.\n1.18\n***\n\n\nITA:Unsup.\n0.48\n***\n\n\nSocial:Oracle\n-0.19\n**\n\n\nSocial:Sup.\n-0.34\n***\n\n\nSocial:Unsup.\n-0.22\n***\n\n\nHighlight Ratio:Oracle\n-0.83\n*\n\n\nHighlight Ratio:Sup.\n-1.33\n***\n\n\nEdit Order\nRandom Factors\n\n\nTranslator ID\n\n\nSegment ID\n\n\n\n\n\nTable C.10: Details for the negative binomial mixed-effect model used for the productivity analysis of Section 9.3.1.\n\n\n\n\n\n \n\n\n\n\n\n\n\n\nTarget: % of edited characters in a segment (0-100).\n\n\nFeature\nCoeff.\nSig.\n\n\n\n\n(Intercept)\n21.0\n***\n\n\nMT Num. Chars\n10.3\n***\n\n\nHighlight Ratio %\n7.1\n***\n\n\nTarget Lang.: ITA\n-9.9\n***\n\n\nText Domain: Social\n10.9\n***\n\n\nOracle Highlight\n-5.2\n\n\n\nSup. Highlight\n-4.7\n\n\n\nUnsup. Highlight\n-0.9\n\n\n\nITA:Oracle\n12.2\n***\n\n\nITA:Sup.\n15.9\n***\n\n\nITA:Unsup.\n13.4\n***\n\n\nSocial:Oracle\n3.5\n***\n\n\nSocial:Sup.\n-0.4\n\n\n\nSocial:Unsup.\n2.1\n**\n\n\nHighlight Ratio:Oracle\n-0.18\n\n\n\nHighlight Ratio:Sup.\n-1.78\n***\n\n\nEdit Order\nRandom Factors\n\n\nTranslator ID\n\n\nSegment ID\n\n\nMT Num. Chars\nZero-Inflation Factors\n\n\nTarget Lang\n\n\nText Domain\n\n\nTranslator ID\n\n\n\n\n\nTable C.11: Details for the zero-inflated negative binomial mixed-effect model used for the editing analysis of Section 9.3.2. The model achieves an RMSE of 0.11 and an \\(R^2\\) of 0.98.\n\n\n\n\n\n\n\n\n\n\n\n\nModalities\nen→it\nen→nl\nBoth\n\n\n\n\nBio\nSocial\nBoth\nBio\nSocial\nBoth\nBio\nSocial\nBoth\n\n\n\n\nOracle and\nSup.\n0.17\n0.32\n0.25\n0.38\n0.29\n0.34\n0.26\n0.29\n0.29\n\n\nUnsup.\n0.14\n0.30\n0.20\n0.31\n0.27\n0.28\n0.22\n0.29\n0.24\n\n\nSup. and\nOracle\n0.19\n0.31\n0.26\n0.30\n0.26\n0.29\n0.24\n0.29\n0.28\n\n\nUnsup.\n0.19\n0.33\n0.25\n0.28\n0.24\n0.25\n0.24\n0.29\n0.25\n\n\nUnsup. and\nOracle\n0.22\n0.32\n0.27\n0.35\n0.30\n0.33\n0.28\n0.31\n0.30\n\n\nSup.\n0.22\n0.37\n0.30\n0.39\n0.27\n0.33\n0.30\n0.31\n0.32\n\n\n\n\n\nTable C.12: Average highlight agreement proportion between different modalities across language pairs and domains (Section 9.3.2). Scores are normalized to account for the relative frequency of highlight modalities compared to the mean highlight frequency for the current language and domain combination.\n\n\n\n\n\n\n\n\n\nDomain\nSpeed\n\\(P(H)\\)\n\\(P(E)\\)\n\\(P(E|H)\\)\n\\(P(E|\\neg H)\\)\n\\(\\Lambda_H(E)\\)\n\\(P(H|E)\\)\n\\(P(H|\\neg E)\\)\n\\(\\Lambda_E(H)\\)\n\n\n\n\n\nen→it\n\n\nBiomed.\nFast\n.09\n.04 / .01\n.12 / .02\n.03 / .01\n4.0 / 2.0\n.30 / .27\n.08 / .11\n3.7 / 2.4\n\n\n\nAvg.\n.10 / .05\n.27 / .12\n.09 / .04\n3.0 / 3.0\n.22 / .30\n.07 / .11\n3.1 / 2.7\n\n\n\nSlow\n.09 / .02\n.21 / .04\n.08 / .01\n2.6 / 4.0\n.19 / .26\n.07 / .11\n2.7 / 2.3\n\n\n\nSocial\nFast\n.14\n.11 / .07\n.30 / .20\n.07 / .04\n4.2 / 5.0\n.40 / .52\n.11 / .16\n3.6 / 3.2\n\n\n\nAvg.\n.23 / .14\n.48 / .32\n.18 / .10\n2.6 / 3.2\n.30 / .42\n.09 / .15\n3.3 / 2.8\n\n\n\nSlow\n.17 / .05\n.39 / .14\n.14 / .03\n2.7 / 4.6\n.31 / .54\n.11 / .17\n2.8 / 3.1\n\n\n\nen→nl\n\n\nBiomed.\nFast\n.14\n.03 / .02\n.11 / .05\n.02 / .01\n5.5 / 5.0\n.48 / .61\n.13 / .18\n3.6 / 3.3\n\n\n\nAvg.\n.11 / .19\n.20 / .30\n.10 / .17\n2.0 / 1.7\n.25 / .29\n.13 / .16\n1.9 / 1.8\n\n\n\nSlow\n.12 / .10\n.26 / .23\n.10 / .07\n2.6 / 3.2\n.29 / .42\n.12 / .16\n2.4 / 2.6\n\n\n\nSocial\nFast\n.12\n.06 / .07\n.19 / .21\n.04 / .04\n4.7 / 5.2\n.37 / .47\n.10 / .13\n3.7 / 3.6\n\n\n\nAvg.\n.17 / .32\n.32 / .48\n.15 / .29\n2.1 / 1.6\n.22 / .23\n.10 / .12\n2.2 / 1.9\n\n\n\nSlow\n.18 / .18\n.38 / .40\n.15 / .14\n2.5 / 2.8\n.25 / .34\n.09 / .11\n2.7 / 3.0\n\n\n\n\n\n\n\n\n\nTable C.13: Highlighting (\\(H\\)) and editing (\\(E\\)) statistics for each domain, and translation direction across translator speeds (\\(n = 4\\) post-editors per combination, regardless of highlight modality). Values after slashes are adjusted by projecting highlights of the specified modality over edits from No Highlight translators to estimate highlight-induced editing biases (Section 9.3.2).\n\n\n\n\n\n\n\n\n\nDomain\nModality\n\\(P(H)\\)\n\\(P(E)\\)\n\\(P(E|H)\\)\n\\(P(E|\\neg H)\\)\n\\(\\Lambda_H(E)\\)\n\\(P(H|E)\\)\n\\(P(H|\\neg E)\\)\n\\(\\Lambda_E(H)\\)\n\n\n\n\n\nen→it\n\n\nBiomed.\nRandom\n.12\n-\n- / .02\n- / .02\n- / 1.0\n- / .11\n- / .13\n- / 0.8\n\n\n\nNo High.\n-\n.02\n-\n-\n-\n-\n-\n-\n\n\n\nOracle\n.08\n.07\n.26 / .08\n.05 / .02\n5.2 / 4.0\n.30 / .26\n.06 / .08\n5.0 / 3.2\n\n\n\nUnsup.\n.16\n.10\n.18 / .06\n.08 / .02\n2.2 / 3.0\n.29 / .36\n.14 / .15\n2.0 / 2.4\n\n\n\nSup.\n.11\n.12\n.18 / .05\n.11 / .02\n1.6 / 2.5\n.16 / .23\n.10 / .10\n1.6 / 2.3\n\n\n\nSocial\nRandom\n.20\n-\n- / .09\n- / .09\n- / 1.0\n- / .21\n- / .20\n- / 1.0\n\n\n\nNo High.\n-\n.09\n-\n-\n-\n-\n-\n-\n\n\n\nOracle\n.25\n.20\n.42 / .23\n.13 / .04\n3.2 / 5.7\n.52 / .66\n.18 / .21\n2.8 / 3.1\n\n\n\nUnsup.\n.17\n.18\n.35 / .19\n.14 / .07\n2.5 / 2.7\n.33 / .37\n.14 / .15\n2.3 / 2.4\n\n\n\nSup.\n.15\n.21\n.38 / .23\n.18 / .06\n2.1 / 3.8\n.27 / .39\n.11 / .12\n2.4 / 3.2\n\n\n\nen→nl\n\n\nBiomed.\nRandom\n.17\n-\n- / .12\n- / .10\n- / 1.2\n- / .19\n- / .17\n- / 1.1\n\n\n\nNo High.\n-\n.10\n-\n-\n-\n-\n-\n-\n\n\n\nOracle\n.21\n.08\n.21 / .20\n.05 / .08\n4.2 / 2.5\n.52 / .41\n.18 / .18\n2.8 / 2.2\n\n\n\nUnsup.\n.23\n.09\n.17 / .17\n.07 / .08\n2.4 / 2.1\n.43 / .38\n.21 / .21\n2.0 / 1.8\n\n\n\nSup.\n.12\n.08\n.20 / .21\n.06 / .09\n3.3 / 2.3\n.30 / .25\n.11 / .11\n2.7 / 2.2\n\n\n\nSocial\nRandom\n.16\n-\n- / .22\n- / .19\n- / 1.1\n- / .19\n- / .16\n- / 1.1\n\n\n\nNo High.\n-\n.19\n-\n-\n-\n-\n-\n-\n\n\n\nOracle\n.19\n.12\n.33 / .39\n.07 / .15\n4.7 / 2.6\n.54 / .39\n.15 / .15\n3.6 / 2.6\n\n\n\nUnsup.\n.15\n.13\n.25 / .33\n.11 / .17\n2.2 / 1.9\n.30 / .26\n.13 / .12\n2.3 / 2.1\n\n\n\nSup.\n.12\n.10\n.30 / .36\n.08 / .17\n3.7 / 2.1\n.36 / .23\n.10 / .10\n3.6 / 2.3\n\n\n\n\n\n\n\n\n\nTable C.14: Highlighting (\\(H\\)) and editing (\\(E\\)) statistics for each domain, modality and translation direction combination (\\(n = 3\\) post-editors per combination). Values after slashes are adjusted by projecting highlights of the specified modality over edits from No Highlight translators to estimate highlight-induced editing biases (Section 9.3.2). A Random baseline is added by projecting random highlights matching the average frequency over all modalities for specific domain and translation direction settings.\n\n\n\n\n\n\n\n\n\nID\nSource text\nTarget text\nProposed correction\nError Cat.\nSeverity\nScore\n\n\n\n\n\n9-1\nSpecifying peri- and postnatal factors in children born very preterm (VPT) that affect later outcome helps to improve long-term treatment.\nSpecificare i fattori peri- e postnatali nei bambini nati molto pretermine (VPT) che influenzano il risultato successivo aiuta a migliorare il trattamento a lungo termine.\nSpecificare i fattori peri- e postnatali nei bambini nati molto pretermine (VPT, Very Preterm) che influenzano il risultato successivo aiuta a migliorare il trattamento a lungo termine.\nReadability\nMinor\n90\n\n\n\n9-2\nTo enhance the predictability of 5-year cognitive outcome by perinatal, 2-year developmental and socio-economic data.\nMigliorare la prevedibilità del risultato cognitivo a 5 anni mediante dati perinatali, di sviluppo e socioeconomici a 2 anni.\n\n\n\n100\n\n\n\n9-3\n5-year infants born VPT were compared to 34 term controls.\nI neonati di 5 anni nati VPT sono stati confrontati con 34 nati a termine come controllo.\nI neonati di 5 anni nati VPT sono stati confrontati con 34 controlli a termine.\nMistranslation\nMinor\n70\n\n\n\n9-4\nThe IQ of 5-year infants born VPT was 10 points lower than that of term controls and influenced independently by preterm birth and SES.\nIl QI dei bambini di 5 anni nati VPT era di 10 punti inferiore a quello dei nati a termine di controllo, e influenzato indipendentemente dalla nascita pretermine e dai dati SES.\nIl QI dei bambini di 5 anni nati VPT era di 10 punti inferiore a quello dei nati a termine e influenzato indipendentemente dalla nascita pretermine e dallo stato socioeconomico (SES).\nMistranslation\nMinor\n70\n\n\n\nIl QI dei bambini di 5 anni nati VPT era di 10 punti inferiore a quello dei nati a termine di controllo, e influenzato indipendentemente dalla nascita pretermine e dai dati SES.\nIl QI dei bambini di 5 anni nati VPT era di 10 punti inferiore a quello dei nati a termine e influenzato indipendentemente dalla nascita pretermine e dallo stato socioeconomico (SES).\nUntranslated\nMinor\n\n\n\n\n52-1\nBut with less than 3 months to go for that, I feel I'm not ready yet, but having never taken it, I have nothing to compare it to besides colleagues' advice.\nMa con meno di 3 mesi per farlo, sento di non essere ancora pronto, ma non l’ho mai preso, non ho nulla con cui confrontarlo oltre ai consigli dei colleghi.\nMa con meno di 3 mesi per farlo, sento di non essere ancora pronto, e non avendolo mai fatto, non ho nulla con cui confrontarlo oltre ai consigli dei colleghi.\n\nMistranslation\nMajor\n30\n\n\n52-2\nWithout knowing what I know, they can't know if I'm actually ready yet, but many of them are pushing me to sign up for it.\nSenza sapere quello che so, non possono sapere se sono ancora pronta, ma molti di loro mi stanno spingendo a iscrivermi.\nSe non hanno idea di quanto sappia, non possono sapere se sono davvero pronta, ma molti di loro mi stanno spingendo a iscrivermi.\nReadability\nMinor\n60\n\n\n\nSenza sapere quello che so, non possono sapere se sono ancora pronta, ma molti di loro mi stanno spingendo a iscrivermi.\nSe non hanno idea di quanto sappia, non possono sapere se sono davvero pronta, ma molti di loro mi stanno spingendo a iscrivermi.\nMistranslation\nMinor\n\n\n\n\n52-3\nI'm close... I just don't know if I'm 2 months study close.\nCi sono quasi… solo che non so se ce la farò in soli 2 mesi, ma penso di potercela fare.\nCi sono quasi... solo che non so se ce la farò in soli 2 mesi.\nAddition\nMajor\n20\n\n\n\n\n\n\n\n\n\nTable C.15: QA interface with cropped examples of biomedical and social media texts with error annotations (Biomedical: post-edited segments with No Highlight; Social media: MT outputs).\n\n\n\n\n\n\n\n\n\nFigure C.5: Top: Post-editing rate across highlight modalities, domains and directions. Bottom: Proportion of edits in highlighted spans across highlight modalities. *** \\(=p&lt;0.001\\), ** \\(=p&lt;0.01\\), * \\(=p&lt;0.05\\), ns \\(=\\) not significant with Bonferroni correction.\n\n\n\n\n\n\n\n\n\nFigure C.6: Post-editing agreement across various modalities Section 9.3.2. Results are averaged across all translator pairs for the two modalities (\\(n = 3\\) intra-modality, \\(n=9\\) inter-modality for every language) and all segments.\n\n\n\n\n\n\n\n\n\nFigure C.7: ESA ratings for MT outputs and post-edits across domains and translation directions.\n\n\n\n\n\n\n\n\n\nFigure C.8: Distribution of MQM error categories for MT and post-edits across highlight modalities for the two translation directions and domains of QE4PE.\n\n\n\n\n\n\n\n\n\nFigure C.9: Editing proportion, measured by word error rate between MT and post-edited texts, with respect to post-editor progression. Values are medians across all post-editors.\n\n\n\n\n\n\n\n\n\nFigure C.10: Segment-level post-editing time with respect to post-editor progression. Values are medians across all annotators. Light gray area is min-max values, dark gray represents 25%-75% quantiles. The annotators do not became considerably faster with the task progression, likely due to the simplicity of the task and the high post-editing proficiency of professional post-editors. The high variability in editing times motivates the careful group assignments performed using Pre task edit logs.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Interpretability in Human Translation Workflows</span>"
    ]
  },
  {
    "objectID": "chapters/appendix-c.html#sec-unsup-wqe-appendix",
    "href": "chapters/appendix-c.html#sec-unsup-wqe-appendix",
    "title": "Appendix C — Interpretability in Human Translation Workflows",
    "section": "C.3 Unsupervised MT Error Detection and Human Disagreement",
    "text": "C.3 Unsupervised MT Error Detection and Human Disagreement\n\nC.3.1 Full Results\n\n\n\n\n\n\nMethod\nQE4PE\\(_{\\mathbf{t1}}\\)\nQE4PE\\(_{\\mathbf{t2}}\\)\nQE4PE\\(_{\\mathbf{t3}}\\)\nQE4PE\\(_{\\mathbf{t4}}\\)\nQE4PE\\(_{\\mathbf{t5}}\\)\nQE4PE\\(_{\\mathbf{t6}}\\)\nQE4PE\\(_{\\mathbf{avg}}\\)\n\n\nAP\nF1*\nAP\nF1*\nAP\nF1*\nAP\nF1*\nAP\nF1*\nAP\nF1*\nAP\nF1*\n\n\n\n\nRandom Baseline\n.08\n.14\n.15\n.26\n.06\n.12\n.11\n.19\n.22\n.36\n.18\n.30\n.13\n.23\n\n\nSurprisal\n.11\n.20\n.21\n.31\n.11\n.17\n.16\n.25\n.30\n.40\n.25\n.35\n.19\n.28\n\n\nOut. Entropy\n.12\n.18\n.22\n.30\n.10\n.16\n.17\n.24\n.30\n.39\n.26\n.34\n.19\n.27\n\n\nSurprisal MCD\\(_{\\text{avg}}\\)\n.12\n.20\n.22\n.32\n.11\n.17\n.16\n.26\n.30\n.41\n.26\n.36\n.19\n.29\n\n\nSurprisal MCD\\(_{\\text{var}}\\)\n.13\n.21\n.26\n.33\n.12\n.20\n.19\n.27\n.31\n.40\n.29\n.36\n.22\n.30\n\n\nLL Surprisal\\(_{\\text{best}}\\)\n.11\n.19\n.21\n.32\n.11\n.16\n.16\n.25\n.29\n.40\n.26\n.35\n.19\n.28\n\n\nLL KL-Div\\(_{\\text{best}}\\)\n.09\n.16\n.19\n.28\n.08\n.14\n.13\n.21\n.25\n.37\n.22\n.31\n.16\n.25\n\n\nLL Pred. Depth\n.09\n.16\n.18\n.28\n.07\n.13\n.14\n.21\n.25\n.37\n.21\n.31\n.16\n.24\n\n\nAttn. Entropy\\(_{\\text{avg}}\\)\n.11\n.16\n.17\n.27\n.12\n.17\n.11\n.19\n.23\n.36\n.19\n.31\n.15\n.24\n\n\nAttn. Entropy\\(_{\\text{max}}\\)\n.09\n.14\n.15\n.26\n.10\n.18\n.09\n.19\n.20\n.36\n.16\n.30\n.13\n.24\n\n\nBlood\\(_{\\text{best}}\\)\n.08\n.14\n.16\n.26\n.06\n.12\n.11\n.19\n.23\n.36\n.18\n.30\n.14\n.23\n\n\nxcomet-xl\n.11\n.24\n.22\n.35\n.10\n.20\n.16\n.30\n.27\n.35\n.23\n.34\n.18\n.30\n\n\nxcomet-xl\\(_{\\text{confw}}\\)\n.20\n.25\n.30\n.36\n.14\n.21\n.25\n.31\n.37\n.40\n.31\n.36\n.26\n.32\n\n\nxcomet-xxl\n.13\n.27\n.22\n.32\n.10\n.24\n.17\n.31\n.28\n.32\n.23\n.31\n.19\n.30\n\n\nxcomet-xxl\\(_{\\text{confw}}\\)\n.19\n.27\n.31\n.36\n.17\n.24\n.26\n.32\n.37\n.41\n.33\n.39\n.27\n.33\n\n\nHuman Editors\\(_{\\text{min}}\\)\n.17\n.33\n.26\n.38\n.10\n.21\n.16\n.26\n.25\n.36\n.23\n.30\n.19\n.31\n\n\nHuman Editors\\(_{\\text{avg}}\\)\n.20\n.38\n.29\n.43\n.14\n.30\n.22\n.39\n.32\n.38\n.30\n.40\n.25\n.39\n\n\nHuman Editors\\(_{\\text{max}}\\)\n.24\n.43\n.31\n.47\n.20\n.41\n.24\n.43\n.37\n.50\n.33\n.50\n.28\n.46\n\n\n\n\n\nTable C.16: WQE metrics’ performance for predicting error spans from the six edit sets over NLLB 3.3B translations in the En\\(\\rightarrow\\)It QE4PE dataset. Best unsupervised and overall best metric results are highlighted.\n\n\n\n\n\n\n\n\n\nMethod\nQE4PE\\(_{\\mathbf{t1}}\\)\nQE4PE\\(_{\\mathbf{t2}}\\)\nQE4PE\\(_{\\mathbf{t3}}\\)\nQE4PE\\(_{\\mathbf{t4}}\\)\nQE4PE\\(_{\\mathbf{t5}}\\)\nQE4PE\\(_{\\mathbf{t6}}\\)\nQE4PE\\(_{\\mathbf{avg}}\\)\n\n\nAP\nF1*\nAP\nF1*\nAP\nF1*\nAP\nF1*\nAP\nF1*\nAP\nF1*\nAP\nF1*\n\n\n\n\nRandom Baseline\n.07\n.14\n.34\n.51\n.22\n.36\n.19\n.32\n.13\n.24\n.22\n.36\n.20\n.32\n\n\nSurprisal\n.12\n.19\n.41\n.51\n.30\n.39\n.29\n.37\n.21\n.30\n.31\n.41\n.27\n.36\n\n\nOut. Entropy\n.11\n.18\n.41\n.51\n.31\n.37\n.29\n.36\n.20\n.27\n.31\n.39\n.27\n.35\n\n\nSurprisal MCD\\(_{\\text{avg}}\\)\n.12\n.19\n.42\n.52\n.31\n.40\n.30\n.40\n.21\n.30\n.31\n.42\n.28\n.37\n\n\nSurprisal MCD\\(_{\\text{var}}\\)\n.13\n.21\n.45\n.53\n.36\n.41\n.34\n.40\n.24\n.32\n.36\n.42\n.31\n.38\n\n\nLL Surprisal\\(_{\\text{best}}\\)\n.12\n.19\n.42\n.53\n.30\n.40\n.29\n.38\n.21\n.30\n.31\n.41\n.27\n.37\n\n\nLL KL-Div\\(_{\\text{best}}\\)\n.09\n.15\n.39\n.52\n.28\n.37\n.25\n.34\n.17\n.26\n.29\n.38\n.25\n.34\n\n\nLL Pred. Depth\n.09\n.16\n.37\n.52\n.26\n.37\n.24\n.33\n.17\n.25\n.27\n.38\n.23\n.33\n\n\nAttn. Entropy\\(_{\\text{avg}}\\)\n.09\n.15\n.37\n.51\n.22\n.36\n.20\n.32\n.13\n.24\n.23\n.37\n.21\n.32\n\n\nAttn. Entropy\\(_{\\text{max}}\\)\n.09\n.15\n.35\n.51\n.22\n.36\n.18\n.32\n.12\n.24\n.21\n.37\n.19\n.32\n\n\nBlood\\(_{\\text{best}}\\)\n.07\n.13\n.35\n.51\n.22\n.36\n.19\n.32\n.14\n.24\n.23\n.36\n.20\n.32\n\n\nxcomet-xl\n.13\n.27\n.39\n.39\n.31\n.44\n.28\n.32\n.20\n.35\n.31\n.44\n.27\n.38\n\n\nxcomet-xl\\(_{\\text{confw}}\\)\n.24\n.31\n.47\n.53\n.43\n.45\n.40\n.43\n.29\n.36\n.43\n.46\n.38\n.42\n\n\nxcomet-xxl\n.13\n.28\n.39\n.29\n.30\n.35\n.26\n.35\n.19\n.31\n.30\n.35\n.26\n.32\n\n\nxcomet-xxl\\(_{\\text{confw}}\\)\n.24\n.30\n.48\n.53\n.43\n.45\n.40\n.42\n.31\n.35\n.43\n.45\n.38\n.42\n\n\nHuman Editors\\(_{\\text{min}}\\)\n.16\n.29\n.43\n.51\n.34\n.45\n.33\n.47\n.26\n.42\n.36\n.46\n.32\n.43\n\n\nHuman Editors\\(_{\\text{avg}}\\)\n.17\n.33\n.44\n.51\n.34\n.45\n.33\n.47\n.26\n.42\n.36\n.46\n.32\n.43\n\n\nHuman Editors\\(_{\\text{max}}\\)\n.19\n.36\n.46\n.51\n.36\n.51\n.37\n.53\n.32\n.51\n.40\n.53\n.35\n.49\n\n\n\n\n\nTable C.17: WQE metrics’ performance for predicting error spans from the six edit sets over NLLB 3.3B translations in the En\\(\\rightarrow\\)Nl QE4PE dataset. Best unsupervised and overall best metric results are highlighted.\n\n\n\n\n\n\n\n\n\nMethod\nItalian\nDutch\nArabic\nTurkish\nVietnamese\nUkrainian\nAverage\n\n\nAP\nF1*\nAP\nF1*\nAP\nF1*\nAP\nF1*\nAP\nF1*\nAP\nF1*\nAP\nF1*\n\n\n\n\nRandom Baseline\n.25\n.40\n.28\n.43\n.33\n.49\n.34\n.50\n.35\n.52\n.48\n.65\n.34\n.50\n\n\nSurprisal\n.34\n.45\n.36\n.46\n.42\n.51\n.43\n.54\n.46\n.55\n.55\n.65\n.43\n.53\n\n\nOut. Entropy\n.37\n.43\n.39\n.45\n.45\n.50\n.49\n.52\n.48\n.54\n.58\n.65\n.46\n.51\n\n\nSurprisal MCD\\(_{\\text{avg}}\\)\n.34\n.45\n.37\n.47\n.43\n.52\n.44\n.54\n.46\n.55\n.56\n.65\n.43\n.53\n\n\nSurprisal MCD\\(_{\\text{var}}\\)\n.39\n.46\n.41\n.47\n.47\n.53\n.49\n.55\n.48\n.55\n.61\n.67\n.48\n.54\n\n\nLL Surprisal\\(_{\\text{best}}\\)\n.33\n.44\n.36\n.45\n.41\n.51\n.44\n.54\n.44\n.55\n.55\n.66\n.42\n.53\n\n\nLL KL-Div\\(_{\\text{best}}\\)\n.34\n.42\n.37\n.45\n.41\n.51\n.44\n.52\n.44\n.52\n.56\n.65\n.43\n.51\n\n\nLL Pred. Depth\n.30\n.42\n.32\n.44\n.39\n.50\n.40\n.52\n.39\n.53\n.54\n.66\n.39\n.51\n\n\nAttn. Entropy\\(_{\\text{avg}}\\)\n.28\n.41\n.30\n.43\n.35\n.49\n.37\n.51\n.40\n.52\n.50\n.65\n.37\n.50\n\n\nAttn. Entropy\\(_{\\text{max}}\\)\n.25\n.41\n.26\n.43\n.34\n.49\n.34\n.50\n.35\n.52\n.47\n.65\n.34\n.50\n\n\nBlood\\(_{\\text{best}}\\)\n.26\n.40\n.28\n.43\n.35\n.52\n.35\n.50\n.36\n.52\n.49\n.65\n.35\n.51\n\n\nxcomet-xl\n.34\n.39\n.37\n.44\n.41\n.47\n.44\n.50\n.42\n.44\n.56\n.44\n.42\n.45\n\n\nxcomet-xl\\(_{\\text{confw}}\\)\n.46\n.47\n.49\n.50\n.51\n.53\n.58\n.56\n.53\n.55\n.68\n.67\n.54\n.55\n\n\nxcomet-xxl\n.34\n.36\n.35\n.35\n.43\n.47\n.45\n.48\n.43\n.42\n.57\n.41\n.43\n.42\n\n\nxcomet-xxl\\(_{\\text{confw}}\\)\n.48\n.49\n.50\n.50\n.55\n.54\n.58\n.56\n.56\n.57\n.70\n.67\n.56\n.55\n\n\n\n\n\nTable C.18: WQE metrics’ performance for predicting error spans from multiple edit sets (one per language) over mBART-50 translations across the six topologically diverse target languages of DivEMT.\n\n\n\n\n\n\n\n\n\nMethod\nEn\\(\\rightarrow\\) Ja\nEn\\(\\rightarrow\\) Zh\nEn\\(\\rightarrow\\) Hi\nCs\\(\\rightarrow\\) Uk\nEn\\(\\rightarrow\\) Cs\nEn\\(\\rightarrow\\) Ru\nAverage\n\n\nAP\nF1*\nAP\nF1*\nAP\nF1*\nAP\nF1*\nAP\nF1*\nAP\nF1*\nAP\nF1*\n\n\n\n\nRandom Baseline\n.02\n.03\n.03\n.07\n.03\n.07\n.05\n.09\n.06\n.11\n.08\n.16\n.05\n.09\n\n\nSurprisal\n.03\n.07\n.05\n.09\n.05\n.09\n.14\n.20\n.10\n.16\n.13\n.19\n.08\n.13\n\n\nOut. Entropy\n.03\n.08\n.06\n.11\n.06\n.10\n.20\n.27\n.12\n.18\n.14\n.20\n.10\n.16\n\n\nLL Surprisal\\(_{\\text{best}}\\)\n.03\n.07\n.05\n.09\n.05\n.09\n.14\n.20\n.10\n.16\n.13\n.19\n.08\n.13\n\n\nLL KL-Div\\(_{\\text{best}}\\)\n.02\n.05\n.04\n.07\n.04\n.08\n.10\n.17\n.09\n.15\n.12\n.19\n.07\n.12\n\n\nLL Pred. Depth\n.02\n.05\n.04\n.08\n.04\n.09\n.09\n.18\n.08\n.14\n.11\n.18\n.06\n.12\n\n\nAttn. Entropy\\(_{\\text{avg}}\\)\n.02\n.03\n.03\n.07\n.03\n.07\n.03\n.09\n.05\n.11\n.07\n.16\n.04\n.09\n\n\nAttn. Entropy\\(_{\\text{max}}\\)\n.01\n.03\n.03\n.07\n.03\n.07\n.03\n.09\n.05\n.11\n.08\n.16\n.04\n.09\n\n\nxcomet-xl\n.04\n.09\n.05\n.11\n.06\n.12\n.13\n.28\n.11\n.24\n.16\n.32\n.09\n.19\n\n\nxcomet-xl\\(_{\\text{confw}}\\)\n.08\n.14\n.10\n.16\n.10\n.19\n.18\n.30\n.19\n.29\n.24\n.32\n.15\n.23\n\n\nxcomet-xxl\n.04\n.11\n.06\n.13\n.05\n.11\n.13\n.28\n.11\n.24\n.16\n.33\n.09\n.20\n\n\nxcomet-xxl\\(_{\\text{confw}}\\)\n.07\n.15\n.09\n.19\n.09\n.17\n.19\n.29\n.22\n.30\n.28\n.33\n.16\n.24\n\n\n\n\n\nTable C.19: WQE metrics’ performance for predicting error spans from the ESA annotations (one set per language) over Aya23-35B outputs for the WMT24 dataset.\n\n\n\n\n\n\n\n\n\nFigure C.11: Precision-recall curves for xcomet metrics and Surprisal MCDvar for all annotators of QE4PE En\\(\\rightarrow\\)It.\n\n\n\n\n\n\n\n\n\nFigure C.12: Precision-recall curves for xcomet metrics and Surprisal MCDvar for all annotators of QE4PE En\\(\\rightarrow\\)Nl.\n\n\n\n\n\n\n\n\n\nFigure C.13: Precision-recall curves for xcomet metrics and Surprisal MCDvar on all DivEMT languages.\n\n\n\n\n\n\n\n\n\nFigure C.14: Precision-recall curves for xcomet metrics and Out. Entropy on all WMT24 languages.\n\n\n\n\n\n\n\nAngela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep Baines, Onur Çelebi, Guillaume Wenzek, Vishrav Chaudhary, Naman Goyal, Tom Birch, Vitaliy Liptchinsky, Sergey Edunov, Edouard Grave, Michael Auli, and Armand Joulin. 2021. Beyond english-centric multilingual machine translation. Journal of Machine Learning Research, 22(107):1–48.\n\n\nMatt Post. 2018. A call for clarity in reporting BLEU scores. In Ondřej Bojar, Rajen Chatterjee, Christian Federmann, Mark Fishel, Yvette Graham, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes, Philipp Koehn, Christof Monz, Matteo Negri, Aurélie Névéol, Mariana Neves, Matt Post, Lucia Specia, Marco Turchi, and Karin Verspoor, editors, Proceedings of the third conference on machine translation: Research papers, pages 186–191, Brussels, Belgium. Association for Computational Linguistics.\n\n\nRicardo Rei, Craig Stewart, Ana C Farinha, and Alon Lavie. 2020. COMET: A neural framework for MT evaluation. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, Proceedings of the 2020 conference on empirical methods in natural language processing (EMNLP), pages 2685–2702, Online. Association for Computational Linguistics.\n\n\nMatthew Snover, Bonnie Dorr, Rich Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proceedings of the 7th conference of the association for machine translation in the americas: Technical papers, pages 223–231, Cambridge, Massachusetts, USA. Association for Machine Translation in the Americas.\n\n\nJörg Tiedemann and Santhosh Thottingal. 2020. OPUS-MT – building open translation services for the world. In André Martins, Helena Moniz, Sara Fumega, Bruno Martins, Fernando Batista, Luisa Coheur, Carla Parra, Isabel Trancoso, Marco Turchi, Arianna Bisazza, Joss Moorkens, Ana Guerberof, Mary Nurminen, Lena Marg, and Mikel L. Forcada, editors, Proceedings of the 22nd annual conference of the european association for machine translation, pages 479–480, Lisboa, Portugal. European Association for Machine Translation.\n\n\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, et al. 2020. Transformers: State-of-the-art natural language processing. In Qun Liu and David Schlangen, editors, Proceedings of the 2020 conference on empirical methods in natural language processing: System demonstrations, pages 38–45, Online. Association for Computational Linguistics.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Interpretability in Human Translation Workflows</span>"
    ]
  },
  {
    "objectID": "chapters/appendix-c.html#footnotes",
    "href": "chapters/appendix-c.html#footnotes",
    "title": "Appendix C — Interpretability in Human Translation Workflows",
    "section": "",
    "text": "The document processing order was originally included to identify possible longitudinal effects but was removed due to a lack of significant improvements.↩︎",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Interpretability in Human Translation Workflows</span>"
    ]
  }
]